{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmSQ0bS+5xqBC6YwYRXCHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminpolydeq/ARSLM/blob/main/ARSLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Traceback (most recent call last):\n",
        "  File \"/data/user/0/ru.iiec.pydroid3/files/accomp_files/iiec_run/iiec_run.py\", line 31, in <module>\n",
        "    start(fakepyfile,mainpyfile)\n",
        "    ~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
        "  File \"/data/user/0/ru.iiec.pydroid3/files/accomp_files/iiec_run/iiec_run.py\", line 30, in start\n",
        "    exec(open(mainpyfile).read(),  __main__.__dict__)\n",
        "    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "  File \"<string>\", line 13, in <module>\n",
        "ModuleNotFoundError: No module named 'torch'\n",
        "\n",
        "[Program finished]"
      ],
      "metadata": {
        "id": "zz-isu3wtlXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d39c8ac"
      },
      "source": [
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McJITnyBulIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5e86f90"
      },
      "source": [
        "## Explication détaillée du modèle ARSLM\n",
        "\n",
        "Le modèle ARSLM (Adaptive Recurrent State Language Model) est un prototype de modèle linguistique basé sur le concept d'états récurrents adaptatifs (ARS), inspiré par les séquences adaptatives BenPolySeq. Il est conçu pour être léger, explicable et extensible, ce qui le rend adapté à la recherche et à l'expérimentation.\n",
        "\n",
        "Voici une décomposition de ses composants clés :\n",
        "\n",
        "### 1. Tokenizer Avancé (`AdvancedTokenizer`)\n",
        "\n",
        "Ce composant gère la conversion du texte en séquences d'identifiants (tokens) et vice-versa. Dans cette version améliorée, il utilise la bibliothèque `transformers` de Hugging Face, spécifiquement le `BertTokenizer`.\n",
        "\n",
        "*   **Pourquoi un tokenizer avancé ?** Contrairement à un simple tokenizer qui pourrait se contenter de diviser le texte par espaces et de gérer un vocabulaire limité, un tokenizer basé sur les sous-mots (comme ceux utilisés par BERT) permet de :\n",
        "    *   Mieux gérer les mots inconnus en les décomposant en sous-unités (par exemple, \"tokenization\" pourrait être décomposé en \"token\", \"iza\", \"tion\").\n",
        "    *   Réduire la taille globale du vocabulaire tout en représentant une plus grande variété de mots.\n",
        "    *   Gérer la ponctuation, les majuscules/minuscules et d'autres subtilités linguistiques.\n",
        "*   **Fonctionnement :** L'`AdvancedTokenizer` utilise un vocabulaire pré-entraîné (ici, celui de `bert-base-uncased` par défaut). Il fournit des méthodes `encode` pour convertir le texte en identifiants et `decode` pour convertir les identifiants en texte. Il définit également des identifiants spéciaux pour le padding (`<pad>`), les mots inconnus (`<unk>`), le début de séquence (`<bos>`, utilisant `[CLS]` de BERT) et la fin de séquence (`<eos>`, utilisant `[SEP]` de BERT).\n",
        "\n",
        "### 2. Cellule ARS (`ARSCell`)\n",
        "\n",
        "C'est le cœur adaptatif du modèle. Une `ARSCell` calcule le prochain état caché (`h_t`) en se basant sur les deux états cachés précédents (`h_{t-2}`, `h_{t-1}`) et l'embedding de l'entrée courante (`x_embed`).\n",
        "\n",
        "*   **Mécanisme d'Adaptation :** L'idée principale, inspirée par BenPolySeq, est que la mise à jour de l'état caché n'est pas une simple combinaison linéaire ou une porte fixe (comme dans les RNN ou LSTM basiques), mais une addition adaptative d'un \"candidat\" pondéré par un \"gate\".\n",
        "    *   **Signal de Différence :** La cellule utilise la différence entre les deux états cachés précédents (`h_{t-1} - h_{t-2}`). Ce signal est censé capturer l'évolution ou la dynamique de l'état récurrent.\n",
        "    *   **Contexte :** Un vecteur de contexte est créé en concaténant les deux états cachés précédents et l'embedding d'entrée (`[h_{t-1}, h_{t-2}, x_embed]`).\n",
        "    *   **Réseau Candidat (`candidate_mlp`) :** Un MLP (réseau de neurones multi-couches) prend le contexte en entrée et génère un \"candidat\" pour la mise à jour de l'état caché.\n",
        "    *   **Réseau de Porte (`gate_net`) :** Un autre réseau (comprenant une fonction sigmoïde à la fin) prend également le contexte en entrée et produit une valeur scalaire entre 0 et 1. Cette valeur agit comme un \"gate\" qui contrôle l'influence du candidat sur la mise à jour de l'état caché.\n",
        "    *   **Résiduel :** Un petit résiduel de l'embedding d'entrée est ajouté pour aider à propager directement l'information de l'entrée.\n",
        "    *   **Mise à Jour :** L'état caché suivant est calculé comme `h_t = h_{t-1} + gate * candidate + 0.1 * residual`. Cette formule montre comment la cellule *adapte* sa mise à jour en fonction du signal de différence et du contexte via le gate.\n",
        "    *   **Dropout et Normalisation :** Une couche de dropout est appliquée pour la régularisation, et une normalisation de couche (`F.layer_norm`) est utilisée pour stabiliser l'entraînement.\n",
        "\n",
        "### 3. Modèle ARSLM (`ARSLM`)\n",
        "\n",
        "Ce module encapsule l'ensemble de l'architecture, combinant l'embedding, plusieurs couches d'`ARSCell`, un mécanisme d'attention et une couche de sortie (head).\n",
        "\n",
        "*   **Embedding (`nn.Embedding`) :** Convertit les identifiants de tokens en vecteurs denses de dimension `emb_dim`.\n",
        "*   **Couches d'ARSCell (`nn.ModuleList`) :** Le modèle utilise une liste de `num_layers` ARSCells empilées. L'entrée de la première cellule est l'embedding, et l'entrée des cellules subséquentes est l'état caché de la couche précédente.\n",
        "*   **Mécanisme d'Attention :** Un mécanisme d'attention additive simple est appliqué après la dernière couche d'ARSCell.\n",
        "    *   **Attention Causale :** Pendant la passe avant (forward), l'attention est causale, ce qui signifie que chaque token ne peut prêter attention qu'aux tokens précédents dans la séquence.\n",
        "    *   **Calcul :** Pour chaque pas temporel `t`, l'état caché de la dernière couche (`last_layer_h_t`) sert de requête. L'attention est calculée sur l'historique de tous les états cachés de la dernière couche jusqu'au pas `t` (`all_last_layer_hidden_states`). Les scores d'attention sont calculés via un petit réseau (`self.attention`), puis normalisés avec un softmax. Un vecteur de contexte est obtenu en pondérant l'historique par les poids d'attention.\n",
        "    *   **Combinaison :** Le vecteur de contexte est ajouté à l'état caché courant de la dernière couche (`attended_h_t = last_layer_h_t + context_vector`). Cette combinaison permet au modèle de tirer parti des informations pertinentes de la séquence passée.\n",
        "*   **Couche de Sortie (`head`) :** Une couche linéaire qui prend l'état caché combiné (avec attention) de la dernière couche et le projette sur la taille du vocabulaire (`self.vocab_size`). Les sorties de cette couche sont les logits, qui représentent la probabilité non normalisée de chaque token dans le vocabulaire d'être le prochain token.\n",
        "\n",
        "### 4. Fonction `forward`\n",
        "\n",
        "Cette fonction décrit comment une séquence d'identifiants (`input_ids`) est traitée par le modèle pour produire les logits et les gates. Elle itère sur chaque pas temporel de la séquence d'entrée, calculant séquentiellement les états cachés pour chaque couche d'ARSCell et appliquant l'attention et la couche de sortie à chaque pas après la dernière couche.\n",
        "\n",
        "### 5. Fonction `generate`\n",
        "\n",
        "Cette fonction implémente le processus de génération de texte. À partir d'une séquence de contexte initiale (`idx`), le modèle génère séquentiellement de nouveaux tokens. Pour chaque nouveau token à générer :\n",
        "*   Il prend l'embedding du dernier token généré.\n",
        "*   Il utilise les états cachés mis à jour des étapes précédentes pour calculer le nouvel état caché pour chaque couche d'ARSCell.\n",
        "*   Il applique le mécanisme d'attention sur l'historique des états cachés de la dernière couche (incluant le nouvel état).\n",
        "*   Il utilise la couche de sortie pour obtenir les logits pour le prochain token.\n",
        "*   Il applique un échantillonnage (ici, `torch.multinomial` avec une température et optionnellement `top_k`) pour sélectionner le prochain token en fonction des probabilités calculées à partir des logits.\n",
        "*   Le nouveau token est ajouté à la séquence de sortie, et le processus se répète jusqu'à ce que le nombre maximal de nouveaux tokens soit atteint.\n",
        "\n",
        "### 6. Utilitaires d'Entraînement/Démo (`collate_batch`, `train_demo`)\n",
        "\n",
        "*   **`collate_batch` :** Prépare les données texte pour l'entraînement en les encodant à l'aide du tokenizer, en ajoutant les tokens de début/fin de séquence et en effectuant le padding pour que toutes les séquences d'un batch aient la même longueur.\n",
        "*   **`train_demo` :** Une fonction de démonstration qui initialise le tokenizer, le modèle, l'optimiseur et la fonction de perte. Elle charge un petit corpus (ou un fichier si spécifié), prépare les données, entraîne le modèle sur un nombre spécifié d'époques, imprime la perte et le taux d'apprentissage, calcule la perplexité et effectue une petite démo de génération de texte.\n",
        "\n",
        "En résumé, l'ARSLM combine l'idée d'états récurrents adaptatifs avec une architecture multi-couches et un mécanisme d'attention causale pour créer un modèle linguistique capable d'apprendre des dépendances séquentielles de manière flexible. L'aspect \"adaptatif\" via le mécanisme de gate dans l'ARSCell est la caractéristique distinctive, permettant potentiellement au modèle d'ajuster son comportement en fonction de la dynamique interne de sa mémoire récurrente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a1548d2"
      },
      "source": [
        "# Install PyTorch\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ae78ef"
      },
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "\n",
        "# Perform an operation\n",
        "y = torch.rand(5, 3)\n",
        "print(x + y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90f2b52a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "\n",
        "# Assuming AdvancedTokenizer and ARSCell are defined or imported elsewhere\n",
        "# For a complete working script, please refer to cell 64161967\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers and attention during generation.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# Training / demo utilities\n",
        "# ----------------------------\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # toy corpus\n",
        "    texts = [\n",
        "        \"hello world this is ars\",\n",
        "        \"the system adapts to its history\",\n",
        "        \"benpolyseq demonstrates adaptive sequences\",\n",
        "        \"ars can inspire new network protocols\",\n",
        "        \"self optimizing systems are possible\"\n",
        "    ]\n",
        "    # Instantiate AdvancedTokenizer\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    # Vocab size is now determined by the tokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "    # Instantiate ARSLM with multiple layers\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device) # Added num_layers\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    # Use ignore_index from the tokenizer\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # create training batches by shifting: predict next token at each position\n",
        "    n_epochs = 200\n",
        "    batch = collate_batch(tokenizer, texts, device) # Pass tokenizer to collate_batch\n",
        "    # targets: same as input (language modeling)\n",
        "    inputs = batch[:, :-1]\n",
        "    targets = batch[:, 1:]\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        logits, gates = model(inputs)\n",
        "        # flatten\n",
        "        b, seq, v = logits.shape\n",
        "        loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — loss: {loss.item():.4f}\")\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    # Use the tokenizer's encode method for the context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0)[0]\n",
        "    print(\"=== Generated ===\")\n",
        "    # Use the tokenizer's decode method for the output\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "MSy191NO-PhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "rL9kQ9sD-PhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64161967"
      },
      "source": [
        "\"\"\"\n",
        "ARSLM prototype (toy engine)\n",
        "Author: Benjamin Amaad Kama (concept)\n",
        "Requirements: Python 3.8+, PyTorch, transformers\n",
        "\n",
        "This is a minimal, fully-contained prototype of an ARS-based language model.\n",
        "It is for research/experimentation: lightweight, explainable, and extendable.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "            # For the next step, we need the hidden state corresponding to the newly generated token.\n",
        "            # We'll simulate computing the next hidden state using the ARSCell with the *newly generated token's embedding*\n",
        "            # and the last two hidden states (h_prev2, h_prev1).\n",
        "            new_token_embed = self.emb(next_token.unsqueeze(1)).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            # Re-running the layers for the new token to get updated hidden states\n",
        "            h_t_input_next = new_token_embed\n",
        "            current_layer_hidden_states_next = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                # Use the updated h_prev2_list and h_prev1_list from the previous time step's processing\n",
        "                h_prev2_l = h_prev2_list[layer]\n",
        "                h_prev1_l = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input_next = current_layer_hidden_states_next[-1]\n",
        "\n",
        "                h_t_next_l, _ = cell(h_prev2_l, h_prev1_l, h_t_input_next)\n",
        "                current_layer_hidden_states_next.append(h_t_next_l)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1_list[layer]\n",
        "                h_prev1_list[layer] = h_t_next_l\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t_next_l.unsqueeze(1))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# Training / demo utilities\n",
        "# ----------------------------\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # toy corpus\n",
        "    # texts = [\n",
        "    #     \"hello world this is ars\",\n",
        "    #     \"the system adapts to its history\",\n",
        "    #     \"benpolyseq demonstrates adaptive sequences\",\n",
        "    #     \"ars can inspire new network protocols\",\n",
        "    #     \"self optimizing systems are possible\"\n",
        "    # ]\n",
        "    # --- Load text from a file (example) ---\n",
        "    corpus_file = \"/tmp/large_corpus.txt\" # Replace with your corpus file path\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            # Read a limited number of lines for demo, for full training, iterate or use data loaders\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1000: # Load first 1000 lines as an example\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "\n",
        "    # Instantiate AdvancedTokenizer\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    # Vocab size is now determined by the tokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "    # Instantiate ARSLM with multiple layers\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device) # Added num_layers\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    # Add a StepLR scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5) # Reduce LR by half every 50 epochs\n",
        "    # Use ignore_index from the tokenizer\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # create training batches by shifting: predict next token at each position\n",
        "    n_epochs = 200\n",
        "    batch = collate_batch(tokenizer, texts, device) # Pass tokenizer to collate_batch\n",
        "    # targets: same as input (language modeling)\n",
        "    inputs = batch[:, :-1]\n",
        "    targets = batch[:, 1:]\n",
        "\n",
        "    # Simple check if batch is empty due to no text loaded\n",
        "    if inputs.numel() == 0:\n",
        "        print(\"No training data loaded. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        logits, gates = model(inputs)\n",
        "        # flatten\n",
        "        b, seq, v = logits.shape\n",
        "        loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — loss: {loss.item():.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\") # Print current LR\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(inputs) # Use the same inputs for simplicity in this demo\n",
        "        # Calculate perplexity\n",
        "        # Perplexity = exp(average negative log-likelihood)\n",
        "        loss = loss_fn(logits.view(-1, v), targets.reshape(-1))\n",
        "        perplexity = torch.exp(loss)\n",
        "        print(f\"Perplexity on training data: {perplexity.item():.4f}\")\n",
        "\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    # Use the tokenizer's encode method for the context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    # Use top_k sampling during generation\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0] # Added top_k=50\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    # Use the tokenizer's decode method for the output\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edb16161"
      },
      "source": [
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d76b4e22"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `transformers` library is installed, I will import a suitable tokenizer, such as `BertTokenizer`, and create a new class that wraps this tokenizer to maintain compatibility with the existing code's `encode` and `decode` methods. I will also define the special tokens needed for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af98d291"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "from typing import List\n",
        "\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# Example usage:\n",
        "# tokenizer = AdvancedTokenizer()\n",
        "# text = \"Hello, world! This is a test sentence.\"\n",
        "# encoded_text = tokenizer.encode(text)\n",
        "# print(\"Encoded:\", encoded_text)\n",
        "# decoded_text = tokenizer.decode(encoded_text)\n",
        "# print(\"Decoded:\", decoded_text)\n",
        "# print(\"Vocab size:\", len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ced6a3b5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install a more robust tokenizer library. Hugging Face's `transformers` library is a good choice as it provides access to many pre-trained tokenizers including those using sub-word techniques. The `tokenizers` library is also a good choice, but `transformers` is more commonly used and includes the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b8abc8a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "8FAjGRVduH7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "\n",
        "# ----------------------------\n",
        "# Simple tokenizer / vocab (to be replaced)\n",
        "# ----------------------------\n",
        "# class SimpleTokenizer:\n",
        "#     def __init__(self, texts: List[str], min_freq=1):\n",
        "#         tokens = []\n",
        "#         for t in texts:\n",
        "#             tokens += t.strip().split()\n",
        "#         freq = {}\n",
        "#         for w in tokens:\n",
        "#             freq[w] = freq.get(w, 0) + 1\n",
        "#         self.vocab = ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "#         for w, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])):\n",
        "#             if c >= min_freq and w not in self.vocab:\n",
        "#                 self.vocab.append(w)\n",
        "#         self.stoi = {w:i for i,w in enumerate(self.vocab)}\n",
        "#         self.itos = {i:w for w,i in self.stoi.items()}\n",
        "\n",
        "#     def encode(self, text: str) -> List[int]:\n",
        "#         toks = text.strip().split()\n",
        "#         ids = [self.stoi.get(t, self.stoi['<unk>']) for t in toks]\n",
        "#         return ids\n",
        "\n",
        "#     def decode(self, ids: List[int]) -> str:\n",
        "#         tokens = [self.itos.get(i, '<unk>') for i in ids]\n",
        "#         return ' '.join(tokens)\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# Training / demo utilities\n",
        "# ----------------------------\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # toy corpus\n",
        "    # texts = [\n",
        "    #     \"hello world this is ars\",\n",
        "    #     \"the system adapts to its history\",\n",
        "    #     \"benpolyseq demonstrates adaptive sequences\",\n",
        "    #     \"ars can inspire new network protocols\",\n",
        "    #     \"self optimizing systems are possible\"\n",
        "    # ]\n",
        "    # --- Load text from a file (example) ---\n",
        "    corpus_file = \"/tmp/large_corpus.txt\" # Replace with your corpus file path\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            # Read a limited number of lines for demo, for full training, iterate or use data loaders\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1000: # Load first 1000 lines as an example\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "\n",
        "    # Instantiate AdvancedTokenizer\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    # Vocab size is now determined by the tokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "    # Instantiate ARSLM with multiple layers\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device) # Added num_layers\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    # Add a StepLR scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5) # Reduce LR by half every 50 epochs\n",
        "    # Use ignore_index from the tokenizer\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # create training batches by shifting: predict next token at each position\n",
        "    n_epochs = 200\n",
        "    batch = collate_batch(tokenizer, texts, device) # Pass tokenizer to collate_batch\n",
        "    # targets: same as input (language modeling)\n",
        "    inputs = batch[:, :-1]\n",
        "    targets = batch[:, 1:]\n",
        "\n",
        "    # Simple check if batch is empty due to no text loaded\n",
        "    if inputs.numel() == 0:\n",
        "        print(\"No training data loaded. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        logits, gates = model(inputs)\n",
        "        # flatten\n",
        "        b, seq, v = logits.shape\n",
        "        loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — loss: {loss.item():.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\") # Print current LR\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(inputs) # Use the same inputs for simplicity in this demo\n",
        "        # Calculate perplexity\n",
        "        # Perplexity = exp(average negative log-likelihood)\n",
        "        loss = loss_fn(logits.view(-1, v), targets.reshape(-1))\n",
        "        perplexity = torch.exp(loss)\n",
        "        print(f\"Perplexity on training data: {perplexity.item():.4f}\")\n",
        "\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    # Use the tokenizer's encode method for the context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    # Use top_k sampling during generation\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0] # Added top_k=50\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    # Use the tokenizer's decode method for the output\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ],
      "metadata": {
        "id": "bR9ngUy7ug-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0_WreBFrmSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSy8mGl7rxDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0goV5N7wrxUV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eae3307"
      },
      "source": [
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kkA1dRX-uxuJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d52a918"
      },
      "source": [
        "# Task\n",
        "Propose des améliorations pour le modèle ARSLM afin de le rendre plus efficace et opérationnel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a243d42"
      },
      "source": [
        "## Amélioration du tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Remplacer le `SimpleTokenizer` par un tokenizer plus robuste capable de gérer la ponctuation, les majuscules/minuscules, les sous-mots (comme les Byte Pair Encoding - BPE ou WordPiece) pour mieux gérer les mots inconnus et réduire la taille du vocabulaire.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09b9c1a5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install a more robust tokenizer library. Hugging Face's `transformers` library is a good choice as it provides access to many pre-trained tokenizers including those using sub-word techniques. The `tokenizers` library is also a good choice, but `transformers` is more commonly used and includes the tokenizers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79429106"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cd18cd"
      },
      "source": [
        "## Explication détaillée du modèle ARSLM\n",
        "\n",
        "Le modèle ARSLM (Adaptive Recurrent State Language Model) est un prototype de modèle linguistique basé sur le concept d'états récurrents adaptatifs (ARS), inspiré par les séquences adaptatives BenPolySeq. Il est conçu pour être léger, explicable et extensible, ce qui le rend adapté à la recherche et à l'expérimentation.\n",
        "\n",
        "Voici une décomposition de ses composants clés :\n",
        "\n",
        "### 1. Tokenizer Avancé (`AdvancedTokenizer`)\n",
        "\n",
        "Ce composant gère la conversion du texte en séquences d'identifiants (tokens) et vice-versa. Dans cette version améliorée, il utilise la bibliothèque `transformers` de Hugging Face, spécifiquement le `BertTokenizer`.\n",
        "\n",
        "*   **Pourquoi un tokenizer avancé ?** Contrairement à un simple tokenizer qui pourrait se contenter de diviser le texte par espaces et de gérer un vocabulaire limité, un tokenizer basé sur les sous-mots (comme ceux utilisés par BERT) permet de :\n",
        "    *   Mieux gérer les mots inconnus en les décomposant en sous-unités (par exemple, \"tokenization\" pourrait être décomposé en \"token\", \"iza\", \"tion\").\n",
        "    *   Réduire la taille globale du vocabulaire tout en représentant une plus grande variété de mots.\n",
        "    *   Gérer la ponctuation, les majuscules/minuscules et d'autres subtilités linguistiques.\n",
        "*   **Fonctionnement :** L'`AdvancedTokenizer` utilise un vocabulaire pré-entraîné (ici, celui de `bert-base-uncased` par défaut). Il fournit des méthodes `encode` pour convertir le texte en identifiants et `decode` pour convertir les identifiants en texte. Il définit également des identifiants spéciaux pour le padding (`<pad>`), les mots inconnus (`<unk>`), le début de séquence (`<bos>`, utilisant `[CLS]` de BERT) et la fin de séquence (`<eos>`, utilisant `[SEP]` de BERT).\n",
        "\n",
        "### 2. Cellule ARS (`ARSCell`)\n",
        "\n",
        "C'est le cœur adaptatif du modèle. Une `ARSCell` calcule le prochain état caché (`h_t`) en se basant sur les deux états cachés précédents (`h_{t-2}`, `h_{t-1}`) et l'embedding de l'entrée courante (`x_embed`).\n",
        "\n",
        "*   **Mécanisme d'Adaptation :** L'idée principale, inspirée par BenPolySeq, est que la mise à jour de l'état caché n'est pas une simple combinaison linéaire ou une porte fixe (comme dans les RNN ou LSTM basiques), mais une addition adaptative d'un \"candidat\" pondéré par un \"gate\".\n",
        "    *   **Signal de Différence :** La cellule utilise la différence entre les deux états cachés précédents (`h_{t-1} - h_{t-2}`). Ce signal est censé capturer l'évolution ou la dynamique de l'état récurrent.\n",
        "    *   **Contexte :** Un vecteur de contexte est créé en concaténant les deux états cachés précédents et l'embedding d'entrée (`[h_{t-1}, h_{t-2}, x_embed]`).\n",
        "    *   **Réseau Candidat (`candidate_mlp`) :** Un MLP (réseau de neurones multi-couches) prend le contexte en entrée et génère un \"candidat\" pour la mise à jour de l'état caché.\n",
        "    *   **Réseau de Porte (`gate_net`) :** Un autre réseau (comprenant une fonction sigmoïde à la fin) prend également le contexte en entrée et produit une valeur scalaire entre 0 et 1. Cette valeur agit comme un \"gate\" qui contrôle l'influence du candidat sur la mise à jour de l'état caché.\n",
        "    *   **Résiduel :** Un petit résiduel de l'embedding d'entrée est ajouté pour aider à propager directement l'information de l'entrée.\n",
        "    *   **Mise à Jour :** L'état caché suivant est calculé comme `h_t = h_{t-1} + gate * candidate + 0.1 * residual`. Cette formule montre comment la cellule *adapte* sa mise à jour en fonction du signal de différence et du contexte via le gate.\n",
        "    *   **Dropout et Normalisation :** Une couche de dropout est appliquée pour la régularisation, et une normalisation de couche (`F.layer_norm`) est utilisée pour stabiliser l'entraînement.\n",
        "\n",
        "### 3. Modèle ARSLM (`ARSLM`)\n",
        "\n",
        "Ce module encapsule l'ensemble de l'architecture, combinant l'embedding, plusieurs couches d'`ARSCell`, un mécanisme d'attention et une couche de sortie (head).\n",
        "\n",
        "*   **Embedding (`nn.Embedding`) :** Convertit les identifiants de tokens en vecteurs denses de dimension `emb_dim`.\n",
        "*   **Couches d'ARSCell (`nn.ModuleList`) :** Le modèle utilise une liste de `num_layers` ARSCells empilées. L'entrée de la première cellule est l'embedding, et l'entrée des cellules subséquentes est l'état caché de la couche précédente.\n",
        "*   **Mécanisme d'Attention :** Un mécanisme d'attention additive simple est appliqué après la dernière couche d'ARSCell.\n",
        "    *   **Attention Causale :** Pendant la passe avant (forward), l'attention est causale, ce qui signifie que chaque token ne peut prêter attention qu'aux tokens précédents dans la séquence.\n",
        "    *   **Calcul :** Pour chaque pas temporel `t`, l'état caché de la dernière couche (`last_layer_h_t`) sert de requête. L'attention est calculée sur l'historique de tous les états cachés de la dernière couche jusqu'au pas `t` (`all_last_layer_hidden_states`). Les scores d'attention sont calculés via un petit réseau (`self.attention`), puis normalisés avec un softmax. Un vecteur de contexte est obtenu en pondérant l'historique par les poids d'attention.\n",
        "    *   **Combinaison :** Le vecteur de contexte est ajouté à l'état caché courant de la dernière couche (`attended_h_t = last_layer_h_t + context_vector`). Cette combinaison permet au modèle de tirer parti des informations pertinentes de la séquence passée.\n",
        "*   **Couche de Sortie (`head`) :** Une couche linéaire qui prend l'état caché combiné (avec attention) de la dernière couche et le projette sur la taille du vocabulaire (`self.vocab_size`). Les sorties de cette couche sont les logits, qui représentent la probabilité non normalisée de chaque token dans le vocabulaire d'être le prochain token.\n",
        "\n",
        "### 4. Fonction `forward`\n",
        "\n",
        "Cette fonction décrit comment une séquence d'identifiants (`input_ids`) est traitée par le modèle pour produire les logits et les gates. Elle itère sur chaque pas temporel de la séquence d'entrée, calculant séquentiellement les états cachés pour chaque couche d'ARSCell et appliquant l'attention et la couche de sortie à chaque pas après la dernière couche.\n",
        "\n",
        "### 5. Fonction `generate`\n",
        "\n",
        "Cette fonction implémente le processus de génération de texte. À partir d'une séquence de contexte initiale (`idx`), le modèle génère séquentiellement de nouveaux tokens. Pour chaque nouveau token à générer :\n",
        "*   Il prend l'embedding du dernier token généré.\n",
        "*   Il utilise les états cachés mis à jour des étapes précédentes pour calculer le nouvel état caché pour chaque couche d'ARSCell.\n",
        "*   Il applique le mécanisme d'attention sur l'historique des états cachés de la dernière couche (incluant le nouvel état).\n",
        "*   Il utilise la couche de sortie pour obtenir les logits pour le prochain token.\n",
        "*   Il applique un échantillonnage (ici, `torch.multinomial` avec une température et optionnellement `top_k`) pour sélectionner le prochain token en fonction des probabilités calculées à partir des logits.\n",
        "*   Le nouveau token est ajouté à la séquence de sortie, et le processus se répète jusqu'à ce que le nombre maximal de nouveaux tokens soit atteint.\n",
        "\n",
        "### 6. Utilitaires d'Entraînement/Démo (`collate_batch`, `train_demo`)\n",
        "\n",
        "*   **`collate_batch` :** Prépare les données texte pour l'entraînement en les encodant à l'aide du tokenizer, en ajoutant les tokens de début/fin de séquence et en effectuant le padding pour que toutes les séquences d'un batch aient la même longueur.\n",
        "*   **`train_demo` :** Une fonction de démonstration qui initialise le tokenizer, le modèle, l'optimiseur et la fonction de perte. Elle charge un petit corpus (ou un fichier si spécifié), prépare les données, entraîne le modèle sur un nombre spécifié d'époques, imprime la perte et le taux d'apprentissage, calcule la perplexité et effectue une petite démo de génération de texte.\n",
        "\n",
        "En résumé, l'ARSLM combine l'idée d'états récurrents adaptatifs avec une architecture multi-couches et un mécanisme d'attention causale pour créer un modèle linguistique capable d'apprendre des dépendances séquentielles de manière flexible. L'aspect \"adaptatif\" via le mécanisme de gate dans l'ARSCell est la caractéristique distinctive, permettant potentiellement au modèle d'ajuster son comportement en fonction de la dynamique interne de sa mémoire récurrente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "342b7ddc"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `transformers` library is installed, I will import a suitable tokenizer, such as `BertTokenizer`, and create a new class that wraps this tokenizer to maintain compatibility with the existing code's `encode` and `decode` methods. I will also define the special tokens needed for the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61764a30"
      },
      "source": [
        "# Task\n",
        "Refactor the data loading and preparation process by creating a `TextDataset` class that integrates with `torch.utils.data.DataLoader`, modifying `collate_batch` to support batched data loading, and implementing training, validation, and test data splits within the `train_demo` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1131c95a"
      },
      "source": [
        "## Refactorisation du Chargement et Préparation des Données\n",
        "\n",
        "### Subtask:\n",
        "Créer une classe `TextDataset` pour une gestion plus structurée des données textuelles et de la tokenisation. Modifier la fonction `collate_batch` pour qu'elle soit compatible avec `torch.utils.data.DataLoader`, et intégrer une séparation des données en ensembles d'entraînement, de validation et de test dans `train_demo`. L'objectif est d'assurer un chargement efficace des données et une évaluation fiable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75c5c42f"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `TextDataset` class which will encapsulate the text data and the tokenizer, and implement its `__len__` and `__getitem__` methods as specified in the instructions to prepare for data loading with `DataLoader`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f23e08c6"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset # Import Dataset\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106b9c42"
      },
      "source": [
        "**Reasoning**:\n",
        "I will modify the `collate_batch` function to accept a list of raw text strings and the tokenizer, then encode, pad, and prepare input and target tensors for language modeling, as required by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cae2df6"
      },
      "source": [
        "import torch\n",
        "from typing import List\n",
        "\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    # All tensors should be on the same device before stacking for DataLoader compatibility\n",
        "    # We'll assume the device is passed to train_demo and handled there for now.\n",
        "    # For simplicity in collate_batch, let's keep it device-agnostic, and move to device later.\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1] # All tokens except the last one\n",
        "    targets = padded_batch[:, 1:]  # All tokens except the first one\n",
        "\n",
        "    return inputs, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "308955db"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the `train_demo` function to split the data into training, validation, and test sets, create `TextDataset` and `DataLoader` instances for each, and modify the training loop to use the DataLoader for batching. This will align with the subtask's requirements for refactoring data loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a70713c0"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    corpus_file = \"/tmp/large_corpus.txt\"\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1000:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Split data into train, validation, and test sets\n",
        "    train_texts, test_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "    val_texts, test_texts = train_test_split(test_texts, test_size=0.5, random_state=42) # 0.1 of total\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    batch_size = 8 # Define a batch size\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = 200\n",
        "    if len(train_texts) == 0:\n",
        "        print(\"No training data available after split. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            logits, gates = model(inputs)\n",
        "            b, seq, v = logits.shape\n",
        "            loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step\n",
        "            model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs_val, targets_val in val_loader:\n",
        "                    inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                    logits_val, _ = model(inputs_val)\n",
        "                    b_val, seq_val, v_val = logits_val.shape\n",
        "                    loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                    total_val_loss += loss_val.item()\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs_test, targets_test in test_loader:\n",
        "            inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "            logits_test, _ = model(inputs_test)\n",
        "            b_test, seq_test, v_test = logits_test.shape\n",
        "            loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "            total_test_loss += loss_test.item()\n",
        "            total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7829f656"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `ValueError` occurred because the `train_test_split` function was called with too few samples to create distinct validation and test sets when using the small toy corpus. I will modify the data splitting logic in `train_demo` to handle cases with very few samples gracefully, ensuring that `train_test_split` is called only when there are enough samples and avoiding empty splits. Additionally, I will add checks for `len(loader)` before calculating average losses to prevent `ZeroDivisionError` if a DataLoader is empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40e8f12"
      },
      "source": [
        "# Task\n",
        "The next step is to introduce a configuration system (using a dictionary or a simple class) to manage model hyperparameters, training parameters (learning rate, number of epochs, etc.), and file paths. This will centralize settings and make the model more easily configurable and adaptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db07624d"
      },
      "source": [
        "## Refactorisation du Chargement et Préparation des Données\n",
        "\n",
        "### Subtask:\n",
        "Créer une classe `TextDataset` pour une gestion plus structurée des données textuelles et de la tokenisation. Modifier la fonction `collate_batch` pour qu'elle soit compatible avec `torch.utils.data.DataLoader`, et intégrer une séparation des données en ensembles d'entraînement, de validation et de test dans `train_demo`. L'objectif est d'assurer un chargement efficace des données et une évaluation fiable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a32607dd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `ValueError` occurred because the `train_test_split` function was called with too few samples to create distinct validation and test sets when using the small toy corpus. I will modify the `train_demo` function to robustly handle data splitting, ensuring that `train_test_split` is called only when there are enough samples and avoiding empty splits. Additionally, I will add checks for `len(loader)` before calculating average losses to prevent `ZeroDivisionError` if a DataLoader is empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1938d21"
      },
      "source": [
        "# Task\n",
        "It looks like the previous execution failed due to a `ValueError` in `train_test_split` when the corpus was too small, and there's also a potential for `ZeroDivisionError` if a DataLoader ends up being empty.\n",
        "\n",
        "The next step is to refine the `train_demo` function to make the data splitting more robust, especially for small datasets, and to ensure that loss calculations only occur when DataLoaders contain data. This involves adjusting the `train_test_split` parameters and adding checks before calculating average losses or perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d8ed764"
      },
      "source": [
        "## Refactorisation du Chargement et Préparation des Données\n",
        "\n",
        "### Subtask:\n",
        "Corriger la fonction `train_demo` pour gérer les `ValueError` lors de la division des données (entraînement, validation, test) avec de petits corpus, en s'assurant que les `DataLoader` sont correctement créés et utilisés uniquement s'ils contiennent des données. Cela inclura l'ajustement des appels à `train_test_split` et les calculs de perte moyenne pour éviter les erreurs de division par zéro.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380f03af"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `ValueError` occurred because `train_test_split` was called with too few samples to create distinct validation and test sets. I will modify the `train_demo` function to include robust data splitting logic, handling cases with small corpora by assigning all data to the training set and ensuring `val_texts` and `test_texts` are empty. I will also add checks for empty `DataLoader` instances before calculating average losses to prevent `ZeroDivisionError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "132ccd1f"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    corpus_file = \"/tmp/large_corpus.txt\"\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1000:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop())\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        # Calculate test_size for validation set: 0.1 / (1-0.2) = 0.125\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = 0.1 / (1.0 - 0.2)\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=42)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop())\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    batch_size = 8 # Define a batch size\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = 200\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "377d3ab7"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a `Config` class to centralize all hyperparameters and settings, making the model more configurable. This class will hold model parameters, training parameters, and file paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da96bfbc"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 1000 # Max lines to load from corpus_file for demo\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4c2aa05"
      },
      "source": [
        "### Déploiement du modèle pour l'inférence\n",
        "\n",
        "Pour déployer le modèle et générer du texte, suivez ces étapes :\n",
        "1.  Chargez le modèle, le tokenizer et la configuration à l'aide de la fonction `load_model_and_tokenizer`.\n",
        "2.  Utilisez la fonction `deploy_model` avec votre texte d'entrée (`input_text`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23f115e9"
      },
      "source": [
        "# Initialize configuration (to get paths)\n",
        "cfg = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Load the trained model, tokenizer, and configuration\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg.model_save_dir, \"best_model.pt\"),\n",
        "        cfg.tokenizer_save_dir,\n",
        "        cfg.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Define your input text for generation\n",
        "    input_text = \"Colab is a great tool for\"\n",
        "\n",
        "    # Deploy the model to generate text\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, input_text)\n",
        "\n",
        "    print(f\"Input Text: {input_text}\")\n",
        "    print(f\"Generated Output: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96fe4aaf"
      },
      "source": [
        "## Implémentation de la Sauvegarde et du Chargement du Modèle\n",
        "\n",
        "### Subtask:\n",
        "Ajouter des fonctionnalités dans train_demo pour sauvegarder les poids du meilleur modèle entraîné (basé sur une métrique comme la perte de validation ou la perplexité) et le tokenizer associé. Développer également une fonction distincte pour charger un modèle et son tokenizer pré-entraînés, essentielle pour l'inférence en production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0387bfd"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define `model_save_dir` and `tokenizer_save_dir` in the `Config` class to specify where the model and tokenizer should be saved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e6ca383"
      },
      "source": [
        "# Task\n",
        "**Task:** Download the \"wikitext-2-raw-v1\" dataset from Hugging Face, extract the text, and save the first 10,000 lines into the `/tmp/large_corpus.txt` file. Then, modify the `Config` class to set `max_corpus_lines` to 10,000 to utilize this larger corpus for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b617c662"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dN3rV1JShnLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110d6a9f"
      },
      "source": [
        "## Rechercher et Télécharger un Corpus de Texte\n",
        "\n",
        "### Subtask:\n",
        "Download the 'wikitext-2-raw-v1' dataset from Hugging Face, extract the text, and save the first 10,000 lines into the `/tmp/large_corpus.txt` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff583a2b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires downloading a dataset from Hugging Face, which needs the `datasets` library. Therefore, the first step is to install this library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96420ed7"
      },
      "source": [
        "pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad024e6a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `datasets` library is confirmed to be installed, I will proceed to download the 'wikitext-2-raw-v1' dataset, extract its training text, filter out empty lines, and save the first 10,000 non-empty lines to `/tmp/large_corpus.txt` as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c95cef14"
      },
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Define the output file path\n",
        "corpus_file_path = \"/tmp/large_corpus.txt\"\n",
        "\n",
        "# Load the 'wikitext-2-raw-v1' dataset\n",
        "print(\"Loading wikitext-2-raw-v1 dataset from Hugging Face...\")\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "\n",
        "# Access the 'train' split and extract 'text' column\n",
        "train_texts = dataset['train']['text']\n",
        "\n",
        "# Filter out empty strings and strip whitespace\n",
        "filtered_texts = [line.strip() for line in train_texts if line.strip()]\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(corpus_file_path), exist_ok=True)\n",
        "\n",
        "# Write the first 10,000 lines to the specified file\n",
        "print(f\"Writing first 10,000 lines to {corpus_file_path}...\")\n",
        "with open(corpus_file_path, 'w', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(filtered_texts):\n",
        "        if i < 10000:\n",
        "            f.write(line + '\\n')\n",
        "        else:\n",
        "            break\n",
        "print(\"Dataset extraction and saving complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af73b0b4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask required saving the first 10,000 lines of the dataset to `/tmp/large_corpus.txt`. Now that this file is created, I need to update the `Config` class to reflect this by setting `max_corpus_lines` to 10,000. This will ensure that the `train_demo` function, when called later, correctly loads the larger corpus for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6cfc04d",
        "outputId": "fc3ebf71-e3ed-44b0-9238-78589dd960b5"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading corpus from /tmp/large_corpus.txt...\n",
            "Loaded 10000 lines.\n",
            "Train size: 4000, Validation size: 4000, Test size: 2000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 — Avg Train Loss: 7.2199\n",
            "Current learning rate: 0.001000\n",
            "Validation Loss: 6.8119\n",
            "Saving best model with validation loss: 6.8119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "msuCpWIVjITu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3184c9f8"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Créez une nouvelle instance de Config et modifiez max_new_tokens\n",
        "    short_generation_config = Config()\n",
        "    short_generation_config.max_new_tokens = 5 # Set to a shorter value\n",
        "\n",
        "    # Définir le texte d'entrée pour la génération\n",
        "    input_text = \"Le modèle linguistique apprend\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte en utilisant la configuration modifiée\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, short_generation_config, device, input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {input_text}\")\n",
        "    print(f\"Texte généré (5 tokens): {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07TOZBzZcgKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "310a455c"
      },
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération\n",
        "    new_input_text = \"L'intelligence artificielle est\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, new_input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {new_input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5373c1d"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Assuming Config class is available in the environment or can be re-imported if necessary.\n",
        "# For a standalone check, we can re-define a minimal Config class or instantiate the one from the previous cell.\n",
        "# Given it's a colab environment, we can assume the Config class is already defined.\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "model_path = os.path.join(cfg.model_save_dir, \"best_model.pt\")\n",
        "tokenizer_path = cfg.tokenizer_save_dir\n",
        "config_path = cfg.config_save_path\n",
        "\n",
        "print(\"Vérification des fichiers sauvegardés :\")\n",
        "\n",
        "# Check for model file\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Le fichier du modèle a été trouvé à : {model_path}\")\n",
        "else:\n",
        "    print(f\"Le fichier du modèle n'a PAS été trouvé à : {model_path}\")\n",
        "\n",
        "# Check for tokenizer directory\n",
        "if os.path.isdir(tokenizer_path):\n",
        "    print(f\"Le répertoire du tokenizer a été trouvé à : {tokenizer_path}\")\n",
        "else:\n",
        "    print(f\"Le répertoire du tokenizer n'a PAS été trouvé à : {tokenizer_path}\")\n",
        "\n",
        "# Check for config file\n",
        "if os.path.exists(config_path):\n",
        "    print(f\"Le fichier de configuration a été trouvé à : {config_path}\")\n",
        "else:\n",
        "    print(f\"Le fichier de configuration n'a PAS été trouvé à : {config_path}\")\n",
        "\n",
        "# Additionally, try to load a small part of the config to ensure it's readable\n",
        "try:\n",
        "    if os.path.exists(config_path):\n",
        "        # Need to add Config to safe_globals for unpickling if not already added\n",
        "        if 'Config' not in torch.serialization.get_safe_globals():\n",
        "            torch.serialization.add_safe_globals([Config])\n",
        "        loaded_cfg = torch.load(config_path)\n",
        "        print(f\"La configuration a été chargée avec succès. emb_dim : {loaded_cfg.emb_dim}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement de la configuration : {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31fe820d"
      },
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le texte d'entrée pour la génération\n",
        "    input_text = \"Le modèle linguistique apprend\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0d7492"
      },
      "source": [
        "## Réexécuter l'Entraînement\n",
        "\n",
        "### Subtask:\n",
        "Réexécuter la fonction `train_demo` avec le nouveau corpus. Cela permettra de tester si le modèle bénéficie d'un jeu de données plus riche.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0d72277"
      },
      "source": [
        "**Reasoning**:\n",
        "I will execute the `train_demo()` function, which now uses the updated `Config` class pointing to the larger corpus, to initiate the training process and observe its performance with the richer dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7051c10"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k-sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance AND max_seq_len\n",
        "def collate_batch(batch_texts: List[str], tokenizer, max_seq_len: int):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        # Truncate if longer than max_seq_len, ensuring EOS is present\n",
        "        if len(ids) > max_seq_len:\n",
        "            ids = ids[:max_seq_len - 1] + [tokenizer.eos_token_id] if max_seq_len > 1 else ids[:max_seq_len]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max_len within the batch, which will be at most max_seq_len due to truncation\n",
        "    max_len_in_batch = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len_in_batch), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699278c5"
      },
      "source": [
        "**Reasoning**:\n",
        "I will execute the provided code block which contains the entire model definition and the `train_demo()` function. This will re-run the training and evaluation process, now utilizing the larger corpus as configured in the `Config` class, and display the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fec2c02"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27893386"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to execute the `train_demo()` function again. The provided code block in the user's prompt contains the complete and updated Python script, including all necessary class definitions and the `train_demo()` function call within the `if __name__ == \"__main__\":` block. Executing this will re-run the training process with the larger corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3fd01f9"
      },
      "source": [
        "# Task\n",
        "Ajouter un attribut `max_seq_len` à la classe `Config` avec une valeur par défaut de 512 et modifier la fonction `collate_batch` pour tronquer les séquences à cette longueur maximale lors de l'encodage des textes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dc09332"
      },
      "source": [
        "## Ajouter max_seq_len à Config\n",
        "\n",
        "### Subtask:\n",
        "Ajouter un attribut `max_seq_len` à la classe `Config` avec une valeur par défaut de 512 (ou une autre valeur appropriée basée sur les contraintes du modèle).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b2ee18b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the `Config` class to include `max_seq_len` as requested by the subtask. This centralizes the sequence length setting, making it easier to manage model constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8a62c8e"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 1000 # Max lines to load from corpus_file for demo\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}