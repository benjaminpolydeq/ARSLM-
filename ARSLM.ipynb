{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminpolydeq/ARSLM/blob/main/ARSLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz-isu3wtlXJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "308d0376-04fa-4eaf-cc01-077f222a157a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-2882163312.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2882163312.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Traceback (most recent call last):\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd0d942e"
      },
      "source": [
        "Pour pousser vos modifications sur GitHub, assurez-vous que vous êtes bien dans le répertoire de votre dépôt local (`ARSLM` si vous avez cloné le dépôt précédemment). Ensuite, vous pouvez utiliser les commandes Git suivantes. Vous devrez peut-être entrer votre nom d'utilisateur et votre mot de passe/token d'accès personnel GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ee7df88"
      },
      "source": [
        "import os\n",
        "\n",
        "# Assurez-vous d'être dans le répertoire 'ARSLM'\n",
        "# Si vous n'êtes pas sûr, vous pouvez ajouter cette ligne pour vous y déplacer:\n",
        "# os.chdir('ARSLM')\n",
        "\n",
        "# Affiche le répertoire de travail actuel pour confirmation\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19c5b8fc"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Configurez votre nom d'utilisateur et votre email Git si ce n'est pas déjà fait\n",
        "# git config --global user.email \"votre_email@example.com\"\n",
        "# git config --global user.name \"Votre Nom\"\n",
        "\n",
        "# Ajoutez les fichiers modifiés ou nouveaux. Par exemple, pour tout ajouter:\n",
        "git add .\n",
        "\n",
        "# Committez les changements\n",
        "git commit -m \"Mise à jour du modèle ARSLM et configuration\"\n",
        "\n",
        "# Poussez les changements vers la branche distante (souvent 'main' ou 'master')\n",
        "git push origin main\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4028d520",
        "outputId": "58ab02c5-4ee5-4913-ce85-677ffd0a7f77"
      },
      "source": [
        "import sys\n",
        "if 'torch' not in sys.modules:\n",
        "    !pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkc22wJgSN2H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d39c8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b8fbb973-d49b-48dd-bb91-c0765eec32f2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2491009384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mARSCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mARSCell\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomputes\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mh_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mImplements\u001b[0m \u001b[0man\u001b[0m \u001b[0;34m'adapt'\u001b[0m \u001b[0mmechanism\u001b[0m \u001b[0minspired\u001b[0m \u001b[0mby\u001b[0m \u001b[0mBenPolySeq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mh_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McJITnyBulIr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5e86f90"
      },
      "source": [
        "## Explication détaillée du modèle ARSLM\n",
        "\n",
        "Le modèle ARSLM (Adaptive Recurrent State Language Model) est un prototype de modèle linguistique basé sur le concept d'états récurrents adaptatifs (ARS), inspiré par les séquences adaptatives BenPolySeq. Il est conçu pour être léger, explicable et extensible, ce qui le rend adapté à la recherche et à l'expérimentation.\n",
        "\n",
        "Voici une décomposition de ses composants clés :\n",
        "\n",
        "### 1. Tokenizer Avancé (`AdvancedTokenizer`)\n",
        "\n",
        "Ce composant gère la conversion du texte en séquences d'identifiants (tokens) et vice-versa. Dans cette version améliorée, il utilise la bibliothèque `transformers` de Hugging Face, spécifiquement le `BertTokenizer`.\n",
        "\n",
        "*   **Pourquoi un tokenizer avancé ?** Contrairement à un simple tokenizer qui pourrait se contenter de diviser le texte par espaces et de gérer un vocabulaire limité, un tokenizer basé sur les sous-mots (comme ceux utilisés par BERT) permet de :\n",
        "    *   Mieux gérer les mots inconnus en les décomposant en sous-unités (par exemple, \"tokenization\" pourrait être décomposé en \"token\", \"iza\", \"tion\").\n",
        "    *   Réduire la taille globale du vocabulaire tout en représentant une plus grande variété de mots.\n",
        "    *   Gérer la ponctuation, les majuscules/minuscules et d'autres subtilités linguistiques.\n",
        "*   **Fonctionnement :** L'`AdvancedTokenizer` utilise un vocabulaire pré-entraîné (ici, celui de `bert-base-uncased` par défaut). Il fournit des méthodes `encode` pour convertir le texte en identifiants et `decode` pour convertir les identifiants en texte. Il définit également des identifiants spéciaux pour le padding (`<pad>`), les mots inconnus (`<unk>`), le début de séquence (`<bos>`, utilisant `[CLS]` de BERT) et la fin de séquence (`<eos>`, utilisant `[SEP]` de BERT).\n",
        "\n",
        "### 2. Cellule ARS (`ARSCell`)\n",
        "\n",
        "C'est le cœur adaptatif du modèle. Une `ARSCell` calcule le prochain état caché (`h_t`) en se basant sur les deux états cachés précédents (`h_{t-2}`, `h_{t-1}`) et l'embedding de l'entrée courante (`x_embed`).\n",
        "\n",
        "*   **Mécanisme d'Adaptation :** L'idée principale, inspirée par BenPolySeq, est que la mise à jour de l'état caché n'est pas une simple combinaison linéaire ou une porte fixe (comme dans les RNN ou LSTM basiques), mais une addition adaptative d'un \"candidat\" pondéré par un \"gate\".\n",
        "    *   **Signal de Différence :** La cellule utilise la différence entre les deux états cachés précédents (`h_{t-1} - h_{t-2}`). Ce signal est censé capturer l'évolution ou la dynamique de l'état récurrent.\n",
        "    *   **Contexte :** Un vecteur de contexte est créé en concaténant les deux états cachés précédents et l'embedding d'entrée (`[h_{t-1}, h_{t-2}, x_embed]`).\n",
        "    *   **Réseau Candidat (`candidate_mlp`) :** Un MLP (réseau de neurones multi-couches) prend le contexte en entrée et génère un \"candidat\" pour la mise à jour de l'état caché.\n",
        "    *   **Réseau de Porte (`gate_net`) :** Un autre réseau (comprenant une fonction sigmoïde à la fin) prend également le contexte en entrée et produit une valeur scalaire entre 0 et 1. Cette valeur agit comme un \"gate\" qui contrôle l'influence du candidat sur la mise à jour de l'état caché.\n",
        "    *   **Résiduel :** Un petit résiduel de l'embedding d'entrée est ajouté pour aider à propager directement l'information de l'entrée.\n",
        "    *   **Mise à Jour :** L'état caché suivant est calculé comme `h_t = h_{t-1} + gate * candidate + 0.1 * residual`. Cette formule montre comment la cellule *adapte* sa mise à jour en fonction du signal de différence et du contexte via le gate.\n",
        "    *   **Dropout et Normalisation :** Une couche de dropout est appliquée pour la régularisation, et une normalisation de couche (`F.layer_norm`) est utilisée pour stabiliser l'entraînement.\n",
        "\n",
        "### 3. Modèle ARSLM (`ARSLM`)\n",
        "\n",
        "Ce module encapsule l'ensemble de l'architecture, combinant l'embedding, plusieurs couches d'`ARSCell`, un mécanisme d'attention et une couche de sortie (head).\n",
        "\n",
        "*   **Embedding (`nn.Embedding`) :** Convertit les identifiants de tokens en vecteurs denses de dimension `emb_dim`.\n",
        "*   **Couches d'ARSCell (`nn.ModuleList`) :** Le modèle utilise une liste de `num_layers` ARSCells empilées. L'entrée de la première cellule est l'embedding, et l'entrée des cellules subséquentes est l'état caché de la couche précédente.\n",
        "*   **Mécanisme d'Attention :** Un mécanisme d'attention additive simple est appliqué après la dernière couche d'ARSCell.\n",
        "    *   **Attention Causale :** Pendant la passe avant (forward), l'attention est causale, ce qui signifie que chaque token ne peut prêter attention qu'aux tokens précédents dans la séquence.\n",
        "    *   **Calcul :** Pour chaque pas temporel `t`, l'état caché de la dernière couche (`last_layer_h_t`) sert de requête. L'attention est calculée sur l'historique de tous les états cachés de la dernière couche jusqu'au pas `t` (`all_last_layer_hidden_states`). Les scores d'attention sont calculés via un petit réseau (`self.attention`), puis normalisés avec un softmax. Un vecteur de contexte est obtenu en pondérant l'historique par les poids d'attention.\n",
        "    *   **Combinaison :** Le vecteur de contexte est ajouté à l'état caché courant de la dernière couche (`attended_h_t = last_layer_h_t + context_vector`). Cette combinaison permet au modèle de tirer parti des informations pertinentes de la séquence passée.\n",
        "*   **Couche de Sortie (`head`) :** Une couche linéaire qui prend l'état caché combiné (avec attention) de la dernière couche et le projette sur la taille du vocabulaire (`self.vocab_size`). Les sorties de cette couche sont les logits, qui représentent la probabilité non normalisée de chaque token dans le vocabulaire d'être le prochain token.\n",
        "\n",
        "### 4. Fonction `forward`\n",
        "\n",
        "Cette fonction décrit comment une séquence d'identifiants (`input_ids`) est traitée par le modèle pour produire les logits et les gates. Elle itère sur chaque pas temporel de la séquence d'entrée, calculant séquentiellement les états cachés pour chaque couche d'ARSCell et appliquant l'attention et la couche de sortie à chaque pas après la dernière couche.\n",
        "\n",
        "### 5. Fonction `generate`\n",
        "\n",
        "Cette fonction implémente le processus de génération de texte. À partir d'une séquence de contexte initiale (`idx`), le modèle génère séquentiellement de nouveaux tokens. Pour chaque nouveau token à générer :\n",
        "*   Il prend l'embedding du dernier token généré.\n",
        "*   Il utilise les états cachés mis à jour des étapes précédentes pour calculer le nouvel état caché pour chaque couche d'ARSCell.\n",
        "*   Il applique le mécanisme d'attention sur l'historique des états cachés de la dernière couche (incluant le nouvel état).\n",
        "*   Il utilise la couche de sortie pour obtenir les logits pour le prochain token.\n",
        "*   Il applique un échantillonnage (ici, `torch.multinomial` avec une température et optionnellement `top_k`) pour sélectionner le prochain token en fonction des probabilités calculées à partir des logits.\n",
        "*   Le nouveau token est ajouté à la séquence de sortie, et le processus se répète jusqu'à ce que le nombre maximal de nouveaux tokens soit atteint.\n",
        "\n",
        "### 6. Utilitaires d'Entraînement/Démo (`collate_batch`, `train_demo`)\n",
        "\n",
        "*   **`collate_batch` :** Prépare les données texte pour l'entraînement en les encodant à l'aide du tokenizer, en ajoutant les tokens de début/fin de séquence et en effectuant le padding pour que toutes les séquences d'un batch aient la même longueur.\n",
        "*   **`train_demo` :** Une fonction de démonstration qui initialise le tokenizer, le modèle, l'optimiseur et la fonction de perte. Elle charge un petit corpus (ou un fichier si spécifié), prépare les données, entraîne le modèle sur un nombre spécifié d'époques, imprime la perte et le taux d'apprentissage, calcule la perplexité et effectue une petite démo de génération de texte.\n",
        "\n",
        "En résumé, l'ARSLM combine l'idée d'états récurrents adaptatifs avec une architecture multi-couches et un mécanisme d'attention causale pour créer un modèle linguistique capable d'apprendre des dépendances séquentielles de manière flexible. L'aspect \"adaptatif\" via le mécanisme de gate dans l'ARSCell est la caractéristique distinctive, permettant potentiellement au modèle d'ajuster son comportement en fonction de la dynamique interne de sa mémoire récurrente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a1548d2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'torch' not in sys.modules:\n",
        "    !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79ae78ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "\n",
        "# Perform an operation\n",
        "y = torch.rand(5, 3)\n",
        "print(x + y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90f2b52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "16f97fad-dcc0-4073-f34e-6a430e92b489"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'random' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-449805765.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "\n",
        "# Assuming AdvancedTokenizer and ARSCell are defined or imported elsewhere\n",
        "# For a complete working script, please refer to cell 64161967\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers and attention during generation.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# Training / demo utilities\n",
        "# ----------------------------\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # toy corpus\n",
        "    texts = [\n",
        "        \"hello world this is ars\",\n",
        "        \"the system adapts to its history\",\n",
        "        \"benpolyseq demonstrates adaptive sequences\",\n",
        "        \"ars can inspire new network protocols\",\n",
        "        \"self optimizing systems are possible\"\n",
        "    ]\n",
        "    # Instantiate AdvancedTokenizer\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    # Vocab size is now determined by the tokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "    # Instantiate ARSLM with multiple layers\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device) # Added num_layers\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    # Use ignore_index from the tokenizer\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # create training batches by shifting: predict next token at each position\n",
        "    n_epochs = 200\n",
        "    batch = collate_batch(tokenizer, texts, device) # Pass tokenizer to collate_batch\n",
        "    # targets: same as input (language modeling)\n",
        "    inputs = batch[:, :-1]\n",
        "    targets = batch[:, 1:]\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        logits, gates = model(inputs)\n",
        "        # flatten\n",
        "        b, seq, v = logits.shape\n",
        "        loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — loss: {loss.item():.4f}\")\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    # Use the tokenizer's encode method for the context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0)[0]\n",
        "    print(\"=== Generated ===\")\n",
        "    # Use the tokenizer's decode method for the output\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSy191NO-PhB"
      },
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9kQ9sD-PhD"
      },
      "outputs": [],
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64161967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "00dc90e7a46044928a78bb952f97835e",
            "6e152e0b49b9408ca6f91419aa0a03d3",
            "504009292b0940c2aaa8351e0127067f",
            "17b2dfc3893a475aba1aa6fbcf729626",
            "f3e4122ba9bb4c308a549fb5e3255e1e",
            "c63f8c63442d456e99e25cf2ac5c7d68",
            "5942670f889a4e81877658144b59b943",
            "892f9bb60bfb4c548d7a0f0a8202634a",
            "1afd6d0456e7492594f673fc5d7671fc",
            "b4f92aebc939447aae6dd6f17212d5c9",
            "cdbd186fba1f4fd8822080f07d1691f0",
            "04588a42758b478ab588a5f7b05e3962",
            "6e56d83fc7084793a7934ed37768fd9f",
            "6ed8350e4ab048b4800641532581ef29",
            "c111558e350047699b84454d6585384c",
            "9c4c60e5f78b4c19a3ff136bcb9aa2ff",
            "2aba23ffcf4d4a1192640162f281d760",
            "8e7f9997f4384345963791c6e6097708",
            "dc13c85f293d40f69f789cd48ef68293",
            "20d81595143b4f4eab8d9c0e53d7c3c5",
            "8e966353e7d7414ea015eb34b6d734f1",
            "a9fb3cdd5da34dfaae33102414e8d855",
            "85beaf1e00fe4386afaf1fc25b49a98d",
            "3f16723ae3ba44e48d4fac39426ca130",
            "2e1ffc1efa7542578d56c37ab9badf6a",
            "570e930da9c243ad9d53322adb0167c5",
            "448ed3ce35cf4a9e8071d40c63fb36c9",
            "af107835680e4ef3a397d3bd877b4224",
            "04f74b40467046f6b975a53812d14e93",
            "dd27f5a94b48445b85d1cae5c2cfec88",
            "cabcb2c74b694d07843e3947fa978bde",
            "8af1ca70ed514f9ba038ec12af263aa1",
            "931a7e31a78146dca1ccd5098570f1be",
            "d768cef1b6d44570a50162035125c2bd",
            "3ec138153f574553a692cdc18a042129",
            "365f2783d9b240ae8dbf9a589bf5f89f",
            "8822fa8bb00a46ea864670d0b00adc8d",
            "11568452fd0d4647933aa1e6e13c055d",
            "e000eb3c5fac4eeb917284ea603cfe6d",
            "0f803f6c67c449e59b2b2d0468ccb792",
            "0754ff5eacf7408f85480c5fe2228b7e",
            "1ef7d51cdabc46e6bb8fb9680f31ea0d",
            "83dca4b9a2d84d2c97a5a1d83bda4df4",
            "53b21829dc0c4ff892cf0d61189e2784"
          ]
        },
        "outputId": "160b843c-8333-4e58-a27e-5610631ece2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00dc90e7a46044928a78bb952f97835e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04588a42758b478ab588a5f7b05e3962"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85beaf1e00fe4386afaf1fc25b49a98d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d768cef1b6d44570a50162035125c2bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200 — loss: 10.9243\n",
            "Current learning rate: 0.001000\n",
            "Epoch 50/200 — loss: 0.8989\n",
            "Current learning rate: 0.000500\n",
            "Epoch 100/200 — loss: 0.3279\n",
            "Current learning rate: 0.000250\n",
            "Epoch 150/200 — loss: 0.2738\n",
            "Current learning rate: 0.000125\n",
            "Epoch 200/200 — loss: 0.2434\n",
            "Current learning rate: 0.000063\n",
            "=== Generated ===\n",
            "hello world is ars\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# Training / demo utilities\n",
        "# ----------------------------\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # toy corpus\n",
        "    texts = [\n",
        "        \"hello world this is ars\",\n",
        "        \"the system adapts to its history\",\n",
        "        \"benpolyseq demonstrates adaptive sequences\",\n",
        "        \"ars can inspire new network protocols\",\n",
        "        \"self optimizing systems are possible\"\n",
        "    ]\n",
        "    # Instantiate AdvancedTokenizer\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    # Vocab size is now determined by the tokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "    # Instantiate ARSLM with multiple layers\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device) # Added num_layers\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    # Add a StepLR scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5) # Reduce LR by half every 50 epochs\n",
        "    # Use ignore_index from the tokenizer\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # create training batches by shifting: predict next token at each position\n",
        "    n_epochs = 200\n",
        "    batch = collate_batch(tokenizer, texts, device) # Pass tokenizer to collate_batch\n",
        "    # targets: same as input (language modeling)\n",
        "    inputs = batch[:, :-1]\n",
        "    targets = batch[:, 1:]\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        logits, gates = model(inputs)\n",
        "        # flatten\n",
        "        b, seq, v = logits.shape\n",
        "        loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — loss: {loss.item():.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\") # Print current LR\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    # Use the tokenizer's encode method for the context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0]\n",
        "    print(\"=== Generated ===\")\n",
        "    # Use the tokenizer's decode method for the output\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edb16161"
      },
      "outputs": [],
      "source": [
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d76b4e22"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `transformers` library is installed, I will import a suitable tokenizer, such as `BertTokenizer`, and create a new class that wraps this tokenizer to maintain compatibility with the existing code's `encode` and `decode` methods. I will also define the special tokens needed for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af98d291"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from typing import List\n",
        "\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# Example usage:\n",
        "# tokenizer = AdvancedTokenizer()\n",
        "# text = \"Hello, world! This is a test sentence.\"\n",
        "# encoded_text = tokenizer.encode(text)\n",
        "# print(\"Encoded:\", encoded_text)\n",
        "# decoded_text = tokenizer.decode(encoded_text)\n",
        "# print(\"Decoded:\", decoded_text)\n",
        "# print(\"Vocab size:\", len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ced6a3b5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install a more robust tokenizer library. Hugging Face's `transformers` library is a good choice as it provides access to many pre-trained tokenizers including those using sub-word techniques. The `tokenizers` library is also a good choice, but `transformers` is more commonly used and includes the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b8abc8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4d5d9b-fe05-444c-acab-a6df206b6228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "if 'transformers' not in sys.modules:\n",
        "    !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FAjGRVduH7B"
      },
      "source": [
        "# Nouvelle section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232fbd36"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération\n",
        "    new_input_text = \"La technologie moderne a\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, new_input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {new_input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR9ngUy7ug-0"
      },
      "outputs": [],
      "source": [
        "pimport math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "\n",
        "# ----------------------------\n",
        "# Simple tokenizer / vocab (to be replaced)\n",
        "# ----------------------------\n",
        "# class SimpleTokenizer:\n",
        "#     def __init__(self, texts: List[str], min_freq=1):\n",
        "#         tokens = []\n",
        "#         for t in texts:\n",
        "#             tokens += t.strip().split()\n",
        "#         freq = {}\n",
        "#         for w in tokens:\n",
        "#             freq[w] = freq.get(w, 0) + 1\n",
        "#         self.vocab = ['<pad>', '<unk>', '<bos>', '<eos>']\n",
        "#         for w, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])):\n",
        "#             if c >= min_freq and w not in self.vocab:\n",
        "#                 self.vocab.append(w)\n",
        "#         self.stoi = {w:i for i,w in enumerate(self.vocab)}\n",
        "#         self.itos = {i:w for w,i in self.stoi.items()}\n",
        "\n",
        "#     def encode(self, text: str) -> List[int]:\n",
        "#         toks = text.strip().split()\n",
        "#         ids = [self.stoi.get(t, self.stoi['<unk>']) for t in toks]\n",
        "#         return ids\n",
        "\n",
        "#     def decode(self, ids: List[int]) -> str:\n",
        "#         tokens = [self.itos.get(i, '<unk>') for i in ids]\n",
        "#         return ' '.join(tokens)\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# Training / demo utilities\n",
        "# ----------------------------\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(tokenizer, texts: List[str], device):\n",
        "    # encode, add bos/eos\n",
        "    encoded = []\n",
        "    for t in texts:\n",
        "        # Use the tokenizer's encode method and add special tokens if needed\n",
        "        # BertTokenizer.encode already handles [CLS] (BOS) and [SEP] (EOS)\n",
        "        # We might need to adjust based on whether the model expects them\n",
        "        # For now, let's use the encode method without adding special tokens\n",
        "        # and add them manually if the model architecture requires it explicitly.\n",
        "        # However, for BERT-like models, the special tokens are typically added by encode_plus or encode.\n",
        "        # Let's stick to encode for now and add BOS/EOS manually if necessary for the ARSLM structure.\n",
        "        # Given the original SimpleTokenizer added BOS/EOS, let's do the same here.\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
        "    # pad to max len\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long, device=device) # Use pad_token_id\n",
        "    for i, x in enumerate(encoded):\n",
        "        batch[i, :x.size(0)] = x\n",
        "    return batch\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # toy corpus\n",
        "    # texts = [\n",
        "    #     \"hello world this is ars\",\n",
        "    #     \"the system adapts to its history\",\n",
        "    #     \"benpolyseq demonstrates adaptive sequences\",\n",
        "    #     \"ars can inspire new network protocols\",\n",
        "    #     \"self optimizing systems are possible\"\n",
        "    # ]\n",
        "    # --- Load text from a file (example) ---\n",
        "    corpus_file = \"/tmp/large_corpus.txt\" # Replace with your corpus file path\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            # Read a limited number of lines for demo, for full training, iterate or use data loaders\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1000: # Load first 1000 lines as an example\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "\n",
        "    # Instantiate AdvancedTokenizer\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    # Vocab size is now determined by the tokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "    # Instantiate ARSLM with multiple layers\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device) # Added num_layers\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    # Add a StepLR scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5) # Reduce LR by half every 50 epochs\n",
        "    # Use ignore_index from the tokenizer\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    # create training batches by shifting: predict next token at each position\n",
        "    n_epochs = 200\n",
        "    batch = collate_batch(tokenizer, texts, device) # Pass tokenizer to collate_batch\n",
        "    # targets: same as input (language modeling)\n",
        "    inputs = batch[:, :-1]\n",
        "    targets = batch[:, 1:]\n",
        "\n",
        "    # Simple check if batch is empty due to no text loaded\n",
        "    if inputs.numel() == 0:\n",
        "        print(\"No training data loaded. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        logits, gates = model(inputs)\n",
        "        # flatten\n",
        "        b, seq, v = logits.shape\n",
        "        loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — loss: {loss.item():.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\") # Print current LR\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(inputs) # Use the same inputs for simplicity in this demo\n",
        "        # Calculate perplexity\n",
        "        # Perplexity = exp(average negative log-likelihood)\n",
        "        loss = loss_fn(logits.view(-1, v), targets.reshape(-1))\n",
        "        perplexity = torch.exp(loss)\n",
        "        print(f\"Perplexity on training data: {perplexity.item():.4f}\")\n",
        "\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    # Use the tokenizer's encode method for the context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    # Use top_k sampling during generation\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0] # Added top_k=50\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    # Use the tokenizer's decode method for the output\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGCoOvTf9Khu"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# ARSLM_LoRA Tout-en-un - Google Colab\n",
        "# =========================\n",
        "\n",
        "# Installer les dépendances\n",
        "!pip install torch transformers datasets accelerate peft streamlit pyngrok --quiet\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# ----------------------------\n",
        "# 1️⃣ Configuration\n",
        "# ----------------------------\n",
        "class CFG:\n",
        "    repo_url = \"https://github.com/benjaminpolydeq/ARSLM.git\"\n",
        "        model_name = \"distilgpt2\"\n",
        "            max_seq_len = 128\n",
        "                batch_size = 4\n",
        "                    gradient_accumulation_steps = 4\n",
        "                        epochs = 10\n",
        "                            lr = 5e-5\n",
        "                                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                                    checkpoint_dir = \"checkpoints\"\n",
        "                                        early_stopping_patience = 3\n",
        "                                            max_gen_len = 50\n",
        "                                            cfg = CFG()\n",
        "                                            os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "                                            # ----------------------------\n",
        "                                            # 2️⃣ Cloner dépôt ARSLM\n",
        "                                            # ----------------------------\n",
        "                                            if not os.path.exists(\"ARSLM\"):\n",
        "                                                !git clone {cfg.repo_url}\n",
        "\n",
        "                                                os.chdir(\"ARSLM\")  # Entrer dans le projet\n",
        "\n",
        "                                                # ----------------------------\n",
        "                                                # 3️⃣ Préparer dataset demo\n",
        "                                                # ----------------------------\n",
        "                                                dataset = [\n",
        "                                                    {\"input\": \"Hello, how are you?\", \"target\": \"I'm fine, thanks!\"},\n",
        "                                                        {\"input\": \"What is AI?\", \"target\": \"AI stands for Artificial Intelligence.\"},\n",
        "                                                            {\"input\": \"Define machine learning.\", \"target\": \"Machine learning is a subset of AI.\"},\n",
        "                                                            ]\n",
        "\n",
        "                                                            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "\n",
        "                                                            def collate_batch(batch):\n",
        "                                                                input_ids = []\n",
        "                                                                    labels = []\n",
        "                                                                        for item in batch:\n",
        "                                                                                ids = tokenizer(item[\"input\"], truncation=True, max_length=cfg.max_seq_len, return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
        "                                                                                        target_ids = tokenizer(item[\"target\"], truncation=True, max_length=cfg.max_seq_len, return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
        "                                                                                                input_ids.append(ids)\n",
        "                                                                                                        labels.append(target_ids)\n",
        "                                                                                                            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "                                                                                                                labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "                                                                                                                    return input_ids, labels\n",
        "\n",
        "                                                                                                                    dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "                                                                                                                    # ----------------------------\n",
        "                                                                                                                    # 4️⃣ Charger modèle et appliquer LoRA\n",
        "                                                                                                                    # ----------------------------\n",
        "                                                                                                                    base_model = AutoModelForCausalLM.from_pretrained(cfg.model_name).to(cfg.device)\n",
        "\n",
        "                                                                                                                    lora_config = LoraConfig(\n",
        "                                                                                                                        task_type=TaskType.CAUSAL_LM,\n",
        "                                                                                                                            r=8,\n",
        "                                                                                                                                lora_alpha=32,\n",
        "                                                                                                                                    lora_dropout=0.1,\n",
        "                                                                                                                                        bias=\"none\"\n",
        "                                                                                                                                        )\n",
        "                                                                                                                                        model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "                                                                                                                                        optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
        "                                                                                                                                        scaler = torch.cuda.amp.GradScaler()  # Mixed precision\n",
        "\n",
        "                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                        # 5️⃣ Early stopping setup\n",
        "                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                        best_val_loss = float(\"inf\")\n",
        "                                                                                                                                        patience_counter = 0\n",
        "\n",
        "                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                        # 6️⃣ Training loop\n",
        "                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                        for epoch in range(cfg.epochs):\n",
        "                                                                                                                                            model.train()\n",
        "                                                                                                                                                running_loss = 0\n",
        "                                                                                                                                                    optimizer.zero_grad()\n",
        "\n",
        "                                                                                                                                                            for step, (inputs, labels) in enumerate(dataloader):\n",
        "                                                                                                                                                                    inputs, labels = inputs.to(cfg.device), labels.to(cfg.device)\n",
        "                                                                                                                                                                            with torch.cuda.amp.autocast():\n",
        "                                                                                                                                                                                        outputs = model(inputs, labels=labels)\n",
        "                                                                                                                                                                                                    loss = outputs.loss / cfg.gradient_accumulation_steps\n",
        "                                                                                                                                                                                                            scaler.scale(loss).backward()\n",
        "                                                                                                                                                                                                                    running_loss += loss.item() * cfg.gradient_accumulation_steps\n",
        "\n",
        "                                                                                                                                                                                                                            if (step + 1) % cfg.gradient_accumulation_steps == 0:\n",
        "                                                                                                                                                                                                                                        scaler.step(optimizer)\n",
        "                                                                                                                                                                                                                                                    scaler.update()\n",
        "                                                                                                                                                                                                                                                                optimizer.zero_grad()\n",
        "\n",
        "                                                                                                                                                                                                                                                                    avg_loss = running_loss / len(dataloader)\n",
        "                                                                                                                                                                                                                                                                        print(f\"[{datetime.now().isoformat()}] Epoch {epoch+1}/{cfg.epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                                                                                                                                                                                                                                                                            # Early stopping & sauvegarde meilleur modèle\n",
        "                                                                                                                                                                                                                                                                                val_loss = avg_loss  # pour demo\n",
        "                                                                                                                                                                                                                                                                                    if val_loss < best_val_loss:\n",
        "                                                                                                                                                                                                                                                                                            best_val_loss = val_loss\n",
        "                                                                                                                                                                                                                                                                                                    patience_counter = 0\n",
        "                                                                                                                                                                                                                                                                                                            torch.save(model.state_dict(), os.path.join(cfg.checkpoint_dir, \"best_model.pth\"))\n",
        "                                                                                                                                                                                                                                                                                                                    print(\"✅ Nouveau meilleur modèle sauvegardé\")\n",
        "                                                                                                                                                                                                                                                                                                                        else:\n",
        "                                                                                                                                                                                                                                                                                                                                patience_counter += 1\n",
        "                                                                                                                                                                                                                                                                                                                                        print(f\"⚠️ Pas d'amélioration, patience {patience_counter}/{cfg.early_stopping_patience}\")\n",
        "                                                                                                                                                                                                                                                                                                                                                if patience_counter >= cfg.early_stopping_patience:\n",
        "                                                                                                                                                                                                                                                                                                                                                            print(\"⏹ Early stopping déclenché\")\n",
        "                                                                                                                                                                                                                                                                                                                                                                        break\n",
        "                                                                                                                                                                                                                                                                                                                                                                            # Checkpoint périodique\n",
        "                                                                                                                                                                                                                                                                                                                                                                                torch.save(model.state_dict(), os.path.join(cfg.checkpoint_dir, f\"epoch_{epoch+1}.pth\"))\n",
        "\n",
        "                                                                                                                                                                                                                                                                                                                                                                                # ----------------------------\n",
        "                                                                                                                                                                                                                                                                                                                                                                                # 7️⃣ Fonction de génération optimisée\n",
        "                                                                                                                                                                                                                                                                                                                                                                                # ----------------------------\n",
        "                                                                                                                                                                                                                                                                                                                                                                                def generate(prompt, max_len=cfg.max_gen_len):\n",
        "                                                                                                                                                                                                                                                                                                                                                                                    model.eval()\n",
        "                                                                                                                                                                                                                                                                                                                                                                                        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
        "                                                                                                                                                                                                                                                                                                                                                                                            with torch.no_grad():\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                    output_ids = model.generate(input_ids, max_length=max_len, pad_token_id=tokenizer.pad_token_id)\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        # 8️⃣ Test génération\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        prompt = \"Artificial intelligence is\"\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        print(\"Prompt:\", prompt)\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        print(\"Generated:\", generate(prompt))\n",
        "\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        # 9️⃣ Lancer Streamlit + Ngrok\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        # ----------------------------\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        %%writefile run_arslm.sh\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        #!/bin/bash\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        echo \"🚀 Lancement Streamlit...\"\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        streamlit run streamlit_app.py --server.port 8501 --server.enableCORS false\n",
        "\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        !chmod +x run_arslm.sh\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        !nohup bash run_arslm.sh &\n",
        "\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        from pyngrok import ngrok\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        public_url = ngrok.connect(8501)\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                        print(\"🌐 Lien public Streamlit:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0_WreBFrmSH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eae3307"
      },
      "outputs": [],
      "source": [
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkA1dRX-uxuJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d52a918"
      },
      "source": [
        "# Task\n",
        "Propose des améliorations pour le modèle ARSLM afin de le rendre plus efficace et opérationnel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a243d42"
      },
      "source": [
        "## Amélioration du tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Remplacer le `SimpleTokenizer` par un tokenizer plus robuste capable de gérer la ponctuation, les majuscules/minuscules, les sous-mots (comme les Byte Pair Encoding - BPE ou WordPiece) pour mieux gérer les mots inconnus et réduire la taille du vocabulaire.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09b9c1a5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install a more robust tokenizer library. Hugging Face's `transformers` library is a good choice as it provides access to many pre-trained tokenizers including those using sub-word techniques. The `tokenizers` library is also a good choice, but `transformers` is more commonly used and includes the tokenizers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79429106"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cd18cd"
      },
      "source": [
        "## Explication détaillée du modèle ARSLM\n",
        "\n",
        "Le modèle ARSLM (Adaptive Recurrent State Language Model) est un prototype de modèle linguistique basé sur le concept d'états récurrents adaptatifs (ARS), inspiré par les séquences adaptatives BenPolySeq. Il est conçu pour être léger, explicable et extensible, ce qui le rend adapté à la recherche et à l'expérimentation.\n",
        "\n",
        "Voici une décomposition de ses composants clés :\n",
        "\n",
        "### 1. Tokenizer Avancé (`AdvancedTokenizer`)\n",
        "\n",
        "Ce composant gère la conversion du texte en séquences d'identifiants (tokens) et vice-versa. Dans cette version améliorée, il utilise la bibliothèque `transformers` de Hugging Face, spécifiquement le `BertTokenizer`.\n",
        "\n",
        "*   **Pourquoi un tokenizer avancé ?** Contrairement à un simple tokenizer qui pourrait se contenter de diviser le texte par espaces et de gérer un vocabulaire limité, un tokenizer basé sur les sous-mots (comme ceux utilisés par BERT) permet de :\n",
        "    *   Mieux gérer les mots inconnus en les décomposant en sous-unités (par exemple, \"tokenization\" pourrait être décomposé en \"token\", \"iza\", \"tion\").\n",
        "    *   Réduire la taille globale du vocabulaire tout en représentant une plus grande variété de mots.\n",
        "    *   Gérer la ponctuation, les majuscules/minuscules et d'autres subtilités linguistiques.\n",
        "*   **Fonctionnement :** L'`AdvancedTokenizer` utilise un vocabulaire pré-entraîné (ici, celui de `bert-base-uncased` par défaut). Il fournit des méthodes `encode` pour convertir le texte en identifiants et `decode` pour convertir les identifiants en texte. Il définit également des identifiants spéciaux pour le padding (`<pad>`), les mots inconnus (`<unk>`), le début de séquence (`<bos>`, utilisant `[CLS]` de BERT) et la fin de séquence (`<eos>`, utilisant `[SEP]` de BERT).\n",
        "\n",
        "### 2. Cellule ARS (`ARSCell`)\n",
        "\n",
        "C'est le cœur adaptatif du modèle. Une `ARSCell` calcule le prochain état caché (`h_t`) en se basant sur les deux états cachés précédents (`h_{t-2}`, `h_{t-1}`) et l'embedding de l'entrée courante (`x_embed`).\n",
        "\n",
        "*   **Mécanisme d'Adaptation :** L'idée principale, inspirée par BenPolySeq, est que la mise à jour de l'état caché n'est pas une simple combinaison linéaire ou une porte fixe (comme dans les RNN ou LSTM basiques), mais une addition adaptative d'un \"candidat\" pondéré par un \"gate\".\n",
        "    *   **Signal de Différence :** La cellule utilise la différence entre les deux états cachés précédents (`h_{t-1} - h_{t-2}`). Ce signal est censé capturer l'évolution ou la dynamique de l'état récurrent.\n",
        "    *   **Contexte :** Un vecteur de contexte est créé en concaténant les deux états cachés précédents et l'embedding d'entrée (`[h_{t-1}, h_{t-2}, x_embed]`).\n",
        "    *   **Réseau Candidat (`candidate_mlp`) :** Un MLP (réseau de neurones multi-couches) prend le contexte en entrée et génère un \"candidat\" pour la mise à jour de l'état caché.\n",
        "    *   **Réseau de Porte (`gate_net`) :** Un autre réseau (comprenant une fonction sigmoïde à la fin) prend également le contexte en entrée et produit une valeur scalaire entre 0 et 1. Cette valeur agit comme un \"gate\" qui contrôle l'influence du candidat sur la mise à jour de l'état caché.\n",
        "    *   **Résiduel :** Un petit résiduel de l'embedding d'entrée est ajouté pour aider à propager directement l'information de l'entrée.\n",
        "    *   **Mise à Jour :** L'état caché suivant est calculé comme `h_t = h_{t-1} + gate * candidate + 0.1 * residual`. Cette formule montre comment la cellule *adapte* sa mise à jour en fonction du signal de différence et du contexte via le gate.\n",
        "    *   **Dropout et Normalisation :** Une couche de dropout est appliquée pour la régularisation, et une normalisation de couche (`F.layer_norm`) est utilisée pour stabiliser l'entraînement.\n",
        "\n",
        "### 3. Modèle ARSLM (`ARSLM`)\n",
        "\n",
        "Ce module encapsule l'ensemble de l'architecture, combinant l'embedding, plusieurs couches d'`ARSCell`, un mécanisme d'attention et une couche de sortie (head).\n",
        "\n",
        "*   **Embedding (`nn.Embedding`) :** Convertit les identifiants de tokens en vecteurs denses de dimension `emb_dim`.\n",
        "*   **Couches d'ARSCell (`nn.ModuleList`) :** Le modèle utilise une liste de `num_layers` ARSCells empilées. L'entrée de la première cellule est l'embedding, et l'entrée des cellules subséquentes est l'état caché de la couche précédente.\n",
        "*   **Mécanisme d'Attention :** Un mécanisme d'attention additive simple est appliqué après la dernière couche d'ARSCell.\n",
        "    *   **Attention Causale :** Pendant la passe avant (forward), l'attention est causale, ce qui signifie que chaque token ne peut prêter attention qu'aux tokens précédents dans la séquence.\n",
        "    *   **Calcul :** Pour chaque pas temporel `t`, l'état caché de la dernière couche (`last_layer_h_t`) sert de requête. L'attention est calculée sur l'historique de tous les états cachés de la dernière couche jusqu'au pas `t` (`all_last_layer_hidden_states`). Les scores d'attention sont calculés via un petit réseau (`self.attention`), puis normalisés avec un softmax. Un vecteur de contexte est obtenu en pondérant l'historique par les poids d'attention.\n",
        "    *   **Combinaison :** Le vecteur de contexte est ajouté à l'état caché courant de la dernière couche (`attended_h_t = last_layer_h_t + context_vector`). Cette combinaison permet au modèle de tirer parti des informations pertinentes de la séquence passée.\n",
        "*   **Couche de Sortie (`head`) :** Une couche linéaire qui prend l'état caché combiné (avec attention) de la dernière couche et le projette sur la taille du vocabulaire (`self.vocab_size`). Les sorties de cette couche sont les logits, qui représentent la probabilité non normalisée de chaque token dans le vocabulaire d'être le prochain token.\n",
        "\n",
        "### 4. Fonction `forward`\n",
        "\n",
        "Cette fonction décrit comment une séquence d'identifiants (`input_ids`) est traitée par le modèle pour produire les logits et les gates. Elle itère sur chaque pas temporel de la séquence d'entrée, calculant séquentiellement les états cachés pour chaque couche d'ARSCell et appliquant l'attention et la couche de sortie à chaque pas après la dernière couche.\n",
        "\n",
        "### 5. Fonction `generate`\n",
        "\n",
        "Cette fonction implémente le processus de génération de texte. À partir d'une séquence de contexte initiale (`idx`), le modèle génère séquentiellement de nouveaux tokens. Pour chaque nouveau token à générer :\n",
        "*   Il prend l'embedding du dernier token généré.\n",
        "*   Il utilise les états cachés mis à jour des étapes précédentes pour calculer le nouvel état caché pour chaque couche d'ARSCell.\n",
        "*   Il applique le mécanisme d'attention sur l'historique des états cachés de la dernière couche (incluant le nouvel état).\n",
        "*   Il utilise la couche de sortie pour obtenir les logits pour le prochain token.\n",
        "*   Il applique un échantillonnage (ici, `torch.multinomial` avec une température et optionnellement `top_k`) pour sélectionner le prochain token en fonction des probabilités calculées à partir des logits.\n",
        "*   Le nouveau token est ajouté à la séquence de sortie, et le processus se répète jusqu'à ce que le nombre maximal de nouveaux tokens soit atteint.\n",
        "\n",
        "### 6. Utilitaires d'Entraînement/Démo (`collate_batch`, `train_demo`)\n",
        "\n",
        "*   **`collate_batch` :** Prépare les données texte pour l'entraînement en les encodant à l'aide du tokenizer, en ajoutant les tokens de début/fin de séquence et en effectuant le padding pour que toutes les séquences d'un batch aient la même longueur.\n",
        "*   **`train_demo` :** Une fonction de démonstration qui initialise le tokenizer, le modèle, l'optimiseur et la fonction de perte. Elle charge un petit corpus (ou un fichier si spécifié), prépare les données, entraîne le modèle sur un nombre spécifié d'époques, imprime la perte et le taux d'apprentissage, calcule la perplexité et effectue une petite démo de génération de texte.\n",
        "\n",
        "En résumé, l'ARSLM combine l'idée d'états récurrents adaptatifs avec une architecture multi-couches et un mécanisme d'attention causale pour créer un modèle linguistique capable d'apprendre des dépendances séquentielles de manière flexible. L'aspect \"adaptatif\" via le mécanisme de gate dans l'ARSCell est la caractéristique distinctive, permettant potentiellement au modèle d'ajuster son comportement en fonction de la dynamique interne de sa mémoire récurrente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "342b7ddc"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `transformers` library is installed, I will import a suitable tokenizer, such as `BertTokenizer`, and create a new class that wraps this tokenizer to maintain compatibility with the existing code's `encode` and `decode` methods. I will also define the special tokens needed for the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61764a30"
      },
      "source": [
        "# Task\n",
        "Refactor the data loading and preparation process by creating a `TextDataset` class that integrates with `torch.utils.data.DataLoader`, modifying `collate_batch` to support batched data loading, and implementing training, validation, and test data splits within the `train_demo` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1131c95a"
      },
      "source": [
        "## Refactorisation du Chargement et Préparation des Données\n",
        "\n",
        "### Subtask:\n",
        "Créer une classe `TextDataset` pour une gestion plus structurée des données textuelles et de la tokenisation. Modifier la fonction `collate_batch` pour qu'elle soit compatible avec `torch.utils.data.DataLoader`, et intégrer une séparation des données en ensembles d'entraînement, de validation et de test dans `train_demo`. L'objectif est d'assurer un chargement efficace des données et une évaluation fiable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75c5c42f"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `TextDataset` class which will encapsulate the text data and the tokenizer, and implement its `__len__` and `__getitem__` methods as specified in the instructions to prepare for data loading with `DataLoader`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f23e08c6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset # Import Dataset\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106b9c42"
      },
      "source": [
        "**Reasoning**:\n",
        "I will modify the `collate_batch` function to accept a list of raw text strings and the tokenizer, then encode, pad, and prepare input and target tensors for language modeling, as required by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cae2df6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List\n",
        "\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    # All tensors should be on the same device before stacking for DataLoader compatibility\n",
        "    # We'll assume the device is passed to train_demo and handled there for now.\n",
        "    # For simplicity in collate_batch, let's keep it device-agnostic, and move to device later.\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1] # All tokens except the last one\n",
        "    targets = padded_batch[:, 1:]  # All tokens except the first one\n",
        "\n",
        "    return inputs, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "308955db"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the `train_demo` function to split the data into training, validation, and test sets, create `TextDataset` and `DataLoader` instances for each, and modify the training loop to use the DataLoader for batching. This will align with the subtask's requirements for refactoring data loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a70713c0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    corpus_file = \"/tmp/large_corpus.txt\"\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1000:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Split data into train, validation, and test sets\n",
        "    train_texts, test_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "    val_texts, test_texts = train_test_split(test_texts, test_size=0.5, random_state=42) # 0.1 of total\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    batch_size = 8 # Define a batch size\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = 200\n",
        "    if len(train_texts) == 0:\n",
        "        print(\"No training data available after split. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            logits, gates = model(inputs)\n",
        "            b, seq, v = logits.shape\n",
        "            loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step\n",
        "            model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs_val, targets_val in val_loader:\n",
        "                    inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                    logits_val, _ = model(inputs_val)\n",
        "                    b_val, seq_val, v_val = logits_val.shape\n",
        "                    loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                    total_val_loss += loss_val.item()\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs_test, targets_test in test_loader:\n",
        "            inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "            logits_test, _ = model(inputs_test)\n",
        "            b_test, seq_test, v_test = logits_test.shape\n",
        "            loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "            total_test_loss += loss_test.item()\n",
        "            total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7829f656"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `ValueError` occurred because the `train_test_split` function was called with too few samples to create distinct validation and test sets when using the small toy corpus. I will modify the data splitting logic in `train_demo` to handle cases with very few samples gracefully, ensuring that `train_test_split` is called only when there are enough samples and avoiding empty splits. Additionally, I will add checks for `len(loader)` before calculating average losses to prevent `ZeroDivisionError` if a DataLoader is empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40e8f12"
      },
      "source": [
        "# Task\n",
        "The next step is to introduce a configuration system (using a dictionary or a simple class) to manage model hyperparameters, training parameters (learning rate, number of epochs, etc.), and file paths. This will centralize settings and make the model more easily configurable and adaptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db07624d"
      },
      "source": [
        "## Refactorisation du Chargement et Préparation des Données\n",
        "\n",
        "### Subtask:\n",
        "Créer une classe `TextDataset` pour une gestion plus structurée des données textuelles et de la tokenisation. Modifier la fonction `collate_batch` pour qu'elle soit compatible avec `torch.utils.data.DataLoader`, et intégrer une séparation des données en ensembles d'entraînement, de validation et de test dans `train_demo`. L'objectif est d'assurer un chargement efficace des données et une évaluation fiable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a32607dd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `ValueError` occurred because the `train_test_split` function was called with too few samples to create distinct validation and test sets when using the small toy corpus. I will modify the `train_demo` function to robustly handle data splitting, ensuring that `train_test_split` is called only when there are enough samples and avoiding empty splits. Additionally, I will add checks for `len(loader)` before calculating average losses to prevent `ZeroDivisionError` if a DataLoader is empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1938d21"
      },
      "source": [
        "# Task\n",
        "It looks like the previous execution failed due to a `ValueError` in `train_test_split` when the corpus was too small, and there's also a potential for `ZeroDivisionError` if a DataLoader ends up being empty.\n",
        "\n",
        "The next step is to refine the `train_demo` function to make the data splitting more robust, especially for small datasets, and to ensure that loss calculations only occur when DataLoaders contain data. This involves adjusting the `train_test_split` parameters and adding checks before calculating average losses or perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d8ed764"
      },
      "source": [
        "## Refactorisation du Chargement et Préparation des Données\n",
        "\n",
        "### Subtask:\n",
        "Corriger la fonction `train_demo` pour gérer les `ValueError` lors de la division des données (entraînement, validation, test) avec de petits corpus, en s'assurant que les `DataLoader` sont correctement créés et utilisés uniquement s'ils contiennent des données. Cela inclura l'ajustement des appels à `train_test_split` et les calculs de perte moyenne pour éviter les erreurs de division par zéro.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380f03af"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `ValueError` occurred because `train_test_split` was called with too few samples to create distinct validation and test sets. I will modify the `train_demo` function to include robust data splitting logic, handling cases with small corpora by assigning all data to the training set and ensuring `val_texts` and `test_texts` are empty. I will also add checks for empty `DataLoader` instances before calculating average losses to prevent `ZeroDivisionError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "132ccd1f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        # Using a pre-trained tokenizer's vocabulary for demonstration\n",
        "        # In a real scenario, you might train a new tokenizer on your specific corpus\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        # Define special tokens explicitly for clarity and compatibility\n",
        "        # These might vary depending on the chosen tokenizer\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # The tokenizer handles BOS/EOS internally with add_special_tokens=True\n",
        "        # We can also manually add them if needed for specific model architectures\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        # skip_special_tokens=True prevents decoding [CLS], [SEP], [PAD] etc.\n",
        "        # We might adjust this based on how the model handles special tokens\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=0.1) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    corpus_file = \"/tmp/large_corpus.txt\"\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < 1000:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer()\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=0.2, random_state=42)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop())\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        # Calculate test_size for validation set: 0.1 / (1-0.2) = 0.125\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = 0.1 / (1.0 - 0.2)\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=42)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop())\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    batch_size = 8 # Define a batch size\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=64, hidden_dim=128, num_layers=2).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = 200\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = \"hello world\"\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=15, temperature=1.0, top_k=50)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "377d3ab7"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a `Config` class to centralize all hyperparameters and settings, making the model more configurable. This class will hold model parameters, training parameters, and file paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da96bfbc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 1000 # Max lines to load from corpus_file for demo\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4c2aa05"
      },
      "source": [
        "### Déploiement du modèle pour l'inférence\n",
        "\n",
        "Pour déployer le modèle et générer du texte, suivez ces étapes :\n",
        "1.  Chargez le modèle, le tokenizer et la configuration à l'aide de la fonction `load_model_and_tokenizer`.\n",
        "2.  Utilisez la fonction `deploy_model` avec votre texte d'entrée (`input_text`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23f115e9"
      },
      "outputs": [],
      "source": [
        "# Initialize configuration (to get paths)\n",
        "cfg = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Load the trained model, tokenizer, and configuration\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg.model_save_dir, \"best_model.pt\"),\n",
        "        cfg.tokenizer_save_dir,\n",
        "        cfg.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Define your input text for generation\n",
        "    input_text = \"Colab is a great tool for\"\n",
        "\n",
        "    # Deploy the model to generate text\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, input_text)\n",
        "\n",
        "    print(f\"Input Text: {input_text}\")\n",
        "    print(f\"Generated Output: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96fe4aaf"
      },
      "source": [
        "## Implémentation de la Sauvegarde et du Chargement du Modèle\n",
        "\n",
        "### Subtask:\n",
        "Ajouter des fonctionnalités dans train_demo pour sauvegarder les poids du meilleur modèle entraîné (basé sur une métrique comme la perte de validation ou la perplexité) et le tokenizer associé. Développer également une fonction distincte pour charger un modèle et son tokenizer pré-entraînés, essentielle pour l'inférence en production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0387bfd"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define `model_save_dir` and `tokenizer_save_dir` in the `Config` class to specify where the model and tokenizer should be saved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e6ca383"
      },
      "source": [
        "# Task\n",
        "**Task:** Download the \"wikitext-2-raw-v1\" dataset from Hugging Face, extract the text, and save the first 10,000 lines into the `/tmp/large_corpus.txt` file. Then, modify the `Config` class to set `max_corpus_lines` to 10,000 to utilize this larger corpus for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN3rV1JShnLS"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 1️⃣ Installer les dépendances\n",
        "# ================================\n",
        "!pip install -q streamlit pyngrok transformers torch datasets peft accelerate gitpython\n",
        "\n",
        "# ================================\n",
        "# 2️⃣ Importer les modules\n",
        "# ================================\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import json\n",
        "from git import Repo\n",
        "\n",
        "# ================================\n",
        "# 3️⃣ Cloner ton repo GitHub ARSLM\n",
        "# ================================\n",
        "GITHUB_REPO = \"https://github.com/benjaminpolydeq/ARSLM.git\"\n",
        "LOCAL_DIR = \"ARSLM\"\n",
        "\n",
        "if not os.path.exists(LOCAL_DIR):\n",
        "    print(\"🔄 Clonage du repo ARSLM depuis GitHub...\")\n",
        "        Repo.clone_from(GITHUB_REPO, LOCAL_DIR)\n",
        "        else:\n",
        "            print(\"ℹ️ Repo ARSLM déjà présent\")\n",
        "\n",
        "            os.chdir(LOCAL_DIR)\n",
        "\n",
        "            # ================================\n",
        "            # 4️⃣ Préparer un dataset minimal\n",
        "            # ================================\n",
        "            dataset_json = [\n",
        "                {\"instruction\": \"Comment demander un congé ?\", \"output\": \"Connectez-vous au portail RH, cliquez sur 'Congés', puis soumettez votre demande.\"},\n",
        "                    {\"instruction\": \"Quelle est la politique de télétravail ?\", \"output\": \"3 jours max par semaine, accord manager requis.\"},\n",
        "                        {\"instruction\": \"Qu'est-ce que ARSLM ?\", \"output\": \"ARSLM est un moteur AI léger pour générer des réponses intelligentes.\"}\n",
        "                        ]\n",
        "\n",
        "                        with open(\"fine_tune.json\", \"w\") as f:\n",
        "                            json.dump(dataset_json, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "                            dataset = load_dataset(\"json\", data_files=\"fine_tune.json\")[\"train\"]\n",
        "\n",
        "                            # ================================\n",
        "                            # 5️⃣ Charger le modèle de base\n",
        "                            # ================================\n",
        "                            base_model = \"distilgpt2\"\n",
        "                            tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "                            model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "                            # ================================\n",
        "                            # 6️⃣ Configurer LoRA\n",
        "                            # ================================\n",
        "                            lora_config = LoraConfig(\n",
        "                                r=8,\n",
        "                                    lora_alpha=32,\n",
        "                                        target_modules=[\"c_attn\"],\n",
        "                                            lora_dropout=0.05,\n",
        "                                                task_type=TaskType.CAUSAL_LM\n",
        "                                                )\n",
        "                                                model = get_peft_model(model, lora_config)\n",
        "\n",
        "                                                # ================================\n",
        "                                                # 7️⃣ Tokenisation\n",
        "                                                # ================================\n",
        "                                                def preprocess(example):\n",
        "                                                    input_text = f\"Instruction: {example['instruction']}\\nRéponse:\"\n",
        "                                                        target_text = example['output']\n",
        "                                                            full_text = input_text + \" \" + target_text\n",
        "                                                                return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "                                                                tokenized_dataset = dataset.map(preprocess)\n",
        "\n",
        "                                                                # ================================\n",
        "                                                                # 8️⃣ Fine-tuning rapide\n",
        "                                                                # ================================\n",
        "                                                                training_args = TrainingArguments(\n",
        "                                                                    output_dir=\"arslm_llm\",\n",
        "                                                                        per_device_train_batch_size=2,\n",
        "                                                                            gradient_accumulation_steps=2,\n",
        "                                                                                num_train_epochs=1,  # augmenter pour dataset réel\n",
        "                                                                                    learning_rate=1e-4,\n",
        "                                                                                        save_total_limit=1,\n",
        "                                                                                            logging_steps=10,\n",
        "                                                                                                fp16=True,\n",
        "                                                                                                    push_to_hub=False\n",
        "                                                                                                    )\n",
        "\n",
        "                                                                                                    trainer = Trainer(\n",
        "                                                                                                        model=model,\n",
        "                                                                                                            args=training_args,\n",
        "                                                                                                                train_dataset=tokenized_dataset\n",
        "                                                                                                                )\n",
        "\n",
        "                                                                                                                trainer.train()\n",
        "\n",
        "                                                                                                                # Sauvegarder le modèle fine-tuné\n",
        "                                                                                                                model.save_pretrained(\"arslm_llm\")\n",
        "                                                                                                                tokenizer.save_pretrained(\"arslm_llm\")\n",
        "                                                                                                                print(\"✅ Fine-tuning terminé et modèle sauvegardé dans arslm_llm/\")\n",
        "\n",
        "                                                                                                                # ================================\n",
        "                                                                                                                # 9️⃣ Déploiement Streamlit via ngrok\n",
        "                                                                                                                # ================================\n",
        "                                                                                                                # Kill toute instance Streamlit précédente\n",
        "                                                                                                                !pkill streamlit\n",
        "\n",
        "                                                                                                                # Créer un tunnel public\n",
        "                                                                                                                public_url = ngrok.connect(port='8501')\n",
        "                                                                                                                print(\"🌐 Lien public Streamlit:\", public_url)\n",
        "\n",
        "                                                                                                                # Lancer Streamlit\n",
        "                                                                                                                # Assurez-vous que streamlit_app.py et arslm/ sont présents\n",
        "                                                                                                                os.system(\"streamlit run streamlit_app.py &\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt9ESjfroHiP"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🚀 Tout-en-un ARSLM Colab\n",
        "# ================================\n",
        "\n",
        "# 1️⃣ Installer les dépendances\n",
        "!pip install -q streamlit pyngrok transformers torch datasets peft accelerate gitpython\n",
        "\n",
        "# 2️⃣ Importer modules\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "from git import Repo\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# 3️⃣ Cloner le repo ARSLM\n",
        "GITHUB_REPO = \"https://github.com/benjaminpolydeq/ARSLM.git\"\n",
        "LOCAL_DIR = \"ARSLM\"\n",
        "if not os.path.exists(LOCAL_DIR):\n",
        "    print(\"Clonage du repo ARSLM depuis GitHub...\")\n",
        "    Repo.clone_from(GITHUB_REPO, LOCAL_DIR)\n",
        "else:\n",
        "    print(\"Repo ARSLM déjà présent\")\n",
        "os.chdir(LOCAL_DIR)\n",
        "\n",
        "# 4️⃣ Préparer datasets par défaut (RH, juridique, medical, default)\n",
        "datasets_templates = {\n",
        "    \"default\": [{\"instruction\": \"Qu'est-ce que ARSLM ?\", \"output\": \"ARSLM est un moteur AI léger pour générer des réponses intelligentes.\"}],\n",
        "    \"rh\": [\n",
        "        {\"instruction\": \"Comment demander un congé ?\", \"output\": \"Connectez-vous au portail RH, cliquez sur 'Congés', puis soumettez votre demande.\"},\n",
        "        {\"instruction\": \"Quelle est la politique de télétravail ?\", \"output\": \"3 jours max par semaine, accord manager requis.\"}\n",
        "    ],\n",
        "    \"juridique\": [{\"instruction\": \"Quelles sont les obligations légales ?\", \"output\": \"Consultez le code du travail applicable et les régulations locales.\"}],\n",
        "    \"medical\": [{\"instruction\": \"Comment prendre rendez-vous ?\", \"output\": \"Utilisez le portail médical ou appelez la réception.\"}]\n",
        "}\n",
        "\n",
        "# 5️⃣ Vérifier modèle fine-tuné\n",
        "MODEL_DIR = \"arslm_llm\"\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    print(\"⚠️ Modèle non trouvé. Tentative récupération depuis Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/arslm_llm\"\n",
        "    if os.path.exists(DRIVE_PATH):\n",
        "        !cp -r /content/drive/MyDrive/arslm_llm ./arslm_llm\n",
        "        print(\"✅ Modèle récupéré depuis Drive\")\n",
        "    else:\n",
        "        print(\"❌ Aucun modèle trouvé. Lancement d'un fine-tuning rapide...\")\n",
        "\n",
        "        # Dataset minimal pour fine-tuning\n",
        "        selected_dataset = datasets_templates[\"default\"]\n",
        "        with open(\"fine_tune.json\", \"w\") as f:\n",
        "            json.dump(selected_dataset, f, ensure_ascii=False, indent=2)\n",
        "        dataset = load_dataset(\"json\", data_files=\"fine_tune.json\")[\"train\"]\n",
        "\n",
        "        # Charger modèle de base + LoRA\n",
        "        base_model = \"distilgpt2\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "        model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "        lora_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"c_attn\"],\n",
        "            lora_dropout=0.05,\n",
        "            task_type=TaskType.CAUSAL_LM\n",
        "        )\n",
        "        model = get_peft_model(model, lora_config)\n",
        "\n",
        "        # Tokenisation\n",
        "        def preprocess(example):\n",
        "            input_text = f\"Instruction: {example['instruction']}\\nRéponse:\"\n",
        "            target_text = example['output']\n",
        "            return tokenizer(input_text + \" \" + target_text, truncation=True, padding=\"max_length\", max_length=128)\n",
        "        tokenized_dataset = dataset.map(preprocess)\n",
        "\n",
        "        # Fine-tuning rapide\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"arslm_llm\",\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=2,\n",
        "            num_train_epochs=1,\n",
        "            learning_rate=1e-4,\n",
        "            save_total_limit=1,\n",
        "            logging_steps=10,\n",
        "            fp16=True,\n",
        "            push_to_hub=False\n",
        "        )\n",
        "        trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset)\n",
        "        trainer.train()\n",
        "\n",
        "        # Sauvegarder modèle\n",
        "        model.save_pretrained(\"arslm_llm\")\n",
        "        tokenizer.save_pretrained(\"arslm_llm\")\n",
        "        print(\"✅ Fine-tuning terminé et modèle sauvegardé dans arslm_llm/\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ Modèle fine-tuné trouvé localement\")\n",
        "\n",
        "# 6️⃣ Déploiement Streamlit via ngrok\n",
        "!pkill streamlit\n",
        "public_url = ngrok.connect(port='8501')\n",
        "print(\"🌐 Lien public Streamlit :\", public_url)\n",
        "os.system(\"streamlit run streamlit_app.py &\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN8D4OsvrdBX"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "try:\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    if tunnels:\n",
        "        public_url = tunnels[0].public_url\n",
        "    else:\n",
        "        public_url = ngrok.connect(8501).public_url\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <h3>🌐 Lien public Streamlit :</h3>\n",
        "    <a href=\"{public_url}\" target=\"_blank\" style=\"font-size:20px;color:blue;\">{public_url}</a>\n",
        "    \"\"\"))\n",
        "    print(f\"Streamlit Public URL: {public_url}\") # Add print for robustness\n",
        "\n",
        "except Exception as e:\n",
        "    display(HTML(f\"\"\"\n",
        "    <h3>⚠️ Impossible de récupérer le lien Streamlit :</h3>\n",
        "    <p style=\"color:red;\">{e}</p>\n",
        "    \"\"\"))\n",
        "    print(f\"Error retrieving Streamlit link: {e}\") # Add print for robustness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI8zQludsvXn"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "try:\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "        if tunnels:\n",
        "                public_url = tunnels[0].public_url\n",
        "                    else:\n",
        "                            public_url = ngrok.connect(8501).public_url\n",
        "\n",
        "                                display(HTML(f\"\"\"\n",
        "                                        <h3>🌐 Lien public Streamlit :</h3>\n",
        "                                                <a href=\"{public_url}\" target=\"_blank\" style=\"font-size:20px;color:blue;\">{public_url}</a>\n",
        "                                                    \"\"\"))\n",
        "\n",
        "                                                    except Exception as e:\n",
        "                                                        display(HTML(f\"\"\"\n",
        "                                                                <h3>⚠️ Impossible de récupérer le lien Streamlit :</h3>\n",
        "                                                                        <p style=\"color:red;\">{e}</p>\n",
        "                                                                            \"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b617c662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331a6884-cdbc-4444-bcc3-d5ec843618ef"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading corpus from /tmp/large_corpus.txt...\n",
            "Loaded 10000 lines.\n",
            "Train size: 4000, Validation size: 4000, Test size: 2000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 — Avg Train Loss: 7.2143\n",
            "Current learning rate: 0.001000\n",
            "Validation Loss: 6.8050\n",
            "Saving best model with validation loss: 6.8050\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        # Calculate test_size for validation set: 0.1 / (1-0.2) = 0.125\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b28984b9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "LOCAL_DIR = \"ARSLM\"\n",
        "\n",
        "# Change to the repository directory if not already there\n",
        "if os.path.exists(LOCAL_DIR) and os.getcwd() != os.path.abspath(LOCAL_DIR):\n",
        "    print(f\"Navigating to {LOCAL_DIR}/\")\n",
        "    os.chdir(LOCAL_DIR)\n",
        "elif not os.path.exists(LOCAL_DIR):\n",
        "    print(f\"Error: The directory {LOCAL_DIR} does not exist. Please ensure the repository is cloned by running cell dN3rV1JShnLS first.\")\n",
        "else:\n",
        "    print(f\"Already in {LOCAL_DIR}/\")\n",
        "\n",
        "# Kill any previous Streamlit instances\n",
        "print(\"Killing any previous Streamlit instances...\")\n",
        "os.system(\"pkill streamlit\")\n",
        "\n",
        "# Create a public ngrok tunnel\n",
        "print(\"Creating public ngrok tunnel...\")\n",
        "public_url = ngrok.connect(port='8501')\n",
        "print(\"🌐 Lien public Streamlit:\", public_url)\n",
        "\n",
        "# Launch Streamlit\n",
        "# Ensure streamlit_app.py and arslm/ are present in the current directory\n",
        "print(\"Launching Streamlit application...\")\n",
        "os.system(\"streamlit run streamlit_app.py &\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110d6a9f"
      },
      "source": [
        "## Rechercher et Télécharger un Corpus de Texte\n",
        "\n",
        "### Subtask:\n",
        "Download the 'wikitext-2-raw-v1' dataset from Hugging Face, extract the text, and save the first 10,000 lines into the `/tmp/large_corpus.txt` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff583a2b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires downloading a dataset from Hugging Face, which needs the `datasets` library. Therefore, the first step is to install this library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96420ed7"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad024e6a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `datasets` library is confirmed to be installed, I will proceed to download the 'wikitext-2-raw-v1' dataset, extract its training text, filter out empty lines, and save the first 10,000 non-empty lines to `/tmp/large_corpus.txt` as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c95cef14"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Define the output file path\n",
        "corpus_file_path = \"/tmp/large_corpus.txt\"\n",
        "\n",
        "# Load the 'wikitext-2-raw-v1' dataset\n",
        "print(\"Loading wikitext-2-raw-v1 dataset from Hugging Face...\")\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "\n",
        "# Access the 'train' split and extract 'text' column\n",
        "train_texts = dataset['train']['text']\n",
        "\n",
        "# Filter out empty strings and strip whitespace\n",
        "filtered_texts = [line.strip() for line in train_texts if line.strip()]\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(corpus_file_path), exist_ok=True)\n",
        "\n",
        "# Write the first 10,000 lines to the specified file\n",
        "print(f\"Writing first 10,000 lines to {corpus_file_path}...\")\n",
        "with open(corpus_file_path, 'w', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(filtered_texts):\n",
        "        if i < 10000:\n",
        "            f.write(line + '\\n')\n",
        "        else:\n",
        "            break\n",
        "print(\"Dataset extraction and saving complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af73b0b4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask required saving the first 10,000 lines of the dataset to `/tmp/large_corpus.txt`. Now that this file is created, I need to update the `Config` class to reflect this by setting `max_corpus_lines` to 10,000. This will ensure that the `train_demo` function, when called later, correctly loads the larger corpus for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6cfc04d"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msuCpWIVjITu"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "try:\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    if tunnels:\n",
        "        public_url = tunnels[0].public_url\n",
        "    else:\n",
        "        public_url = ngrok.connect(8501).public_url\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "        <h3>🌐 Lien public Streamlit :</h3>\n",
        "        <a href=\"{public_url}\" target=\"_blank\" style=\"font-size:20px;color:blue;\">{public_url}</a>\n",
        "    \"\"\"))\n",
        "\n",
        "except Exception as e:\n",
        "    display(HTML(f\"\"\"\n",
        "        <h3>⚠️ Impossible de récupérer le lien Streamlit :</h3>\n",
        "        <p style=\"color:red;\">{e}</p>\n",
        "    \"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3184c9f8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance AND max_seq_len\n",
        "def collate_batch(batch_texts: List[str], tokenizer, max_seq_len: int):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        # Truncate if longer than max_seq_len, ensuring EOS is present\n",
        "        if len(ids) > max_seq_len:\n",
        "            ids = ids[:max_seq_len - 1] + [tokenizer.eos_token_id] if max_seq_len > 1 else ids[:max_seq_len]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max_len within the batch, which will be at most max_seq_len due to truncation\n",
        "    max_len_in_batch = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len_in_batch), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "# Explicitly call train_demo() to ensure it runs and saves the model\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07TOZBzZcgKF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "310a455c"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération\n",
        "    new_input_text = \"L'intelligence artificielle est\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, new_input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {new_input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5373c1d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Assuming Config class is available in the environment or can be re-imported if necessary.\n",
        "# For a standalone check, we can re-define a minimal Config class or instantiate the one from the previous cell.\n",
        "# Given it's a colab environment, we can assume the Config class is already defined.\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "model_path = os.path.join(cfg.model_save_dir, \"best_model.pt\")\n",
        "tokenizer_path = cfg.tokenizer_save_dir\n",
        "config_path = cfg.config_save_path\n",
        "\n",
        "print(\"Vérification des fichiers sauvegardés :\")\n",
        "\n",
        "# Check for model file\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"Le fichier du modèle a été trouvé à : {model_path}\")\n",
        "else:\n",
        "    print(f\"Le fichier du modèle n'a PAS été trouvé à : {model_path}\")\n",
        "\n",
        "# Check for tokenizer directory\n",
        "if os.path.isdir(tokenizer_path):\n",
        "    print(f\"Le répertoire du tokenizer a été trouvé à : {tokenizer_path}\")\n",
        "else:\n",
        "    print(f\"Le répertoire du tokenizer n'a PAS été trouvé à : {tokenizer_path}\")\n",
        "\n",
        "# Check for config file\n",
        "if os.path.exists(config_path):\n",
        "    print(f\"Le fichier de configuration a été trouvé à : {config_path}\")\n",
        "else:\n",
        "    print(f\"Le fichier de configuration n'a PAS été trouvé à : {config_path}\")\n",
        "\n",
        "# Additionally, try to load a small part of the config to ensure it's readable\n",
        "try:\n",
        "    if os.path.exists(config_path):\n",
        "        # Need to add Config to safe_globals for unpickling if not already added\n",
        "        if 'Config' not in torch.serialization.get_safe_globals():\n",
        "            torch.serialization.add_safe_globals([Config])\n",
        "        loaded_cfg = torch.load(config_path)\n",
        "        print(f\"La configuration a été chargée avec succès. emb_dim : {loaded_cfg.emb_dim}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement de la configuration : {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31fe820d"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le texte d'entrée pour la génération\n",
        "    input_text = \"Le modèle linguistique apprend\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba0d7492"
      },
      "source": [
        "## Réexécuter l'Entraînement\n",
        "\n",
        "### Subtask:\n",
        "Réexécuter la fonction `train_demo` avec le nouveau corpus. Cela permettra de tester si le modèle bénéficie d'un jeu de données plus riche.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0d72277"
      },
      "source": [
        "**Reasoning**:\n",
        "I will execute the `train_demo()` function, which now uses the updated `Config` class pointing to the larger corpus, to initiate the training process and observe its performance with the richer dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKkRzYjVsda9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68484000"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "cfg_instance = Config() # Assuming Config class is still defined from previous cells\n",
        "\n",
        "print(f\"Contents of {cfg_instance.model_save_dir}:\")\n",
        "if os.path.exists(cfg_instance.model_save_dir):\n",
        "    for item in os.listdir(cfg_instance.model_save_dir):\n",
        "        print(f\"- {item}\")\n",
        "else:\n",
        "    print(f\"Directory {cfg_instance.model_save_dir} does not exist.\")\n",
        "\n",
        "print(f\"\\nContents of {cfg_instance.tokenizer_save_dir}:\")\n",
        "if os.path.exists(cfg_instance.tokenizer_save_dir):\n",
        "    for item in os.listdir(cfg_instance.tokenizer_save_dir):\n",
        "        print(f\"- {item}\")\n",
        "else:\n",
        "    print(f\"Directory {cfg_instance.tokenizer_save_dir} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5484543"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération\n",
        "    new_input_text = \"Le modèle linguistique adapte\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, new_input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {new_input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f7051c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751,
          "referenced_widgets": [
            "1691fbd98daa4bdc8a79dc2329ecea50",
            "72f09842cd444caea5b0f25c6a19a370",
            "cfcea13c50614839870005da73c69613",
            "a5feda0934a0477d920f74ba19742c51",
            "ae8a4568e5ca40a694779465ca9f7ce2",
            "0e89eb8830cf439b8690b9e770711cf7",
            "2a9b05e8bd794947aff10daa4490dde1",
            "c6f0f0a30905472cb310b6ae5e693152",
            "b51606af85974f3dbcf7390a6413e2b3",
            "97704179da0c4bd8a92755993f147ce8",
            "695821696bb84cac8ee35d73443c3d33",
            "88beee49142d4a8abb63124d27385070",
            "13a6789a2c7740d7af3f38d6ce3f29b5",
            "bb4d1484e8c04fa4b8146261a81bfbc6",
            "ee2a29e364a54a7fa85706561b30a541",
            "ebe8c69270d143ec84bc9b47e32ca21e",
            "b6c953b934584544a9cb448900c9a4fd",
            "97429abe93474ab8a3c3a5dada673bcd",
            "cf323a2014a84735825246d99803dc09",
            "e1245db234fe411d9355e026e5da4d6c",
            "2aad39fb3d284503a47a1b31fac2041c",
            "74b9b7f69add4015919dfa8626745f7c",
            "24c1e04aae0348edb119465c455c9ff1",
            "90990b0229aa46d0842feddd41318acd",
            "0267471008794ae5b5af42327eca402f",
            "9bafa09f96ad404b959ac530f11430bd",
            "b6e2bf5f99c94f609a78fd463b8f5092",
            "c7e4d1cf089a49bc8f72a6d0ba5ca139",
            "7b2405c5d7224710af37a01cba4faaf2",
            "882dbd88f8af48cd9d9efe4706c33f40",
            "36effa32eeca431294d5bdbbb16c1959",
            "c5bc967d59c94b1c91ad1dfcabafa2d8",
            "77375f506bb74e21bdd2758fa729329e",
            "6277659bcd884f0a89763c5dac54c083",
            "c169a21737ff4e499342f1ea33a33137",
            "5279dc849afe48f886ec2b01a0de6b01",
            "49607961d7ab44aabb4f5a6bff990ee3",
            "4672065520ad42d788d7cc0f9e6bb29a",
            "b847ab58dd954fbbaae962abbe506693",
            "2f217c117e71451caf19f04770e581c5",
            "b4268310415f4167b4ed2bce922aabbf",
            "eb1ec76e955f4b70b7ffc3cc86edab6c",
            "aeb6b35748564da29f1cebb91512da0f",
            "9a5bff902a294cbe9ff1173d10e91bae"
          ]
        },
        "outputId": "5fbf8a43-68fe-4ac4-92a3-ff04e035ecaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus file not found at /tmp/large_corpus.txt. Using a small toy corpus instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1691fbd98daa4bdc8a79dc2329ecea50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88beee49142d4a8abb63124d27385070"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24c1e04aae0348edb119465c455c9ff1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6277659bcd884f0a89763c5dac54c083"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 2, Validation size: 2, Test size: 1\n",
            "Epoch 1/200 — Avg Train Loss: 11.0606\n",
            "Current learning rate: 0.001000\n",
            "Validation Loss: 10.5915\n",
            "Saving best model with validation loss: 10.5915\n",
            "Epoch 50/200 — Avg Train Loss: 0.2829\n",
            "Current learning rate: 0.000500\n",
            "Validation Loss: 13.3332\n",
            "Epoch 100/200 — Avg Train Loss: 0.1067\n",
            "Current learning rate: 0.000250\n",
            "Validation Loss: 14.4110\n",
            "Epoch 150/200 — Avg Train Loss: 0.0883\n",
            "Current learning rate: 0.000125\n",
            "Validation Loss: 14.8565\n",
            "Epoch 200/200 — Avg Train Loss: 0.1089\n",
            "Current learning rate: 0.000063\n",
            "Validation Loss: 14.9513\n",
            "\n",
            "=== Evaluation ===\n",
            "Test Loss: 14.6811\n",
            "Perplexity on test data: 2376320.0000\n",
            "\n",
            "=== Generated ===\n",
            "hello world is ars\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased', max_seq_len=512):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "        self.max_seq_len = max_seq_len # Store max_seq_len\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        # Pass max_length and truncation to the internal tokenizer\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False, max_length=self.max_seq_len, truncation=True)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer, max_seq_len: int):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        # Truncate if longer than max_seq_len, ensuring EOS is present\n",
        "        if len(ids) > max_seq_len:\n",
        "            ids = ids[:max_seq_len - 1] + [tokenizer.eos_token_id] if max_seq_len > 1 else ids[:max_seq_len]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max_len within the batch, which will be at most max_seq_len due to truncation\n",
        "    max_len_in_batch = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len_in_batch), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer(max_seq_len=cfg.max_seq_len) # Initialize AdvancedTokenizer and pass max_seq_len\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path, max_seq_len=cfg.max_seq_len)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "# Explicitly call train_demo() to ensure it runs and saves the model\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699278c5"
      },
      "source": [
        "**Reasoning**:\n",
        "I will execute the provided code block which contains the entire model definition and the `train_demo()` function. This will re-run the training and evaluation process, now utilizing the larger corpus as configured in the `Config` class, and display the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fec2c02"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taWLwjZTQTed"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/benjaminpolydeq/ARSLM.git\n",
        "%cd ARSLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTPshx3mQcEf"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat << 'EOF' > run_arslm.sh\n",
        "#!/bin/bash\n",
        "set -e\n",
        "\n",
        "echo \"🚀 ARSLM – Démarrage automatique\"\n",
        "\n",
        "# Vérification Python\n",
        "if ! command -v python3 &> /dev/null; then\n",
        "  echo \"❌ Python3 introuvable\"\n",
        "    exit 1\n",
        "    fi\n",
        "\n",
        "    # Environnement virtuel\n",
        "    if [ ! -d \".venv\" ]; then\n",
        "      echo \"📦 Création environnement virtuel\"\n",
        "        python3 -m venv .venv\n",
        "        fi\n",
        "\n",
        "        source .venv/bin/activate\n",
        "\n",
        "        # Dépendances\n",
        "        echo \"📥 Installation dépendances\"\n",
        "        pip install --upgrade pip\n",
        "        pip install -r requirements.txt\n",
        "\n",
        "        # Vérification modèle\n",
        "        if [ ! -d \"arslm_lora\" ]; then\n",
        "          echo \"❌ ERREUR : dossier arslm_lora introuvable\"\n",
        "            exit 1\n",
        "            fi\n",
        "\n",
        "            # Lancement Streamlit\n",
        "            echo \"🧠 Lancement ARSLM\"\n",
        "            streamlit run streamlit_app.py \\\n",
        "              --server.port 8501 \\\n",
        "                --server.address 0.0.0.0\n",
        "                EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4lo3Jn2QrZ8"
      },
      "outputs": [],
      "source": [
        "!bash run_arslm.sh & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU09HkBeS7IP"
      },
      "outputs": [],
      "source": [
        "!git add run_arslm.sh\n",
        "!git commit -m \"Add run_arslm.sh bootstrap\"\n",
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_itijtYV7xA"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 🔥 Génération automatique d'arslm_lora\n",
        "# ================================\n",
        "!pip install --upgrade pip\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import os, json\n",
        "\n",
        "# ————————— Dataset simple (exemple)\n",
        "dataset = [\n",
        "    {\"instruction\": \"Qu'est-ce que ARSLM ?\", \"output\": \"ARSLM est un moteur AI léger pour générer des réponses intelligentes.\"},\n",
        "        {\"instruction\": \"Comment demander un congé ?\", \"output\": \"Connectez‑vous au portail RH, cliquez sur 'Congés', soumettez la demande.\"},\n",
        "            {\"instruction\": \"Quels sont les jours de télétravail autorisés ?\", \"output\": \"Jusqu'à 3 jours par semaine en accord avec votre manager.\"}\n",
        "            ]\n",
        "\n",
        "            # Sauvegarde JSON\n",
        "            with open(\"fine_tune.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "                # Charger dataset\n",
        "                ds = load_dataset(\"json\", data_files=\"fine_tune.json\")[\"train\"]\n",
        "\n",
        "                # ————————— Charger modèle de base (CPU/GPU)\n",
        "                base_model = \"distilgpt2\"\n",
        "                tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "                model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "                # ————————— Config LoRA\n",
        "                lora_config = LoraConfig(\n",
        "                    r=8,\n",
        "                        lora_alpha=32,\n",
        "                            target_modules=[\"c_attn\"],\n",
        "                                lora_dropout=0.05,\n",
        "                                    bias=\"none\",\n",
        "                                        task_type=\"CAUSAL_LM\"\n",
        "                                        )\n",
        "                                        model = get_peft_model(model, lora_config)\n",
        "\n",
        "                                        # ————————— Préparation tokenisation\n",
        "                                        def tokenize(ex):\n",
        "                                            prompt = f\"Instruction: {ex['instruction']}\\nRéponse:\"\n",
        "                                                full = prompt + \" \" + ex[\"output\"]\n",
        "                                                    return tokenizer(full, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "                                                    tokenized = ds.map(tokenize)\n",
        "\n",
        "                                                    # ————————— Fine‑tuning\n",
        "                                                    training_args = TrainingArguments(\n",
        "                                                        output_dir=\"arslm_lora\",\n",
        "                                                            per_device_train_batch_size=2,\n",
        "                                                                gradient_accumulation_steps=4,\n",
        "                                                                    num_train_epochs=4,\n",
        "                                                                        learning_rate=2e-4,\n",
        "                                                                            fp16=True,\n",
        "                                                                                logging_steps=10,\n",
        "                                                                                    save_total_limit=2\n",
        "                                                                                    )\n",
        "\n",
        "                                                                                    trainer = SFTTrainer(\n",
        "                                                                                        model=model,\n",
        "                                                                                            tokenizer=tokenizer,\n",
        "                                                                                                train_dataset=tokenized,\n",
        "                                                                                                    args=training_args\n",
        "                                                                                                    )\n",
        "\n",
        "                                                                                                    trainer.train()\n",
        "\n",
        "                                                                                                    # ————————— Sauvegarder modèle\n",
        "                                                                                                    model.save_pretrained(\"arslm_lora\")\n",
        "                                                                                                    tokenizer.save_pretrained(\"arslm_lora\")\n",
        "\n",
        "                                                                                                    print(\"✅ Modèle fine‑tuné généré dans arslm_lora/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1A9johSWtPP"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers datasets peft accelerate trl bitsandbytes -q\n",
        "!python generate_arslm_lora.pygenerate_arslm_lora.pygenerate_arslm_lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO8AGu3XW62q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2JCLYU7XB86"
      },
      "outputs": [],
      "source": [
        "!bash run_arslm.sh & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK8gY7XlXeTx"
      },
      "outputs": [],
      "source": [
        "git add arslm_lora\n",
        "git commit -m \"Add fine-tuned ARSLM LoRA model\"\n",
        "git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytGNKNyMYCeu"
      },
      "outputs": [],
      "source": [
        "# Liste les fichiers à la racine\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0QCwniRb-eX"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q transformers datasets peft accelerate trl bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbn0P31ScD0K"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "dataset = [\n",
        "    {\"instruction\": \"Qu'est-ce que ARSLM ?\", \"output\": \"ARSLM est un moteur AI léger pour générer des réponses intelligentes.\"},\n",
        "        {\"instruction\": \"Comment demander un congé ?\", \"output\": \"Connectez-vous au portail RH, cliquez sur 'Congés', soumettez la demande.\"},\n",
        "            {\"instruction\": \"Quels sont les jours de télétravail autorisés ?\", \"output\": \"Jusqu'à 3 jours par semaine en accord avec votre manager.\"}\n",
        "            ]\n",
        "\n",
        "            with open(\"fine_tune.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(dataset, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlt2qFoecMKp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Charger dataset\n",
        "ds = load_dataset(\"json\", data_files=\"fine_tune.json\")[\"train\"]\n",
        "\n",
        "# Modèle pré-entraîné\n",
        "base_model = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "# Configuration LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "        lora_alpha=32,\n",
        "            target_modules=[\"c_attn\"],  # modules de attention à fine-tuner\n",
        "                lora_dropout=0.05,\n",
        "                    bias=\"none\",\n",
        "                        task_type=\"CAUSAL_LM\"\n",
        "                        )\n",
        "\n",
        "                        model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c80db322"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Charger dataset\n",
        "ds = load_dataset(\"json\", data_files=\"fine_tune.json\")[\"train\"]\n",
        "\n",
        "# Modèle pré-entraîné\n",
        "base_model = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "# Configuration LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "        lora_alpha=32,\n",
        "            target_modules=[\"c_attn\"],  # modules de attention à fine-tuner\n",
        "                lora_dropout=0.05,\n",
        "                    bias=\"none\",\n",
        "                        task_type=\"CAUSAL_LM\"\n",
        "                        )\n",
        "\n",
        "                        model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxg5dFI3eJFJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCfU4E7NeaPd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "b4cc318d44344a7ba6ecb12991283022",
            "622096eac6a94285a9cb0cf1730a9064",
            "3874c42576074162b603f25ce5ccf8dc",
            "3d85a1bc004a4595b83ff3f439b31a27",
            "4efe87f305864d2fa0c5d7b2d6db830e",
            "31c746fd86a14d4aa5fdd745b848f7ed",
            "150fe937ef3e4130a5d792353227655a",
            "3b7c0767158a46abb5293c7f7c66ed1a",
            "0c51fea60df74db8a0e702efa10b3a03",
            "c118b8de5bb5402aa5583793200225bd",
            "c0543e6c30484053b5544875dc2c3395",
            "647aba443291437c96afc8b031efe984",
            "bb3dd6ef36a94c0b9c818fef88f8690e",
            "768cefe3faa14e52a20f59cb0cdc79ec",
            "491227fff50248b79a639e08d2aea71a",
            "f3e188fcfd8544cdbe11ae6316f9701f",
            "e279bdabac5046ae8c39f1e80928ee54",
            "924e2612dcd340a8ad3abe061674bcd8",
            "5e560cb8462f46f3b0e7795736b224c5",
            "d6a15601b0754500aa86aec1238e3021",
            "84c1f8118b274380a28d7117403f6b3c",
            "cec53a120ea94dd29c512e72fe55e64e",
            "e6335adbe69e4be782c972c0832e624f",
            "36245de608b546f28c262718719df35d",
            "2d231167d6134c20b7cb0fe86bc382b2",
            "5611f862289b467eb34631f5f36c339a",
            "900fd4e4ebe24a0e9c504838582d0e99",
            "11a104269be746fba16060dacca56beb",
            "13b5275bab2140f7922378954b1202b9",
            "928ca07445554efcb7d60f0d562eb39a",
            "cc10d314a8204da29627b609a552915f",
            "c4187ac6bff2461e8bf954431d9f698e",
            "ed776daaf11b474380106d7d1cf46d27",
            "c28f9ab574874200b6b32b119d2b9c2f",
            "a9c976b8c2b849a0bd7cf8413a07a46c",
            "da01d3a90a1c472792f2bf0c39cd165b",
            "72043bd724a246d1801c3b37db327d94",
            "4bf1fe657149481099e5b48048bcf4c0",
            "d147b783b5494f86a8ddf22d35e5fd83",
            "a10f86b25d7346a0b2b02024953288b0",
            "a7d44c43e9854caba4030efee502713e",
            "f657abb627ab4e7882fee3030855f9dd",
            "c1a985982acb48ecbf91a06814ed96ae",
            "c1fffe67465146a587f681206f760d2b",
            "e59709c6efcf4cf4ad2a45c599e0d824",
            "38c6be84668f4b7c93fe52ea7c411c27",
            "1a6d7ab5b28748d1a4f56829e91a375d",
            "57f4d08d76ec4e3d87fc03f9f760c264",
            "e3adb302722b4660b8a4d8371217cd41",
            "e6bbc487ed7d4f35a51a8c060f68cd77",
            "024f13f953ab4a87b4ba7f5f28ecd9b2",
            "238f03e82db748b5b1254b0ccad5549d",
            "527d42d10f364db89ec5a8e9d825afce",
            "2290d9923e6a4e5d84c51cfb5e709c1e",
            "9a6251bee3eb46e9a99176bdd24111b4",
            "4b0e99e40b0e4cea9dbe1ea89d7042c7",
            "1f43a414b56144e0b12755cd852e20b4",
            "eb6d693a70ea43178b7f21c36bf896da",
            "1c2a0abd88cc424a91dec23def6b69ff",
            "832049e112d644c5a56a572a0a5b53a1",
            "5b8d277c5289401a802c09a0a68140b9",
            "1c26a350440d4ce997cd29d995de5bb6",
            "93878fcab90a45db9a092f226ca4922a",
            "0786ab84e8544ef4a736fd1375218b1b",
            "f40d026af25c485196353a81fb8b6f31",
            "921f8f502c53490e9f7ded1f453d5bc0",
            "59b56391b28347abb308b8f3b04c638c",
            "c30541c4c3cf46b59bb936241dd46dbe",
            "5ba4cbaebd2d493da78ecea75adf4401",
            "8209a4f3c59f4784868e91fd82ac6c1f",
            "0a995516a0094f149fc20539bd312378",
            "5e83cef65c7b4173aa3867634ac6be5c",
            "b061fca9dc224750889912c479bfd9c4",
            "aa7d2913c966482b82cf20fb6211c3c2",
            "66cd7fc8ae4643cf864cb7b8151d03ea",
            "d3dff3e9e68945fa97542d707a045c63",
            "9cf38f9f0e3c40c6ae21280490c65f23"
          ]
        },
        "id": "dd35396d",
        "outputId": "0f7fa1dc-1c77-4133-ce0a-a4b5860f8e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading wikitext-2-raw-v1 dataset from Hugging Face...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4cc318d44344a7ba6ecb12991283022"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "647aba443291437c96afc8b031efe984"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6335adbe69e4be782c972c0832e624f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c28f9ab574874200b6b32b119d2b9c2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e59709c6efcf4cf4ad2a45c599e0d824"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b0e99e40b0e4cea9dbe1ea89d7042c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b56391b28347abb308b8f3b04c638c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing first 10,000 lines to /tmp/large_corpus.txt...\n",
            "Dataset extraction and saving complete.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Define the output file path\n",
        "corpus_file_path = \"/tmp/large_corpus.txt\"\n",
        "\n",
        "# Load the 'wikitext-2-raw-v1' dataset\n",
        "print(\"Loading wikitext-2-raw-v1 dataset from Hugging Face...\")\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "\n",
        "# Access the 'train' split and extract 'text' column\n",
        "train_texts = dataset['train']['text']\n",
        "\n",
        "# Filter out empty strings and strip whitespace\n",
        "filtered_texts = [line.strip() for line in train_texts if line.strip()]\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(corpus_file_path), exist_ok=True)\n",
        "\n",
        "# Write the first 10,000 lines to the specified file\n",
        "print(f\"Writing first 10,000 lines to {corpus_file_path}...\")\n",
        "with open(corpus_file_path, 'w', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(filtered_texts):\n",
        "        if i < 10000:\n",
        "            f.write(line + '\\n')\n",
        "        else:\n",
        "            break\n",
        "print(\"Dataset extraction and saving complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42fc6937"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 10000 # Max lines to load from corpus_file for demo - UPDATED TO 10,000\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance AND max_seq_len\n",
        "def collate_batch(batch_texts: List[str], tokenizer, max_seq_len: int):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        # Truncate if longer than max_seq_len, ensuring EOS is present\n",
        "        if len(ids) > max_seq_len:\n",
        "            ids = ids[:max_seq_len - 1] + [tokenizer.eos_token_id] if max_seq_len > 1 else ids[:max_seq_len]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max_len within the batch, which will be at most max_seq_len due to truncation\n",
        "    max_len_in_batch = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len_in_batch), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer, cfg.max_seq_len))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "# Explicitly call train_demo() to ensure it runs and saves the model\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "model, tokenizer = train_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f7d6dee"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération\n",
        "    new_input_text = \"Le langage naturel est\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, new_input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {new_input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2468cb94"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération\n",
        "    new_input_text = \"Le langage naturel est\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, new_input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {new_input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a7KoJ3Vcok3"
      },
      "outputs": [],
      "source": [
        "def tokenize(ex):\n",
        "      prompt = f\"Instruction: {ex['instruction']}\\nRéponse:\"\n",
        "          full = prompt + \" \" + ex[\"output\"]\n",
        "              return tokenizer(full, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "              tokenized = ds.map(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvDFFBWLcuaI"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"arslm_lora\",\n",
        "        per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=4,\n",
        "                num_train_epochs=4,\n",
        "                    learning_rate=2e-4,\n",
        "                        fp16=True,\n",
        "                            logging_steps=10,\n",
        "                                save_total_limit=2\n",
        "                                )\n",
        "\n",
        "                                trainer = SFTTrainer(\n",
        "                                    model=model,\n",
        "                                        tokenizer=tokenizer,\n",
        "                                            train_dataset=tokenized,\n",
        "                                                args=training_args\n",
        "                                                )\n",
        "\n",
        "                                                trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-M10ezSc3cd"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"arslm_lora\")\n",
        "tokenizer.save_pretrained(\"arslm_lora\")\n",
        "print(\"✅ Modèle ARSLM LoRA créé dans le dossier 'arslm_lora/'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERBSEHVSc66A"
      },
      "outputs": [],
      "source": [
        "!ls -l arslm_lora/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3Zela64dnvu"
      },
      "outputs": [],
      "source": [
        "config.json\n",
        "pytorch_model.bin  (ou model.safetensors)\n",
        "tokenizer.json\n",
        "tokenizer_config.json\n",
        "special_tokens_map.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmS5G83CdtFj"
      },
      "outputs": [],
      "source": [
        "!git add arslm_lora\n",
        "!git commit -m \"Add fine-tuned ARSLM LoRA model\"\n",
        "!git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2WyolNsdGw6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO8wNqzac-ql"
      },
      "outputs": [],
      "source": [
        "!ls -l arslm_lora/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95x60fgGRS6H"
      },
      "outputs": [],
      "source": [
        "!ls -l run_arslm.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ_0z8K4QnUn"
      },
      "outputs": [],
      "source": [
        "!chmod +x run_arslm.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e50c1fe2"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération en anglais\n",
        "    english_input_text = \"Artificial intelligence is a powerful tool for\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_english_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, english_input_text)\n",
        "\n",
        "    print(f\"English Input Text: {english_input_text}\")\n",
        "    print(f\"Generated English Output: {generated_english_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The model, tokenizer, or configuration were not found. Please ensure training was completed and files were saved.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during deployment: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e032b1b"
      },
      "outputs": [],
      "source": [
        "cfg_instance = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # Charger le modèle, le tokenizer et la configuration entraînés\n",
        "    loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "        os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "        cfg_instance.tokenizer_save_dir,\n",
        "        cfg_instance.config_save_path,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Définir le nouveau texte d'entrée pour la génération\n",
        "    new_input_text = \"L'intelligence artificielle est\"\n",
        "\n",
        "    # Déployer le modèle pour générer du texte\n",
        "    generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, new_input_text)\n",
        "\n",
        "    print(f\"Texte d'entrée: {new_input_text}\")\n",
        "    print(f\"Texte généré: {generated_output}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur: Le modèle, le tokenizer ou la configuration n'ont pas été trouvés. Veuillez vous assurer que l'entraînement a été effectué et que les fichiers ont été sauvegardés.\")\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur inattendue est survenue lors du déploiement : {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27893386"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to execute the `train_demo()` function again. The provided code block in the user's prompt contains the complete and updated Python script, including all necessary class definitions and the `train_demo()` function call within the `if __name__ == \"__main__\":` block. Executing this will re-run the training process with the larger corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3fd01f9"
      },
      "source": [
        "# Task\n",
        "Ajouter un attribut `max_seq_len` à la classe `Config` avec une valeur par défaut de 512 et modifier la fonction `collate_batch` pour tronquer les séquences à cette longueur maximale lors de l'encodage des textes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dc09332"
      },
      "source": [
        "## Ajouter max_seq_len à Config\n",
        "\n",
        "### Subtask:\n",
        "Ajouter un attribut `max_seq_len` à la classe `Config` avec une valeur par défaut de 512 (ou une autre valeur appropriée basée sur les contraintes du modèle).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b2ee18b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the `Config` class to include `max_seq_len` as requested by the subtask. This centralizes the sequence length setting, making it easier to manage model constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8a62c8e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 1000 # Max lines to load from corpus_file for demo\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                # The input to subsequent layers is the hidden state of the previous layer\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model\n",
        "    best_val_loss = float('inf')\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save the model if this is the best validation loss so far\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57994f1b"
      },
      "source": [
        "# Task\n",
        "Install all necessary Python libraries (streamlit, pyngrok, transformers, torch, datasets, peft, accelerate, gitpython), clone the 'https://github.com/benjaminpolydeq/ARSLM.git' repository, ensure the ARSLM model and its tokenizer are ready for use (either loaded or fine-tuned), launch the Streamlit application with Ngrok to expose port 8501, and then display the public URL of the deployed Streamlit application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35607cf1"
      },
      "source": [
        "## Installer les dépendances nécessaires\n",
        "\n",
        "### Subtask:\n",
        "Installer toutes les bibliothèques Python requises, telles que streamlit, pyngrok, transformers, torch, datasets, peft, accelerate et gitpython, pour s'assurer que l'environnement est prêt pour le déploiement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2fb24fc"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to install all the required Python libraries mentioned in the subtask. This will prepare the environment for deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbeda274"
      },
      "outputs": [],
      "source": [
        "print(\"Installing required Python libraries...\")\n",
        "!pip install -q streamlit pyngrok transformers torch datasets peft accelerate gitpython\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b69da2"
      },
      "source": [
        "# Task\n",
        "Okay, I understand. The previous action, installing the necessary libraries, has been approved.\n",
        "\n",
        "The next step, as per your latest request, is to clone the `ARSLM` repository, prepare the model (fine-tune if necessary), launch the Streamlit application, and display its public URL. I will execute the relevant code cell to perform these actions.\n",
        "\n",
        "```python\n",
        "# ================================\n",
        "# 🚀 Tout-en-un ARSLM Colab\n",
        "# ================================\n",
        "\n",
        "# 1️⃣ Installer les dépendances\n",
        "!pip install -q streamlit pyngrok transformers torch datasets peft accelerate gitpython\n",
        "\n",
        "# 2️⃣ Importer modules\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "from git import Repo\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# 3️⃣ Cloner le repo ARSLM\n",
        "GITHUB_REPO = \"https://github.com/benjaminpolydeq/ARSLM.git\"\n",
        "LOCAL_DIR = \"ARSLM\"\n",
        "if not os.path.exists(LOCAL_DIR):\n",
        "    print(\"Clonage du repo ARSLM depuis GitHub...\")\n",
        "    Repo.clone_from(GITHUB_REPO, LOCAL_DIR)\n",
        "else:\n",
        "    print(\"Repo ARSLM déjà présent\")\n",
        "os.chdir(LOCAL_DIR)\n",
        "\n",
        "# 4️⃣ Préparer datasets par défaut (RH, juridique, medical, default)\n",
        "datasets_templates = {\n",
        "    \"default\": [{\"instruction\": \"Qu'est-ce que ARSLM ?\", \"output\": \"ARSLM est un moteur AI léger pour générer des réponses intelligentes.\"}],\n",
        "    \"rh\": [\n",
        "        {\"instruction\": \"Comment demander un congé ?\", \"output\": \"Connectez-vous au portail RH, cliquez sur 'Congés', puis soumettez votre demande.\"},\n",
        "        {\"instruction\": \"Quelle est la politique de télétravail ?\", \"output\": \"3 jours max par semaine, accord manager requis.\"}\n",
        "    ],\n",
        "    \"juridique\": [{\"instruction\": \"Quelles sont les obligations légales ?\", \"output\": \"Consultez le code du travail applicable et les régulations locales.\"}],\n",
        "    \"medical\": [{\"instruction\": \"Comment prendre rendez-vous ?\", \"output\": \"Utilisez le portail médical ou appelez la réception.\"}]\n",
        "}\n",
        "\n",
        "# 5️⃣ Vérifier modèle fine-tuné\n",
        "MODEL_DIR = \"arslm_llm\"\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    print(\"⚠️ Modèle non trouvé. Tentative récupération depuis Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/arslm_llm\"\n",
        "    if os.path.exists(DRIVE_PATH):\n",
        "        !cp -r /content/drive/MyDrive/arslm_llm ./arslm_llm\n",
        "        print(\"✅ Modèle récupéré depuis Drive\")\n",
        "    else:\n",
        "        print(\"❌ Aucun modèle trouvé. Lancement d'un fine-tuning rapide...\")\n",
        "\n",
        "        # Dataset minimal pour fine-tuning\n",
        "        selected_dataset = datasets_templates[\"default\"]\n",
        "        with open(\"fine_tune.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(selected_dataset, f, ensure_ascii=False, indent=2)\n",
        "        dataset = load_dataset(\"json\", data_files=\"fine_tune.json\")[\"train\"]\n",
        "\n",
        "        # Charger modèle de base + LoRA\n",
        "        base_model = \"distilgpt2\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "        model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "        lora_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"c_attn\"],\n",
        "            lora_dropout=0.05,\n",
        "            task_type=TaskType.CAUSAL_LM\n",
        "        )\n",
        "        model = get_peft_model(model, lora_config)\n",
        "\n",
        "        # Tokenisation\n",
        "        def preprocess(example):\n",
        "            input_text = f\"Instruction: {example['instruction']}\\nRéponse:\"\n",
        "            target_text = example['output']\n",
        "            return tokenizer(input_text + \" \" + target_text, truncation=True, padding=\"max_length\", max_length=128)\n",
        "        tokenized_dataset = dataset.map(preprocess)\n",
        "\n",
        "        # Fine-tuning rapide\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"arslm_llm\",\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=2,\n",
        "            num_train_epochs=1,\n",
        "            learning_rate=1e-4,\n",
        "            save_total_limit=1,\n",
        "            logging_steps=10,\n",
        "            fp16=True,\n",
        "            push_to_hub=False\n",
        "        )\n",
        "        trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset)\n",
        "        trainer.train()\n",
        "\n",
        "        # Sauvegarder modèle\n",
        "        model.save_pretrained(\"arslm_llm\")\n",
        "        tokenizer.save_pretrained(\"arslm_llm\")\n",
        "        print(\"✅ Fine-tuning terminé et modèle sauvegardé dans arslm_llm/\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ Modèle fine-tuné trouvé localement\")\n",
        "\n",
        "# 6️⃣ Déploiement Streamlit via ngrok\n",
        "!pkill streamlit\n",
        "public_url = ngrok.connect(port='8501')\n",
        "print(\"🌐 Lien public Streamlit :\", public_url)\n",
        "os.system(\"streamlit run streamlit_app.py &\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49edd2e7"
      },
      "source": [
        "## Enforcer max_seq_len dans collate_batch\n",
        "\n",
        "### Subtask:\n",
        "Modify the `collate_batch` function to explicitly truncate sequences to `cfg.max_seq_len` before padding to resolve the sequence length warning, optimize memory usage, and improve computation speed by avoiding excessively long sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e46cb9f"
      },
      "source": [
        "## Implémenter l'arrêt anticipé (Early Stopping)\n",
        "\n",
        "### Subtask:\n",
        "Ajouter une logique d'arrêt anticipé dans la fonction train_demo pour surveiller la perte de validation et arrêter l'entraînement si la perte ne s'améliore plus après un certain nombre d'époques. Cela permettra d'éviter le surapprentissage et d'économiser des ressources de calcul.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aec9491b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will update the `Config` class to include the `early_stopping_patience` attribute and then modify the `train_demo` function to implement the early stopping logic based on validation loss, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f8e7447"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "        self.early_stopping_patience = 5 # Added early_stopping_patience\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f541d904"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed due to an incomplete assignment in the `Config` class for `self.generation_context`. I will fix this syntax error by providing a default string value and then implement the early stopping logic in the `train_demo` function as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c3fdbff"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "from transformers import BertTokenizer # Import BertTokenizer here\n",
        "from torch.optim.lr_scheduler import StepLR # Import StepLR\n",
        "import os # Import os for file operations\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration Class\n",
        "# ----------------------------\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the ARSLM model and training.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model Parameters\n",
        "        self.emb_dim = 64\n",
        "        self.hidden_dim = 128\n",
        "        self.num_layers = 2\n",
        "        self.dropout_prob = 0.1\n",
        "        self.max_seq_len = 512 # Added max_seq_len\n",
        "\n",
        "        # Training Parameters\n",
        "        self.learning_rate = 1e-3\n",
        "        self.n_epochs = 200\n",
        "        self.batch_size = 8\n",
        "        self.lr_scheduler_step_size = 50\n",
        "        self.lr_scheduler_gamma = 0.5\n",
        "        self.clip_grad_norm = 1.0\n",
        "        self.early_stopping_patience = 5 # Added early_stopping_patience\n",
        "\n",
        "        # Generation Parameters\n",
        "        self.max_new_tokens = 15\n",
        "        self.temperature = 1.0\n",
        "        self.top_k = 50\n",
        "        self.generation_context = \"hello world\"\n",
        "\n",
        "        # Data Parameters\n",
        "        self.corpus_file = \"/tmp/large_corpus.txt\"\n",
        "        self.max_corpus_lines = 1000 # Max lines to load from corpus_file for demo\n",
        "        self.train_test_split_ratio = 0.2\n",
        "        self.train_val_split_ratio = 0.5 # 0.1 of total\n",
        "        self.random_state = 42\n",
        "\n",
        "        # Saving/Loading Paths\n",
        "        self.model_save_dir = \"./model_checkpoint\"\n",
        "        self.tokenizer_save_dir = \"./tokenizer_checkpoint\"\n",
        "        self.config_save_path = os.path.join(self.model_save_dir, \"config.pt\") # Path to save config\n",
        "\n",
        "# ----------------------------\n",
        "# Advanced Tokenizer (using transformers)\n",
        "# ----------------------------\n",
        "class AdvancedTokenizer:\n",
        "    def __init__(self, vocab_file=None, pretrained_model_name_or_path='bert-base-uncased'):\n",
        "        if vocab_file:\n",
        "             self.tokenizer = BertTokenizer(vocab_file)\n",
        "        else:\n",
        "             # Check if pretrained_model_name_or_path is a directory for a saved tokenizer\n",
        "             if os.path.isdir(pretrained_model_name_or_path):\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "             else:\n",
        "                 self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        self.vocab = list(self.tokenizer.vocab.keys())\n",
        "        self.stoi = self.tokenizer.vocab\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "\n",
        "        self.pad_token = self.tokenizer.pad_token\n",
        "        self.unk_token = self.tokenizer.unk_token\n",
        "        self.bos_token = self.tokenizer.cls_token # Using [CLS] as BOS for BERT-like tokenizers\n",
        "        self.eos_token = self.tokenizer.sep_token # Using [SEP] as EOS for BERT-like tokenizers\n",
        "\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.unk_token_id = self.tokenizer.unk_token_id\n",
        "        self.bos_token_id = self.tokenizer.cls_token_id\n",
        "        self.eos_token_id = self.tokenizer.sep_token_id\n",
        "\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "# ----------------------------\n",
        "# ARSCell: core adaptive cell\n",
        "# ----------------------------\n",
        "class ARSCell(nn.Module):\n",
        "    \"\"\"\n",
        "    ARSCell: computes next hidden state from h_{t-2}, h_{t-1}, and input embedding x_{t-1}\n",
        "    Implements an 'adapt' mechanism inspired by BenPolySeq:\n",
        "      h_t = h_{t-1} + gate * transform(h_{t-1}, h_{t-2}, x)\n",
        "    where gate is a function of the difference (h_{t-1} - h_{t-2}) and context.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int, dropout_prob: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # transform candidate from concat([h_{t-1}, h_{t-2}, x])\n",
        "        self.candidate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        )\n",
        "        # gate network produces scalar gating factor in (0,1)\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2 + emb_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # small residual projection\n",
        "        self.res_proj = nn.Linear(emb_dim, hidden_dim)\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, h_prev2, h_prev1, x_embed):\n",
        "        # h_prev2, h_prev1: (batch, hidden_dim)\n",
        "        # x_embed: (batch, emb_dim)\n",
        "        # difference signal\n",
        "        diff = h_prev1 - h_prev2    # (batch, hidden_dim)\n",
        "        # context vector (concat)\n",
        "        ctx = torch.cat([h_prev1, h_prev2, x_embed], dim=-1)\n",
        "        candidate = self.candidate_mlp(ctx)                       # (batch, hidden_dim)\n",
        "        gate = self.gate_net(ctx).squeeze(-1)                     # (batch,)\n",
        "        # incorporate residual from input embedding\n",
        "        residual = self.res_proj(x_embed)\n",
        "        # ARS-style update: adaptive addition of candidate scaled by gate\n",
        "        h_t = h_prev1 + gate.unsqueeze(-1) * candidate + 0.1 * residual\n",
        "        # Apply dropout\n",
        "        h_t = self.dropout(h_t)\n",
        "        # optional normalization\n",
        "        h_t = F.layer_norm(h_t, (self.hidden_dim,))\n",
        "        return h_t, gate\n",
        "\n",
        "# ----------------------------\n",
        "# ARSLM model: embed -> ARSCell(s) -> attention -> head\n",
        "# ----------------------------\n",
        "class ARSLM(nn.Module):\n",
        "    # Modified to accept a tokenizer instance, include attention, and support multiple layers\n",
        "    def __init__(self, tokenizer, emb_dim=64, hidden_dim=128, num_layers=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer # Store tokenizer instance\n",
        "        self.vocab_size = len(tokenizer) # Get vocab size from tokenizer\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding(self.vocab_size, emb_dim)\n",
        "        # Create a list of ARSCells\n",
        "        # Pass dropout_prob to ARSCell constructor\n",
        "        self.cells = nn.ModuleList([ARSCell(emb_dim if i == 0 else hidden_dim, hidden_dim, dropout_prob=dropout_prob) for i in range(num_layers)])\n",
        "\n",
        "        # Simple additive attention mechanism, applied after the last layer's hidden states\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        We compute autoregressively, passing hidden states through layers.\n",
        "        Returns logits (batch, seq_len, vocab).\n",
        "        \"\"\"\n",
        "        bsz, seq_len = input_ids.shape\n",
        "        emb = self.emb(input_ids)  # (b, seq, emb)\n",
        "        device = emb.device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        # We need two previous states for each layer's ARSCell\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "        logits = []\n",
        "        gates = [] # Store gates from the last layer\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:, t, :] # embedding at position t\n",
        "            h_t_input = x_t # Input to the first layer is the embedding\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1] # Use the output of the previous layer\n",
        "\n",
        "                h_t, gate = cell(h_prev2, h_prev1, h_t_input)\n",
        "\n",
        "                # Store current layer's hidden state as input for the next layer in the next time step\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next time step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1: # Only store gates from the last layer\n",
        "                    gates.append(gate.unsqueeze(1))\n",
        "\n",
        "            # After processing all layers for time step t, the output is the hidden state of the last layer\n",
        "            last_layer_h_t = current_layer_hidden_states[-1]\n",
        "            all_last_layer_hidden_states.append(last_layer_h_t.unsqueeze(1))\n",
        "\n",
        "            # Apply attention and head after the last layer's hidden state\n",
        "            # Causal Attention: attend over hidden states up to current time step t from the last layer\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, t+1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim) - query is the current last layer hidden state\n",
        "\n",
        "            # Calculate attention scores\n",
        "            scores = self.attention(last_layer_history) # (b, t+1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, t+1, 1)\n",
        "\n",
        "            # Apply attention weights\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            # Combine last layer's current hidden state with context vector before the head\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logit = self.head(attended_h_t) # predict token at t (or next token)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "\n",
        "        logits = torch.cat(logits, dim=1)          # (b, seq, vocab)\n",
        "        gates = torch.cat(gates, dim=1)            # (b, seq) # Gates from the last layer\n",
        "        return logits, gates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=20, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        idx: (batch, current_seq_len) - input context (token ids)\n",
        "        Returns extended token ids.\n",
        "        Modified for multiple layers, attention during generation, and optional top-k sampling.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        bsz, seq_len = idx.shape\n",
        "        device = next(self.parameters()).device\n",
        "        hidden_dim = self.cells[0].hidden_dim\n",
        "\n",
        "        # Initialize hidden states for each layer\n",
        "        h_prev2_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "        h_prev1_list = [torch.zeros(bsz, hidden_dim, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store hidden states for causal attention (from the last layer)\n",
        "        all_last_layer_hidden_states = []\n",
        "\n",
        "        # Process initial context to get starting hidden states for each layer\n",
        "        input_ids = idx.clone()\n",
        "        emb = self.emb(input_ids) # (b, seq_len, emb_dim)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = emb[:,t,:] # embedding at position t\n",
        "            h_t_input = x_t\n",
        "\n",
        "            current_layer_hidden_states = []\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "        out_ids = input_ids.tolist()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Input for the first layer's ARSCell is the embedding of the last generated token\n",
        "            last_token_ids = torch.tensor([ids[-1] for ids in out_ids], dtype=torch.long, device=device).unsqueeze(1)\n",
        "            x_embed = self.emb(last_token_ids).squeeze(1) # (b, emb_dim)\n",
        "\n",
        "            h_t_input = x_embed\n",
        "            current_layer_hidden_states = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "                cell = self.cells[layer]\n",
        "                h_prev2 = h_prev2_list[layer]\n",
        "                h_prev1 = h_prev1_list[layer]\n",
        "\n",
        "                if layer > 0:\n",
        "                    h_t_input = current_layer_hidden_states[-1]\n",
        "\n",
        "                h_t, _ = cell(h_prev2, h_prev1, h_t_input)\n",
        "                current_layer_hidden_states.append(h_t)\n",
        "\n",
        "                # Update previous hidden states for the next generation step for this layer\n",
        "                h_prev2_list[layer] = h_prev1\n",
        "                h_prev1_list[layer] = h_t\n",
        "\n",
        "                if layer == self.num_layers - 1:\n",
        "                    # Add the new last layer hidden state to the history for attention\n",
        "                    all_last_layer_hidden_states.append(h_t.unsqueeze(1))\n",
        "\n",
        "\n",
        "            # After processing all layers for the new token, apply attention and head\n",
        "            last_layer_h_t = current_layer_hidden_states[-1] # Hidden state of the last layer for the new token\n",
        "\n",
        "            # Causal Attention over all last layer hidden states history\n",
        "            last_layer_history = torch.cat(all_last_layer_hidden_states, dim=1) # (b, current_seq_len + 1, hidden_dim)\n",
        "            query = last_layer_h_t.unsqueeze(1) # (b, 1, hidden_dim)\n",
        "\n",
        "            scores = self.attention(last_layer_history) # (b, current_seq_len + 1, 1)\n",
        "            attention_weights = F.softmax(scores, dim=1) # (b, current_seq_len + 1, 1)\n",
        "\n",
        "            context_vector = torch.sum(attention_weights * last_layer_history, dim=1) # (b, hidden_dim)\n",
        "\n",
        "            attended_h_t = last_layer_h_t + context_vector\n",
        "\n",
        "            logits = self.head(attended_h_t) / max(1e-6, temperature)\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                # get top k logits\n",
        "                top_k = min(max(top_k, 1), logits.size(-1))  # Clamp k to be at least 1 and at most vocab size\n",
        "                # Remove all tokens with a probability less than the top-k'th token's probability\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Check if all probabilities are zero after filtering/temperature, if so, fall back to argmax\n",
        "            if torch.all(logits == -float('Inf')):\n",
        "                # This means all tokens were filtered out or resulted in 0 probability. Choose a random valid token or a default.\n",
        "                # For now, let's just pick a generic unk_token or a random one if unk is not available/appropriate\n",
        "                if self.tokenizer.unk_token_id is not None:\n",
        "                    next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                else:\n",
        "                    # Fallback to random if unk token is not defined\n",
        "                    next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # Ensure that if probabilities sum to zero (e.g., all logits were -inf before soft max),\n",
        "                # we handle it gracefully, though the above check should prevent it.\n",
        "                if torch.sum(probs) == 0: # This case should ideally not happen after the -inf check\n",
        "                    if self.tokenizer.unk_token_id is not None:\n",
        "                        next_token = torch.tensor([self.tokenizer.unk_token_id] * bsz, device=device)\n",
        "                    else:\n",
        "                        next_token = torch.randint(0, self.vocab_size, (bsz,), device=device)\n",
        "                else:\n",
        "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "\n",
        "\n",
        "            # append the newly generated token\n",
        "            for i in range(bsz):\n",
        "                out_ids[i].append(int(next_token[i].item()))\n",
        "\n",
        "\n",
        "        return out_ids\n",
        "\n",
        "# ----------------------------\n",
        "# TextDataset class for structured data handling\n",
        "# ----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts: list, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Modified to accept and use the tokenizer instance\n",
        "def collate_batch(batch_texts: List[str], tokenizer):\n",
        "    # Encode each text, add BOS/EOS, and convert to tensor\n",
        "    encoded = []\n",
        "    for t in batch_texts:\n",
        "        ids = [tokenizer.bos_token_id] + tokenizer.encode(t) + [tokenizer.eos_token_id]\n",
        "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
        "\n",
        "    # Pad to max len within the batch\n",
        "    max_len = max([x.size(0) for x in encoded])\n",
        "    padded_batch = torch.full((len(encoded), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, x in enumerate(encoded):\n",
        "        padded_batch[i, :x.size(0)] = x\n",
        "\n",
        "    # Prepare inputs and targets for language modeling\n",
        "    inputs = padded_batch[:, :-1]\n",
        "    targets = padded_batch[:, 1:]\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Modified to use AdvancedTokenizer and handle multiple layers, and DataLoaders\n",
        "def train_demo():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize configuration\n",
        "    cfg = Config()\n",
        "\n",
        "    corpus_file = cfg.corpus_file\n",
        "    texts = []\n",
        "    if os.path.exists(corpus_file):\n",
        "        print(f\"Loading corpus from {corpus_file}...\")\n",
        "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i < cfg.max_corpus_lines:\n",
        "                    texts.append(line.strip())\n",
        "                else:\n",
        "                    break\n",
        "        print(f\"Loaded {len(texts)} lines.\")\n",
        "    else:\n",
        "        print(f\"Corpus file not found at {corpus_file}. Using a small toy corpus instead.\")\n",
        "        texts = [\n",
        "            \"hello world this is ars\",\n",
        "            \"the system adapts to its history\",\n",
        "            \"benpolyseq demonstrates adaptive sequences\",\n",
        "            \"ars can inspire new network protocols\",\n",
        "            \"self optimizing systems are possible\"\n",
        "        ]\n",
        "\n",
        "    tokenizer = AdvancedTokenizer() # Initialize AdvancedTokenizer\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # Modified Data Splitting Logic\n",
        "    num_samples = len(texts)\n",
        "    if num_samples < 3:\n",
        "        print(f\"Warning: Corpus size ({num_samples}) is too small for a proper train/validation/test split. Using all data for training.\")\n",
        "        train_texts = texts\n",
        "        val_texts = []\n",
        "        test_texts = []\n",
        "    else:\n",
        "        # First split: train_val and test\n",
        "        train_val_texts, test_texts = train_test_split(texts, test_size=cfg.train_test_split_ratio, random_state=cfg.random_state)\n",
        "        # Defensive check for test_texts being empty\n",
        "        if not test_texts and train_val_texts:\n",
        "            test_texts.append(train_val_texts.pop()) # Move one item to test if test is empty and train_val is not\n",
        "\n",
        "        # Second split: train and validation from train_val\n",
        "        if len(train_val_texts) >= 2: # Need at least two samples to split into train and val\n",
        "            val_split_size = cfg.train_val_split_ratio\n",
        "            train_texts, val_texts = train_test_split(train_val_texts, test_size=val_split_size, random_state=cfg.random_state)\n",
        "            # Defensive check for val_texts being empty\n",
        "            if not val_texts and train_texts:\n",
        "                val_texts.append(train_texts.pop()) # Move one item to val if val is empty and train is not\n",
        "        else:\n",
        "            train_texts = train_val_texts\n",
        "            val_texts = []\n",
        "\n",
        "    print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer)\n",
        "    test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "                               collate_fn=lambda b: collate_batch(b, tokenizer))\n",
        "\n",
        "    model = ARSLM(tokenizer, emb_dim=cfg.emb_dim, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers, dropout_prob=cfg.dropout_prob).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=cfg.lr_scheduler_step_size, gamma=cfg.lr_scheduler_gamma)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "    n_epochs = cfg.n_epochs\n",
        "    if len(train_texts) == 0: # Check if train_texts is empty, not inputs.numel()\n",
        "        print(\"No training data available. Skipping training.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    # Initialize best validation loss for saving the best model and early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0 # Initialize patience counter\n",
        "\n",
        "    # Create directories for saving if they don't exist\n",
        "    os.makedirs(cfg.model_save_dir, exist_ok=True)\n",
        "    os.makedirs(cfg.tokenizer_save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_loader)\n",
        "        if num_batches > 0:\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                logits, gates = model(inputs)\n",
        "                b, seq, v = logits.shape\n",
        "                loss = loss_fn(logits.view(b*seq, v), targets.reshape(b*seq))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / num_batches\n",
        "        else:\n",
        "            avg_train_loss = float('inf') # Set to inf if no training batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 50 == 0 or epoch==0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "            print(f\"Current learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Validation step with check for empty loader\n",
        "            if len(val_loader) > 0:\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs_val, targets_val in val_loader:\n",
        "                        inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                        logits_val, _ = model(inputs_val)\n",
        "                        b_val, seq_val, v_val = logits_val.shape\n",
        "                        loss_val = loss_fn(logits_val.view(b_val*seq_val, v_val), targets_val.reshape(b_val*seq_val))\n",
        "                        total_val_loss += loss_val.item()\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Early stopping logic\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    patience_counter = 0 # Reset patience since improvement was observed\n",
        "                    print(f\"Saving best model with validation loss: {best_val_loss:.4f}\")\n",
        "                    torch.save(model.state_dict(), os.path.join(cfg.model_save_dir, \"best_model.pt\"))\n",
        "                    tokenizer.tokenizer.save_pretrained(cfg.tokenizer_save_dir)\n",
        "                    # Save the configuration as well\n",
        "                    torch.save(cfg, cfg.config_save_path)\n",
        "                else:\n",
        "                    patience_counter += 1 # Increment patience as no improvement\n",
        "                    print(f\"Validation loss did not improve. Patience: {patience_counter}/{cfg.early_stopping_patience}\")\n",
        "\n",
        "                if patience_counter >= cfg.early_stopping_patience:\n",
        "                    print(f\"Early stopping triggered after {cfg.early_stopping_patience} epochs without improvement.\")\n",
        "                    break # Exit training loop\n",
        "\n",
        "            else:\n",
        "                print(\"No validation data available. Skipping validation.\")\n",
        "                avg_val_loss = float('inf')\n",
        "\n",
        "    print(\"\\n=== Evaluation ===\")\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    total_perplexity = 0\n",
        "    if len(test_loader) > 0:\n",
        "        with torch.no_grad():\n",
        "            for inputs_test, targets_test in test_loader:\n",
        "                inputs_test, targets_test = inputs_test.to(device), targets_test.to(device)\n",
        "                logits_test, _ = model(inputs_test)\n",
        "                b_test, seq_test, v_test = logits_test.shape\n",
        "                loss_test = loss_fn(logits_test.view(b_test*seq_test, v_test), targets_test.reshape(b_test*seq_test))\n",
        "                total_test_loss += loss_test.item()\n",
        "                total_perplexity += torch.exp(loss_test).item()\n",
        "\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        avg_test_perplexity = total_perplexity / len(test_loader)\n",
        "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "        print(f\"Perplexity on test data: {avg_test_perplexity:.4f}\")\n",
        "    else:\n",
        "        print(\"No test data available. Skipping test evaluation.\")\n",
        "        avg_test_loss = float('inf')\n",
        "        avg_test_perplexity = float('inf')\n",
        "\n",
        "    # demo generation\n",
        "    context = cfg.generation_context\n",
        "    idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(context) ], dtype=torch.long, device=device)\n",
        "    out_ids = model.generate(idx, max_new_tokens=cfg.max_new_tokens, temperature=cfg.temperature, top_k=cfg.top_k)[0]\n",
        "    print(\"\\n=== Generated ===\")\n",
        "    print(tokenizer.decode(out_ids))\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_path: str, tokenizer_path: str, config_path: str, device):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained ARSLM model and its tokenizer along with its configuration.\n",
        "    \"\"\"\n",
        "    # Add Config class to safe globals for unpickling\n",
        "    torch.serialization.add_safe_globals([Config])\n",
        "    # Load the configuration first\n",
        "    cfg = torch.load(config_path)\n",
        "\n",
        "    loaded_tokenizer = AdvancedTokenizer(pretrained_model_name_or_path=tokenizer_path)\n",
        "\n",
        "    # Instantiate the model using the loaded configuration\n",
        "    loaded_model = ARSLM(loaded_tokenizer,\n",
        "                         emb_dim=cfg.emb_dim,\n",
        "                         hidden_dim=cfg.hidden_dim,\n",
        "                         num_layers=cfg.num_layers,\n",
        "                         dropout_prob=cfg.dropout_prob).to(device)\n",
        "\n",
        "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    loaded_model.eval() # Set model to evaluation mode\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
        "    print(f\"Configuration loaded from {config_path}\")\n",
        "\n",
        "    return loaded_model, loaded_tokenizer, cfg\n",
        "\n",
        "def deploy_model(model, tokenizer, config, device, input_text: str):\n",
        "    \"\"\"\n",
        "    Function to deploy the model for inference (text generation).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        idx = torch.tensor([ [tokenizer.bos_token_id] + tokenizer.encode(input_text) ], dtype=torch.long, device=device)\n",
        "        out_ids = model.generate(idx, max_new_tokens=config.max_new_tokens, temperature=config.temperature, top_k=config.top_k)[0]\n",
        "        generated_text = tokenizer.decode(out_ids)\n",
        "    return generated_text\n",
        "\n",
        "# ----------------------------\n",
        "# Run demo if main\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    model, tokenizer = train_demo()\n",
        "\n",
        "    # Demonstrate loading the saved model and tokenizer\n",
        "    print(\"\\n=== Demonstrating Model Loading and Deployment ===\")\n",
        "    cfg_instance = Config()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    try:\n",
        "        loaded_model, loaded_tokenizer, loaded_config = load_model_and_tokenizer(\n",
        "            os.path.join(cfg_instance.model_save_dir, \"best_model.pt\"),\n",
        "            cfg_instance.tokenizer_save_dir,\n",
        "            cfg_instance.config_save_path,\n",
        "            device\n",
        "        )\n",
        "        # Test generation with loaded model using the deploy_model function\n",
        "        context = \"loaded model generates\"\n",
        "        generated_output = deploy_model(loaded_model, loaded_tokenizer, loaded_config, device, context)\n",
        "        print(\"Generated with loaded model:\")\n",
        "        print(generated_output)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No saved model found. Please ensure training completed successfully and saved a model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhqOqnrQLE1c"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers datasets peft accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI3CcDamLKJf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0aOcEaiLPRh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJJ6zau1M_1e"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "assert model is not None, \"❌ Le modèle n'est pas chargé\"\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "        lora_alpha=32,\n",
        "            target_modules=[\"c_attn\", \"c_proj\"],\n",
        "                lora_dropout=0.05,\n",
        "                    bias=\"none\",\n",
        "                        task_type=\"CAUSAL_LM\"\n",
        "                        )\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A57VC894NmU7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyGqE-OWN2Px"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    tokenized_inputs = tokenizer(\n",
        "            batch[\"text\"],\n",
        "                    truncation=True,\n",
        "                            padding=\"max_length\",\n",
        "                                    max_length=256\n",
        "                                        )\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vuZx3M2QyaH"
      },
      "outputs": [],
      "source": [
        "# 🚀 Installer les dépendances nécessaires\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes wandb torch streamlit pyngrok\n",
        "\n",
        "# ------------------------------\n",
        "# 1️⃣ Importer les bibliothèques\n",
        "# ------------------------------\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# ------------------------------\n",
        "# 2️⃣ Charger dataset et tokenizer\n",
        "# ------------------------------\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # S'assurer qu'il y a un token pad\n",
        "\n",
        "# ------------------------------\n",
        "# 3️⃣ Tokenisation avec labels\n",
        "# ------------------------------\n",
        "max_seq_len = 128\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_seq_len\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Nécessaire pour causal LM\n",
        "    return tokenized\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ------------------------------\n",
        "# 4️⃣ Data collator\n",
        "# ------------------------------\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# ------------------------------\n",
        "# 5️⃣ Préparer le modèle avec LoRA\n",
        "# ------------------------------\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_proj\", \"c_attn\"],  # Modules GPT2 à fine-tuner\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# ------------------------------\n",
        "# 6️⃣ Arguments d'entraînement\n",
        "# ------------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./arslm_lora\",\n",
        "    overwrite_output_dir=True,\n",
        "    # evaluation_strategy=\"epoch\", # Removed as it caused a TypeError\n",
        "    learning_rate=5e-4,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    fp16=True,  # précision mixte si GPU\n",
        "    # load_best_model_at_end=True, # Removed as it depends on evaluation_strategy\n",
        "    # metric_for_best_model=\"eval_loss\" # Removed as it depends on evaluation_strategy\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 7️⃣ Créer Trainer\n",
        "# ------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 8️⃣ Lancer fine-tuning\n",
        "# ------------------------------\n",
        "trainer.train()\n",
        "\n",
        "# ------------------------------\n",
        "# 9️⃣ Sauvegarder modèle et tokenizer\n",
        "# ------------------------------\n",
        "model.save_pretrained(\"./arslm_lora\")\n",
        "tokenizer.save_pretrained(\"./arslm_lora\")\n",
        "\n",
        "print(\"✅ Fine-tuning terminé et modèle sauvegardé dans './arslm_lora'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 Installer les dépendances\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes wandb torch streamlit pyngrok\n",
        "\n",
        "# ------------------------------\n",
        "# 1️⃣ Importer les bibliothèques\n",
        "# ------------------------------\n",
        "import os\n",
        "import threading\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,\n",
        "    DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ------------------------------\n",
        "# 2️⃣ Charger dataset et tokenizer\n",
        "# ------------------------------\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "max_seq_len = 128\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_seq_len\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# ------------------------------\n",
        "# 3️⃣ Data collator\n",
        "# ------------------------------\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# ------------------------------\n",
        "# 4️⃣ Préparer le modèle avec LoRA\n",
        "# ------------------------------\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_proj\", \"c_attn\"],  # Modules GPT2 à fine-tuner\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# ------------------------------\n",
        "# 5️⃣ Arguments d'entraînement\n",
        "# ------------------------------\n",
        "output_dir = \"./arslm_lora\"\n",
        "checkpoint_path = os.path.join(output_dir, \"checkpoint-32560\")  # remplacer si nécessaire\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    learning_rate=5e-4,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 6️⃣ Créer Trainer\n",
        "# ------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 7️⃣ Reprendre depuis checkpoint\n",
        "# ------------------------------\n",
        "resume_from_checkpoint = checkpoint_path if os.path.exists(checkpoint_path) else None\n",
        "\n",
        "# ------------------------------\n",
        "# 8️⃣ Lancer fine-tuning\n",
        "# ------------------------------\n",
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "# ------------------------------\n",
        "# 9️⃣ Sauvegarder modèle et tokenizer\n",
        "# ------------------------------\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"✅ Fine-tuning terminé. Modèle sauvegardé dans '{output_dir}'\")\n",
        "\n",
        "# ------------------------------\n",
        "# 🔹 Déploiement Streamlit + Ngrok\n",
        "# ------------------------------\n",
        "# Écrire un streamlit_app.py minimal\n",
        "streamlit_app = \"\"\"\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "st.title('ARSLM LoRA Demo')\n",
        "\n",
        "model_dir = './arslm_lora'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "\n",
        "prompt = st.text_area('Entrez votre texte :', 'Artificial intelligence is a powerful tool for')\n",
        "if st.button('Générer'):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    st.write(text)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"streamlit_app.py\", \"w\") as f:\n",
        "    f.write(streamlit_app)\n",
        "\n",
        "# Fonction pour lancer Streamlit\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run streamlit_app.py\")\n",
        "\n",
        "# Lancer Streamlit en thread\n",
        "threading.Thread(target=run_streamlit, daemon=True).start()\n",
        "\n",
        "# Créer tunnel Ngrok\n",
        "public_url = ngrok.connect(port=\"8501\")\n",
        "print(\"🌐 Lien public Streamlit :\", public_url)"
      ],
      "metadata": {
        "id": "bKnP9vl0G1QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q transformers datasets"
      ],
      "metadata": {
        "id": "-ywzskbfd1Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nuXGwR-XeLml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faae7b05"
      },
      "source": [
        "# Task\n",
        "Réexécuter la cellule `dd35396d` pour télécharger le dataset 'wikitext-2-raw-v1' et enregistrer les 10 000 premières lignes dans `/tmp/large_corpus.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "090fec95"
      },
      "source": [
        "## Télécharger et enregistrer le corpus Wikitext-2\n",
        "\n",
        "### Subtask:\n",
        "Réexécuter la cellule `dd35396d` pour télécharger le dataset 'wikitext-2-raw-v1' et enregistrer les 10 000 premières lignes dans `/tmp/large_corpus.txt`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebb29010"
      },
      "source": [
        "**Reasoning**:\n",
        "The user explicitly requested to re-execute cell `dd35396d` to download the 'wikitext-2-raw-v1' dataset and save the first 10,000 lines to `/tmp/large_corpus.txt`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621,
          "referenced_widgets": [
            "f40aaaeb357b414d98f1d40a61c2c00f",
            "4c95c2fbd19548549a7710fb7c9afe1a",
            "d8b66f368d1d40f89291dbfd64cb14b6",
            "c4ae19f935fa4e3e8b5d660fcbaec0a4",
            "3627204e2a78480e9d16a226e8de1403",
            "25ff40a02bda4a82bb28e3456e514246",
            "708284c5fde4446c849531cbc884576a",
            "19a60047e6f549ca98d95b2b082bb52b",
            "19d84a9001e14e7884f3a109626b3796",
            "829aabd2919a4847b70ce6b24b91404d",
            "788019ae64344563993809cbba66cdab",
            "bed8263a88184c198813b317ca055c21",
            "7e35ac55b95048d09cae8b827d0327df",
            "a828cd02760e44b3abc8994a27cbe95f",
            "64e47633ddc740928d2bdf7f69566e9b",
            "45969d0e8e744f74bcbf2641be12958b",
            "4763c603d64248dab044cfc23ffe1025",
            "9ba6a163e8844b0d91617b32d8ea4690",
            "d474a6872e2041e0ab4c31adb0007f28",
            "cec486eb0f0f4459b6ffc770ac9cf38d",
            "77348288110d4e2cbbd45d0279fbb2a8",
            "18162880e36942b593a8e9f240889777",
            "eefa9fad27b3436e9b31019a0c199aa7",
            "a0ef53d124c84f328d626154a14a8f3d",
            "7a54914fea864565b794017b09daded7",
            "a54b4fcc72544d139b2544dc91593ef4",
            "3fe514e3df31482bb9a6ba910b58ebb2",
            "c36c51c99b9c4f099b083e2012b51772",
            "87e0497ced3940939ad275dc87b9dd84",
            "b7b50b00c1d34058b7d2b5806893022e",
            "4722a1df0933475fa55642d09c8d8af0",
            "000ee768bde2478a85f019d99ae1c8de",
            "2573f83198d4415e888f2e7af1984db3",
            "368899587909479183167b7d667b3807",
            "f8c7ca8844a848bc9bebafcc3668ee49",
            "5231ed8b55ba45e190ed6a30a5fcb7a7",
            "7f69e847ec7a4ecb916c074d53f1b758",
            "4f52d2a38ae346b2af3b469a2c6b10c7",
            "00165fb518164c558fe3f1787fc4020b",
            "f0125abc9554455eb5b84abc58945950",
            "e5773e681cbe46708688b1eb3032faa8",
            "c1259eba459642ab97c0f44830db6713",
            "810f99c56e1d4b45a2a21df6303312ee",
            "b49cfaed3b2d4fa999edb06ff6e9ff78",
            "ab7d731d4b3d43dd9a2bb6ca017b6ed6",
            "cea50a29dd064587a19dbc6e0013099c",
            "ab0e1bb9f7d34b44a4e135c79f249fc1",
            "a79d92012b93405e8b17d4541fe14717",
            "d3b10ad11ff54562ad5cb92044ae1ec2",
            "18c081b8111043a98c24e5a6ad31d033",
            "d735ed0f35dc42708ad98e366affaf82",
            "ea5574934e664d548eb0ffadd4b9c0b3",
            "532f6d0bb6554dc8883e7d4748c03fb2",
            "7f620d3047f74a3890928d2db445a9df",
            "ef7048edbe8b4cc7900c729b158c65f8",
            "c83d24c853134d28887363eb99a303b3",
            "5dc0dec644054c8ba9b02ac6e8c87961",
            "b793465af77a484cafea03e1e76e2283",
            "430277fda70241fb86a94154aea245fb",
            "76eacdf27af1488aa01cdb7276ed7252",
            "a1d275a2f0ba419aa9f093395240ebb0",
            "5a76e00bc7a247d3be05090e65e4f730",
            "f671c5fdc2d34c008643217d6aa27401",
            "ed516c94638a4f21a949b947b1dfb025",
            "83778e08fae945718e1d360a1c3590b7",
            "b7e33e669b5843658470422da3b06cef",
            "7398100c47284ddd9f526a94d9e14f28",
            "0d1feeed90214240b56a9399a2452f83",
            "c72795f88c0d412c9d707acb7efd52be",
            "1e7393be4ae84b3e83fdefc17f58555b",
            "99296fb6dca447db9e96cc2912c1dfaf",
            "ffe86b987cd0454fbed1a530d1b215f4",
            "732242f82cfa496890530638475b59e5",
            "0420076ae00141ba96fd2da4ca412681",
            "39f622faf12d47be87d8fa419639cb3f",
            "fffa8e54605845b2ac4005d5565c565a",
            "3b432906188346fd9f9dc0f0f3d0389b"
          ]
        },
        "id": "aea5acb7",
        "outputId": "c7726d5d-e1b2-402c-d1f5-a46467d5c4af"
      },
      "source": [
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Define the output file path\n",
        "corpus_file_path = \"/tmp/large_corpus.txt\"\n",
        "\n",
        "# Load the 'wikitext-2-raw-v1' dataset\n",
        "print(\"Loading wikitext-2-raw-v1 dataset from Hugging Face...\")\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "\n",
        "# Access the 'train' split and extract 'text' column\n",
        "train_texts = dataset['train']['text']\n",
        "\n",
        "# Filter out empty strings and strip whitespace\n",
        "filtered_texts = [line.strip() for line in train_texts if line.strip()]\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(corpus_file_path), exist_ok=True)\n",
        "\n",
        "# Write the first 10,000 lines to the specified file\n",
        "print(f\"Writing first 10,000 lines to {corpus_file_path}...\")\n",
        "with open(corpus_file_path, 'w', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(filtered_texts):\n",
        "        if i < 10000:\n",
        "            f.write(line + '\\n')\n",
        "        else:\n",
        "            break\n",
        "print(\"Dataset extraction and saving complete.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading wikitext-2-raw-v1 dataset from Hugging Face...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f40aaaeb357b414d98f1d40a61c2c00f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bed8263a88184c198813b317ca055c21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eefa9fad27b3436e9b31019a0c199aa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "368899587909479183167b7d667b3807"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab7d731d4b3d43dd9a2bb6ca017b6ed6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c83d24c853134d28887363eb99a303b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7398100c47284ddd9f526a94d9e14f28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing first 10,000 lines to /tmp/large_corpus.txt...\n",
            "Dataset extraction and saving complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec4de7cc"
      },
      "source": [
        "## Réexécuter l'entraînement du modèle avec le corpus complet\n",
        "\n",
        "### Subtask:\n",
        "Exécuter à nouveau la fonction `train_demo` avec le corpus complet pour observer les performances du modèle ARSLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fefbd611"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `wikitext-2-raw-v1` dataset was successfully loaded from Hugging Face.\n",
        "*   The 'text' column from the 'train' split was extracted and processed.\n",
        "*   The first 10,000 non-empty lines of the training data were successfully saved to `/tmp/large_corpus.txt`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The prepared `large_corpus.txt` file is now ready for use as the complete corpus for retraining the ARSLM model, as specified in the subtask description.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1lyLo8h6TZ2lLXfWTQL6Ja2-3ZENAErBY",
      "authorship_tag": "ABX9TyMPTWsd7XVjBADkWaV6SJLR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00dc90e7a46044928a78bb952f97835e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e152e0b49b9408ca6f91419aa0a03d3",
              "IPY_MODEL_504009292b0940c2aaa8351e0127067f",
              "IPY_MODEL_17b2dfc3893a475aba1aa6fbcf729626"
            ],
            "layout": "IPY_MODEL_f3e4122ba9bb4c308a549fb5e3255e1e"
          }
        },
        "6e152e0b49b9408ca6f91419aa0a03d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c63f8c63442d456e99e25cf2ac5c7d68",
            "placeholder": "​",
            "style": "IPY_MODEL_5942670f889a4e81877658144b59b943",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "504009292b0940c2aaa8351e0127067f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_892f9bb60bfb4c548d7a0f0a8202634a",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1afd6d0456e7492594f673fc5d7671fc",
            "value": 48
          }
        },
        "17b2dfc3893a475aba1aa6fbcf729626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4f92aebc939447aae6dd6f17212d5c9",
            "placeholder": "​",
            "style": "IPY_MODEL_cdbd186fba1f4fd8822080f07d1691f0",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.29kB/s]"
          }
        },
        "f3e4122ba9bb4c308a549fb5e3255e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c63f8c63442d456e99e25cf2ac5c7d68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5942670f889a4e81877658144b59b943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "892f9bb60bfb4c548d7a0f0a8202634a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1afd6d0456e7492594f673fc5d7671fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4f92aebc939447aae6dd6f17212d5c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdbd186fba1f4fd8822080f07d1691f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04588a42758b478ab588a5f7b05e3962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e56d83fc7084793a7934ed37768fd9f",
              "IPY_MODEL_6ed8350e4ab048b4800641532581ef29",
              "IPY_MODEL_c111558e350047699b84454d6585384c"
            ],
            "layout": "IPY_MODEL_9c4c60e5f78b4c19a3ff136bcb9aa2ff"
          }
        },
        "6e56d83fc7084793a7934ed37768fd9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aba23ffcf4d4a1192640162f281d760",
            "placeholder": "​",
            "style": "IPY_MODEL_8e7f9997f4384345963791c6e6097708",
            "value": "vocab.txt: 100%"
          }
        },
        "6ed8350e4ab048b4800641532581ef29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc13c85f293d40f69f789cd48ef68293",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20d81595143b4f4eab8d9c0e53d7c3c5",
            "value": 231508
          }
        },
        "c111558e350047699b84454d6585384c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e966353e7d7414ea015eb34b6d734f1",
            "placeholder": "​",
            "style": "IPY_MODEL_a9fb3cdd5da34dfaae33102414e8d855",
            "value": " 232k/232k [00:00&lt;00:00, 3.60MB/s]"
          }
        },
        "9c4c60e5f78b4c19a3ff136bcb9aa2ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aba23ffcf4d4a1192640162f281d760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7f9997f4384345963791c6e6097708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc13c85f293d40f69f789cd48ef68293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d81595143b4f4eab8d9c0e53d7c3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e966353e7d7414ea015eb34b6d734f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9fb3cdd5da34dfaae33102414e8d855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85beaf1e00fe4386afaf1fc25b49a98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f16723ae3ba44e48d4fac39426ca130",
              "IPY_MODEL_2e1ffc1efa7542578d56c37ab9badf6a",
              "IPY_MODEL_570e930da9c243ad9d53322adb0167c5"
            ],
            "layout": "IPY_MODEL_448ed3ce35cf4a9e8071d40c63fb36c9"
          }
        },
        "3f16723ae3ba44e48d4fac39426ca130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af107835680e4ef3a397d3bd877b4224",
            "placeholder": "​",
            "style": "IPY_MODEL_04f74b40467046f6b975a53812d14e93",
            "value": "tokenizer.json: 100%"
          }
        },
        "2e1ffc1efa7542578d56c37ab9badf6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd27f5a94b48445b85d1cae5c2cfec88",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cabcb2c74b694d07843e3947fa978bde",
            "value": 466062
          }
        },
        "570e930da9c243ad9d53322adb0167c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af1ca70ed514f9ba038ec12af263aa1",
            "placeholder": "​",
            "style": "IPY_MODEL_931a7e31a78146dca1ccd5098570f1be",
            "value": " 466k/466k [00:00&lt;00:00, 3.65MB/s]"
          }
        },
        "448ed3ce35cf4a9e8071d40c63fb36c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af107835680e4ef3a397d3bd877b4224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04f74b40467046f6b975a53812d14e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd27f5a94b48445b85d1cae5c2cfec88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cabcb2c74b694d07843e3947fa978bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8af1ca70ed514f9ba038ec12af263aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931a7e31a78146dca1ccd5098570f1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d768cef1b6d44570a50162035125c2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ec138153f574553a692cdc18a042129",
              "IPY_MODEL_365f2783d9b240ae8dbf9a589bf5f89f",
              "IPY_MODEL_8822fa8bb00a46ea864670d0b00adc8d"
            ],
            "layout": "IPY_MODEL_11568452fd0d4647933aa1e6e13c055d"
          }
        },
        "3ec138153f574553a692cdc18a042129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e000eb3c5fac4eeb917284ea603cfe6d",
            "placeholder": "​",
            "style": "IPY_MODEL_0f803f6c67c449e59b2b2d0468ccb792",
            "value": "config.json: 100%"
          }
        },
        "365f2783d9b240ae8dbf9a589bf5f89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0754ff5eacf7408f85480c5fe2228b7e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ef7d51cdabc46e6bb8fb9680f31ea0d",
            "value": 570
          }
        },
        "8822fa8bb00a46ea864670d0b00adc8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83dca4b9a2d84d2c97a5a1d83bda4df4",
            "placeholder": "​",
            "style": "IPY_MODEL_53b21829dc0c4ff892cf0d61189e2784",
            "value": " 570/570 [00:00&lt;00:00, 40.7kB/s]"
          }
        },
        "11568452fd0d4647933aa1e6e13c055d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e000eb3c5fac4eeb917284ea603cfe6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f803f6c67c449e59b2b2d0468ccb792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0754ff5eacf7408f85480c5fe2228b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ef7d51cdabc46e6bb8fb9680f31ea0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83dca4b9a2d84d2c97a5a1d83bda4df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b21829dc0c4ff892cf0d61189e2784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4cc318d44344a7ba6ecb12991283022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_622096eac6a94285a9cb0cf1730a9064",
              "IPY_MODEL_3874c42576074162b603f25ce5ccf8dc",
              "IPY_MODEL_3d85a1bc004a4595b83ff3f439b31a27"
            ],
            "layout": "IPY_MODEL_4efe87f305864d2fa0c5d7b2d6db830e"
          }
        },
        "622096eac6a94285a9cb0cf1730a9064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c746fd86a14d4aa5fdd745b848f7ed",
            "placeholder": "​",
            "style": "IPY_MODEL_150fe937ef3e4130a5d792353227655a",
            "value": "README.md: "
          }
        },
        "3874c42576074162b603f25ce5ccf8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7c0767158a46abb5293c7f7c66ed1a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c51fea60df74db8a0e702efa10b3a03",
            "value": 1
          }
        },
        "3d85a1bc004a4595b83ff3f439b31a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c118b8de5bb5402aa5583793200225bd",
            "placeholder": "​",
            "style": "IPY_MODEL_c0543e6c30484053b5544875dc2c3395",
            "value": " 10.5k/? [00:00&lt;00:00, 927kB/s]"
          }
        },
        "4efe87f305864d2fa0c5d7b2d6db830e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c746fd86a14d4aa5fdd745b848f7ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "150fe937ef3e4130a5d792353227655a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b7c0767158a46abb5293c7f7c66ed1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0c51fea60df74db8a0e702efa10b3a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c118b8de5bb5402aa5583793200225bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0543e6c30484053b5544875dc2c3395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "647aba443291437c96afc8b031efe984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb3dd6ef36a94c0b9c818fef88f8690e",
              "IPY_MODEL_768cefe3faa14e52a20f59cb0cdc79ec",
              "IPY_MODEL_491227fff50248b79a639e08d2aea71a"
            ],
            "layout": "IPY_MODEL_f3e188fcfd8544cdbe11ae6316f9701f"
          }
        },
        "bb3dd6ef36a94c0b9c818fef88f8690e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e279bdabac5046ae8c39f1e80928ee54",
            "placeholder": "​",
            "style": "IPY_MODEL_924e2612dcd340a8ad3abe061674bcd8",
            "value": "wikitext-2-raw-v1/test-00000-of-00001.pa(…): 100%"
          }
        },
        "768cefe3faa14e52a20f59cb0cdc79ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e560cb8462f46f3b0e7795736b224c5",
            "max": 732610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6a15601b0754500aa86aec1238e3021",
            "value": 732610
          }
        },
        "491227fff50248b79a639e08d2aea71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c1f8118b274380a28d7117403f6b3c",
            "placeholder": "​",
            "style": "IPY_MODEL_cec53a120ea94dd29c512e72fe55e64e",
            "value": " 733k/733k [00:00&lt;00:00, 46.5kB/s]"
          }
        },
        "f3e188fcfd8544cdbe11ae6316f9701f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e279bdabac5046ae8c39f1e80928ee54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "924e2612dcd340a8ad3abe061674bcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e560cb8462f46f3b0e7795736b224c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6a15601b0754500aa86aec1238e3021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84c1f8118b274380a28d7117403f6b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec53a120ea94dd29c512e72fe55e64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6335adbe69e4be782c972c0832e624f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36245de608b546f28c262718719df35d",
              "IPY_MODEL_2d231167d6134c20b7cb0fe86bc382b2",
              "IPY_MODEL_5611f862289b467eb34631f5f36c339a"
            ],
            "layout": "IPY_MODEL_900fd4e4ebe24a0e9c504838582d0e99"
          }
        },
        "36245de608b546f28c262718719df35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11a104269be746fba16060dacca56beb",
            "placeholder": "​",
            "style": "IPY_MODEL_13b5275bab2140f7922378954b1202b9",
            "value": "wikitext-2-raw-v1/train-00000-of-00001.p(…): 100%"
          }
        },
        "2d231167d6134c20b7cb0fe86bc382b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928ca07445554efcb7d60f0d562eb39a",
            "max": 6357543,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc10d314a8204da29627b609a552915f",
            "value": 6357543
          }
        },
        "5611f862289b467eb34631f5f36c339a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4187ac6bff2461e8bf954431d9f698e",
            "placeholder": "​",
            "style": "IPY_MODEL_ed776daaf11b474380106d7d1cf46d27",
            "value": " 6.36M/6.36M [00:00&lt;00:00, 460kB/s]"
          }
        },
        "900fd4e4ebe24a0e9c504838582d0e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a104269be746fba16060dacca56beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13b5275bab2140f7922378954b1202b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "928ca07445554efcb7d60f0d562eb39a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc10d314a8204da29627b609a552915f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4187ac6bff2461e8bf954431d9f698e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed776daaf11b474380106d7d1cf46d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c28f9ab574874200b6b32b119d2b9c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9c976b8c2b849a0bd7cf8413a07a46c",
              "IPY_MODEL_da01d3a90a1c472792f2bf0c39cd165b",
              "IPY_MODEL_72043bd724a246d1801c3b37db327d94"
            ],
            "layout": "IPY_MODEL_4bf1fe657149481099e5b48048bcf4c0"
          }
        },
        "a9c976b8c2b849a0bd7cf8413a07a46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d147b783b5494f86a8ddf22d35e5fd83",
            "placeholder": "​",
            "style": "IPY_MODEL_a10f86b25d7346a0b2b02024953288b0",
            "value": "wikitext-2-raw-v1/validation-00000-of-00(…): 100%"
          }
        },
        "da01d3a90a1c472792f2bf0c39cd165b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7d44c43e9854caba4030efee502713e",
            "max": 657209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f657abb627ab4e7882fee3030855f9dd",
            "value": 657209
          }
        },
        "72043bd724a246d1801c3b37db327d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a985982acb48ecbf91a06814ed96ae",
            "placeholder": "​",
            "style": "IPY_MODEL_c1fffe67465146a587f681206f760d2b",
            "value": " 657k/657k [00:00&lt;00:00, 488kB/s]"
          }
        },
        "4bf1fe657149481099e5b48048bcf4c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d147b783b5494f86a8ddf22d35e5fd83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a10f86b25d7346a0b2b02024953288b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7d44c43e9854caba4030efee502713e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f657abb627ab4e7882fee3030855f9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1a985982acb48ecbf91a06814ed96ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1fffe67465146a587f681206f760d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e59709c6efcf4cf4ad2a45c599e0d824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38c6be84668f4b7c93fe52ea7c411c27",
              "IPY_MODEL_1a6d7ab5b28748d1a4f56829e91a375d",
              "IPY_MODEL_57f4d08d76ec4e3d87fc03f9f760c264"
            ],
            "layout": "IPY_MODEL_e3adb302722b4660b8a4d8371217cd41"
          }
        },
        "38c6be84668f4b7c93fe52ea7c411c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6bbc487ed7d4f35a51a8c060f68cd77",
            "placeholder": "​",
            "style": "IPY_MODEL_024f13f953ab4a87b4ba7f5f28ecd9b2",
            "value": "Generating test split: 100%"
          }
        },
        "1a6d7ab5b28748d1a4f56829e91a375d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_238f03e82db748b5b1254b0ccad5549d",
            "max": 4358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_527d42d10f364db89ec5a8e9d825afce",
            "value": 4358
          }
        },
        "57f4d08d76ec4e3d87fc03f9f760c264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2290d9923e6a4e5d84c51cfb5e709c1e",
            "placeholder": "​",
            "style": "IPY_MODEL_9a6251bee3eb46e9a99176bdd24111b4",
            "value": " 4358/4358 [00:00&lt;00:00, 118429.58 examples/s]"
          }
        },
        "e3adb302722b4660b8a4d8371217cd41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6bbc487ed7d4f35a51a8c060f68cd77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "024f13f953ab4a87b4ba7f5f28ecd9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "238f03e82db748b5b1254b0ccad5549d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "527d42d10f364db89ec5a8e9d825afce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2290d9923e6a4e5d84c51cfb5e709c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a6251bee3eb46e9a99176bdd24111b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b0e99e40b0e4cea9dbe1ea89d7042c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f43a414b56144e0b12755cd852e20b4",
              "IPY_MODEL_eb6d693a70ea43178b7f21c36bf896da",
              "IPY_MODEL_1c2a0abd88cc424a91dec23def6b69ff"
            ],
            "layout": "IPY_MODEL_832049e112d644c5a56a572a0a5b53a1"
          }
        },
        "1f43a414b56144e0b12755cd852e20b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b8d277c5289401a802c09a0a68140b9",
            "placeholder": "​",
            "style": "IPY_MODEL_1c26a350440d4ce997cd29d995de5bb6",
            "value": "Generating train split: 100%"
          }
        },
        "eb6d693a70ea43178b7f21c36bf896da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93878fcab90a45db9a092f226ca4922a",
            "max": 36718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0786ab84e8544ef4a736fd1375218b1b",
            "value": 36718
          }
        },
        "1c2a0abd88cc424a91dec23def6b69ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f40d026af25c485196353a81fb8b6f31",
            "placeholder": "​",
            "style": "IPY_MODEL_921f8f502c53490e9f7ded1f453d5bc0",
            "value": " 36718/36718 [00:00&lt;00:00, 405888.98 examples/s]"
          }
        },
        "832049e112d644c5a56a572a0a5b53a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b8d277c5289401a802c09a0a68140b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c26a350440d4ce997cd29d995de5bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93878fcab90a45db9a092f226ca4922a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0786ab84e8544ef4a736fd1375218b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f40d026af25c485196353a81fb8b6f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "921f8f502c53490e9f7ded1f453d5bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59b56391b28347abb308b8f3b04c638c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c30541c4c3cf46b59bb936241dd46dbe",
              "IPY_MODEL_5ba4cbaebd2d493da78ecea75adf4401",
              "IPY_MODEL_8209a4f3c59f4784868e91fd82ac6c1f"
            ],
            "layout": "IPY_MODEL_0a995516a0094f149fc20539bd312378"
          }
        },
        "c30541c4c3cf46b59bb936241dd46dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e83cef65c7b4173aa3867634ac6be5c",
            "placeholder": "​",
            "style": "IPY_MODEL_b061fca9dc224750889912c479bfd9c4",
            "value": "Generating validation split: 100%"
          }
        },
        "5ba4cbaebd2d493da78ecea75adf4401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa7d2913c966482b82cf20fb6211c3c2",
            "max": 3760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66cd7fc8ae4643cf864cb7b8151d03ea",
            "value": 3760
          }
        },
        "8209a4f3c59f4784868e91fd82ac6c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3dff3e9e68945fa97542d707a045c63",
            "placeholder": "​",
            "style": "IPY_MODEL_9cf38f9f0e3c40c6ae21280490c65f23",
            "value": " 3760/3760 [00:00&lt;00:00, 132168.28 examples/s]"
          }
        },
        "0a995516a0094f149fc20539bd312378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e83cef65c7b4173aa3867634ac6be5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b061fca9dc224750889912c479bfd9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa7d2913c966482b82cf20fb6211c3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66cd7fc8ae4643cf864cb7b8151d03ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3dff3e9e68945fa97542d707a045c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cf38f9f0e3c40c6ae21280490c65f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1691fbd98daa4bdc8a79dc2329ecea50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72f09842cd444caea5b0f25c6a19a370",
              "IPY_MODEL_cfcea13c50614839870005da73c69613",
              "IPY_MODEL_a5feda0934a0477d920f74ba19742c51"
            ],
            "layout": "IPY_MODEL_ae8a4568e5ca40a694779465ca9f7ce2"
          }
        },
        "72f09842cd444caea5b0f25c6a19a370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e89eb8830cf439b8690b9e770711cf7",
            "placeholder": "​",
            "style": "IPY_MODEL_2a9b05e8bd794947aff10daa4490dde1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "cfcea13c50614839870005da73c69613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6f0f0a30905472cb310b6ae5e693152",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b51606af85974f3dbcf7390a6413e2b3",
            "value": 48
          }
        },
        "a5feda0934a0477d920f74ba19742c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97704179da0c4bd8a92755993f147ce8",
            "placeholder": "​",
            "style": "IPY_MODEL_695821696bb84cac8ee35d73443c3d33",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.13kB/s]"
          }
        },
        "ae8a4568e5ca40a694779465ca9f7ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e89eb8830cf439b8690b9e770711cf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a9b05e8bd794947aff10daa4490dde1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6f0f0a30905472cb310b6ae5e693152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51606af85974f3dbcf7390a6413e2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97704179da0c4bd8a92755993f147ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "695821696bb84cac8ee35d73443c3d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88beee49142d4a8abb63124d27385070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13a6789a2c7740d7af3f38d6ce3f29b5",
              "IPY_MODEL_bb4d1484e8c04fa4b8146261a81bfbc6",
              "IPY_MODEL_ee2a29e364a54a7fa85706561b30a541"
            ],
            "layout": "IPY_MODEL_ebe8c69270d143ec84bc9b47e32ca21e"
          }
        },
        "13a6789a2c7740d7af3f38d6ce3f29b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6c953b934584544a9cb448900c9a4fd",
            "placeholder": "​",
            "style": "IPY_MODEL_97429abe93474ab8a3c3a5dada673bcd",
            "value": "vocab.txt: 100%"
          }
        },
        "bb4d1484e8c04fa4b8146261a81bfbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf323a2014a84735825246d99803dc09",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1245db234fe411d9355e026e5da4d6c",
            "value": 231508
          }
        },
        "ee2a29e364a54a7fa85706561b30a541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aad39fb3d284503a47a1b31fac2041c",
            "placeholder": "​",
            "style": "IPY_MODEL_74b9b7f69add4015919dfa8626745f7c",
            "value": " 232k/232k [00:00&lt;00:00, 9.77MB/s]"
          }
        },
        "ebe8c69270d143ec84bc9b47e32ca21e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6c953b934584544a9cb448900c9a4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97429abe93474ab8a3c3a5dada673bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf323a2014a84735825246d99803dc09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1245db234fe411d9355e026e5da4d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2aad39fb3d284503a47a1b31fac2041c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b9b7f69add4015919dfa8626745f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24c1e04aae0348edb119465c455c9ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90990b0229aa46d0842feddd41318acd",
              "IPY_MODEL_0267471008794ae5b5af42327eca402f",
              "IPY_MODEL_9bafa09f96ad404b959ac530f11430bd"
            ],
            "layout": "IPY_MODEL_b6e2bf5f99c94f609a78fd463b8f5092"
          }
        },
        "90990b0229aa46d0842feddd41318acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e4d1cf089a49bc8f72a6d0ba5ca139",
            "placeholder": "​",
            "style": "IPY_MODEL_7b2405c5d7224710af37a01cba4faaf2",
            "value": "tokenizer.json: 100%"
          }
        },
        "0267471008794ae5b5af42327eca402f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_882dbd88f8af48cd9d9efe4706c33f40",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36effa32eeca431294d5bdbbb16c1959",
            "value": 466062
          }
        },
        "9bafa09f96ad404b959ac530f11430bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5bc967d59c94b1c91ad1dfcabafa2d8",
            "placeholder": "​",
            "style": "IPY_MODEL_77375f506bb74e21bdd2758fa729329e",
            "value": " 466k/466k [00:00&lt;00:00, 2.17MB/s]"
          }
        },
        "b6e2bf5f99c94f609a78fd463b8f5092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e4d1cf089a49bc8f72a6d0ba5ca139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b2405c5d7224710af37a01cba4faaf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "882dbd88f8af48cd9d9efe4706c33f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36effa32eeca431294d5bdbbb16c1959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5bc967d59c94b1c91ad1dfcabafa2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77375f506bb74e21bdd2758fa729329e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6277659bcd884f0a89763c5dac54c083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c169a21737ff4e499342f1ea33a33137",
              "IPY_MODEL_5279dc849afe48f886ec2b01a0de6b01",
              "IPY_MODEL_49607961d7ab44aabb4f5a6bff990ee3"
            ],
            "layout": "IPY_MODEL_4672065520ad42d788d7cc0f9e6bb29a"
          }
        },
        "c169a21737ff4e499342f1ea33a33137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b847ab58dd954fbbaae962abbe506693",
            "placeholder": "​",
            "style": "IPY_MODEL_2f217c117e71451caf19f04770e581c5",
            "value": "config.json: 100%"
          }
        },
        "5279dc849afe48f886ec2b01a0de6b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4268310415f4167b4ed2bce922aabbf",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb1ec76e955f4b70b7ffc3cc86edab6c",
            "value": 570
          }
        },
        "49607961d7ab44aabb4f5a6bff990ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeb6b35748564da29f1cebb91512da0f",
            "placeholder": "​",
            "style": "IPY_MODEL_9a5bff902a294cbe9ff1173d10e91bae",
            "value": " 570/570 [00:00&lt;00:00, 17.5kB/s]"
          }
        },
        "4672065520ad42d788d7cc0f9e6bb29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b847ab58dd954fbbaae962abbe506693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f217c117e71451caf19f04770e581c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4268310415f4167b4ed2bce922aabbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb1ec76e955f4b70b7ffc3cc86edab6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aeb6b35748564da29f1cebb91512da0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a5bff902a294cbe9ff1173d10e91bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f40aaaeb357b414d98f1d40a61c2c00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c95c2fbd19548549a7710fb7c9afe1a",
              "IPY_MODEL_d8b66f368d1d40f89291dbfd64cb14b6",
              "IPY_MODEL_c4ae19f935fa4e3e8b5d660fcbaec0a4"
            ],
            "layout": "IPY_MODEL_3627204e2a78480e9d16a226e8de1403"
          }
        },
        "4c95c2fbd19548549a7710fb7c9afe1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ff40a02bda4a82bb28e3456e514246",
            "placeholder": "​",
            "style": "IPY_MODEL_708284c5fde4446c849531cbc884576a",
            "value": "README.md: "
          }
        },
        "d8b66f368d1d40f89291dbfd64cb14b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a60047e6f549ca98d95b2b082bb52b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19d84a9001e14e7884f3a109626b3796",
            "value": 1
          }
        },
        "c4ae19f935fa4e3e8b5d660fcbaec0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_829aabd2919a4847b70ce6b24b91404d",
            "placeholder": "​",
            "style": "IPY_MODEL_788019ae64344563993809cbba66cdab",
            "value": " 10.5k/? [00:00&lt;00:00, 306kB/s]"
          }
        },
        "3627204e2a78480e9d16a226e8de1403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ff40a02bda4a82bb28e3456e514246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "708284c5fde4446c849531cbc884576a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19a60047e6f549ca98d95b2b082bb52b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "19d84a9001e14e7884f3a109626b3796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "829aabd2919a4847b70ce6b24b91404d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788019ae64344563993809cbba66cdab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed8263a88184c198813b317ca055c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e35ac55b95048d09cae8b827d0327df",
              "IPY_MODEL_a828cd02760e44b3abc8994a27cbe95f",
              "IPY_MODEL_64e47633ddc740928d2bdf7f69566e9b"
            ],
            "layout": "IPY_MODEL_45969d0e8e744f74bcbf2641be12958b"
          }
        },
        "7e35ac55b95048d09cae8b827d0327df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4763c603d64248dab044cfc23ffe1025",
            "placeholder": "​",
            "style": "IPY_MODEL_9ba6a163e8844b0d91617b32d8ea4690",
            "value": "wikitext-2-raw-v1/test-00000-of-00001.pa(…): 100%"
          }
        },
        "a828cd02760e44b3abc8994a27cbe95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d474a6872e2041e0ab4c31adb0007f28",
            "max": 732610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cec486eb0f0f4459b6ffc770ac9cf38d",
            "value": 732610
          }
        },
        "64e47633ddc740928d2bdf7f69566e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77348288110d4e2cbbd45d0279fbb2a8",
            "placeholder": "​",
            "style": "IPY_MODEL_18162880e36942b593a8e9f240889777",
            "value": " 733k/733k [00:01&lt;00:00, 23.0kB/s]"
          }
        },
        "45969d0e8e744f74bcbf2641be12958b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4763c603d64248dab044cfc23ffe1025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba6a163e8844b0d91617b32d8ea4690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d474a6872e2041e0ab4c31adb0007f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec486eb0f0f4459b6ffc770ac9cf38d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77348288110d4e2cbbd45d0279fbb2a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18162880e36942b593a8e9f240889777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eefa9fad27b3436e9b31019a0c199aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0ef53d124c84f328d626154a14a8f3d",
              "IPY_MODEL_7a54914fea864565b794017b09daded7",
              "IPY_MODEL_a54b4fcc72544d139b2544dc91593ef4"
            ],
            "layout": "IPY_MODEL_3fe514e3df31482bb9a6ba910b58ebb2"
          }
        },
        "a0ef53d124c84f328d626154a14a8f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c36c51c99b9c4f099b083e2012b51772",
            "placeholder": "​",
            "style": "IPY_MODEL_87e0497ced3940939ad275dc87b9dd84",
            "value": "wikitext-2-raw-v1/train-00000-of-00001.p(…): 100%"
          }
        },
        "7a54914fea864565b794017b09daded7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b50b00c1d34058b7d2b5806893022e",
            "max": 6357543,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4722a1df0933475fa55642d09c8d8af0",
            "value": 6357543
          }
        },
        "a54b4fcc72544d139b2544dc91593ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000ee768bde2478a85f019d99ae1c8de",
            "placeholder": "​",
            "style": "IPY_MODEL_2573f83198d4415e888f2e7af1984db3",
            "value": " 6.36M/6.36M [00:00&lt;00:00, 273kB/s]"
          }
        },
        "3fe514e3df31482bb9a6ba910b58ebb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36c51c99b9c4f099b083e2012b51772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87e0497ced3940939ad275dc87b9dd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7b50b00c1d34058b7d2b5806893022e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4722a1df0933475fa55642d09c8d8af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "000ee768bde2478a85f019d99ae1c8de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2573f83198d4415e888f2e7af1984db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "368899587909479183167b7d667b3807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8c7ca8844a848bc9bebafcc3668ee49",
              "IPY_MODEL_5231ed8b55ba45e190ed6a30a5fcb7a7",
              "IPY_MODEL_7f69e847ec7a4ecb916c074d53f1b758"
            ],
            "layout": "IPY_MODEL_4f52d2a38ae346b2af3b469a2c6b10c7"
          }
        },
        "f8c7ca8844a848bc9bebafcc3668ee49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00165fb518164c558fe3f1787fc4020b",
            "placeholder": "​",
            "style": "IPY_MODEL_f0125abc9554455eb5b84abc58945950",
            "value": "wikitext-2-raw-v1/validation-00000-of-00(…): 100%"
          }
        },
        "5231ed8b55ba45e190ed6a30a5fcb7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5773e681cbe46708688b1eb3032faa8",
            "max": 657209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1259eba459642ab97c0f44830db6713",
            "value": 657209
          }
        },
        "7f69e847ec7a4ecb916c074d53f1b758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_810f99c56e1d4b45a2a21df6303312ee",
            "placeholder": "​",
            "style": "IPY_MODEL_b49cfaed3b2d4fa999edb06ff6e9ff78",
            "value": " 657k/657k [00:00&lt;00:00, 98.4kB/s]"
          }
        },
        "4f52d2a38ae346b2af3b469a2c6b10c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00165fb518164c558fe3f1787fc4020b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0125abc9554455eb5b84abc58945950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5773e681cbe46708688b1eb3032faa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1259eba459642ab97c0f44830db6713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "810f99c56e1d4b45a2a21df6303312ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b49cfaed3b2d4fa999edb06ff6e9ff78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab7d731d4b3d43dd9a2bb6ca017b6ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cea50a29dd064587a19dbc6e0013099c",
              "IPY_MODEL_ab0e1bb9f7d34b44a4e135c79f249fc1",
              "IPY_MODEL_a79d92012b93405e8b17d4541fe14717"
            ],
            "layout": "IPY_MODEL_d3b10ad11ff54562ad5cb92044ae1ec2"
          }
        },
        "cea50a29dd064587a19dbc6e0013099c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18c081b8111043a98c24e5a6ad31d033",
            "placeholder": "​",
            "style": "IPY_MODEL_d735ed0f35dc42708ad98e366affaf82",
            "value": "Generating test split: 100%"
          }
        },
        "ab0e1bb9f7d34b44a4e135c79f249fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea5574934e664d548eb0ffadd4b9c0b3",
            "max": 4358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_532f6d0bb6554dc8883e7d4748c03fb2",
            "value": 4358
          }
        },
        "a79d92012b93405e8b17d4541fe14717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f620d3047f74a3890928d2db445a9df",
            "placeholder": "​",
            "style": "IPY_MODEL_ef7048edbe8b4cc7900c729b158c65f8",
            "value": " 4358/4358 [00:00&lt;00:00, 79819.29 examples/s]"
          }
        },
        "d3b10ad11ff54562ad5cb92044ae1ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18c081b8111043a98c24e5a6ad31d033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d735ed0f35dc42708ad98e366affaf82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea5574934e664d548eb0ffadd4b9c0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "532f6d0bb6554dc8883e7d4748c03fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f620d3047f74a3890928d2db445a9df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef7048edbe8b4cc7900c729b158c65f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c83d24c853134d28887363eb99a303b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dc0dec644054c8ba9b02ac6e8c87961",
              "IPY_MODEL_b793465af77a484cafea03e1e76e2283",
              "IPY_MODEL_430277fda70241fb86a94154aea245fb"
            ],
            "layout": "IPY_MODEL_76eacdf27af1488aa01cdb7276ed7252"
          }
        },
        "5dc0dec644054c8ba9b02ac6e8c87961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d275a2f0ba419aa9f093395240ebb0",
            "placeholder": "​",
            "style": "IPY_MODEL_5a76e00bc7a247d3be05090e65e4f730",
            "value": "Generating train split: 100%"
          }
        },
        "b793465af77a484cafea03e1e76e2283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f671c5fdc2d34c008643217d6aa27401",
            "max": 36718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed516c94638a4f21a949b947b1dfb025",
            "value": 36718
          }
        },
        "430277fda70241fb86a94154aea245fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83778e08fae945718e1d360a1c3590b7",
            "placeholder": "​",
            "style": "IPY_MODEL_b7e33e669b5843658470422da3b06cef",
            "value": " 36718/36718 [00:00&lt;00:00, 467591.65 examples/s]"
          }
        },
        "76eacdf27af1488aa01cdb7276ed7252": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d275a2f0ba419aa9f093395240ebb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a76e00bc7a247d3be05090e65e4f730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f671c5fdc2d34c008643217d6aa27401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed516c94638a4f21a949b947b1dfb025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83778e08fae945718e1d360a1c3590b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e33e669b5843658470422da3b06cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7398100c47284ddd9f526a94d9e14f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d1feeed90214240b56a9399a2452f83",
              "IPY_MODEL_c72795f88c0d412c9d707acb7efd52be",
              "IPY_MODEL_1e7393be4ae84b3e83fdefc17f58555b"
            ],
            "layout": "IPY_MODEL_99296fb6dca447db9e96cc2912c1dfaf"
          }
        },
        "0d1feeed90214240b56a9399a2452f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffe86b987cd0454fbed1a530d1b215f4",
            "placeholder": "​",
            "style": "IPY_MODEL_732242f82cfa496890530638475b59e5",
            "value": "Generating validation split: 100%"
          }
        },
        "c72795f88c0d412c9d707acb7efd52be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0420076ae00141ba96fd2da4ca412681",
            "max": 3760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39f622faf12d47be87d8fa419639cb3f",
            "value": 3760
          }
        },
        "1e7393be4ae84b3e83fdefc17f58555b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fffa8e54605845b2ac4005d5565c565a",
            "placeholder": "​",
            "style": "IPY_MODEL_3b432906188346fd9f9dc0f0f3d0389b",
            "value": " 3760/3760 [00:00&lt;00:00, 117160.20 examples/s]"
          }
        },
        "99296fb6dca447db9e96cc2912c1dfaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe86b987cd0454fbed1a530d1b215f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732242f82cfa496890530638475b59e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0420076ae00141ba96fd2da4ca412681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f622faf12d47be87d8fa419639cb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fffa8e54605845b2ac4005d5565c565a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b432906188346fd9f9dc0f0f3d0389b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}