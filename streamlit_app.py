import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# Configuration de la page
st.set_page_config(
    page_title="ARSLM - Text Generation",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Styles CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #4A90E2;
        margin-bottom: 1rem;
    }
    .sub-header {
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .generated-box {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #4A90E2;
        margin-top: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# En-t√™te
st.markdown('<div class="main-header">üß† ARSLM Text Generation</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">Adaptive Reasoning Semantic Language Model</div>', unsafe_allow_html=True)

# Utiliser un mod√®le public accessible (pas de fichiers locaux)
# Vous pouvez changer pour votre mod√®le h√©berg√© sur Hugging Face
MODEL_NAME = "gpt2"  # Remplacez par "votre-username/votre-modele" si vous avez upload√© votre mod√®le

@st.cache_resource(show_spinner=True)
def load_model_and_tokenizer():
    """
    Charge le mod√®le depuis Hugging Face Hub
    """
    try:
        # Streamlit Cloud utilise CPU uniquement
        device = "cpu"
        
        # Chargement du tokenizer
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        # D√©finir le pad_token si n√©cessaire
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Chargement du mod√®le (optimis√© pour CPU)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True
        )
        model.to(device)
        model.eval()
        
        # Cr√©ation du pipeline
        generator = pipeline(
            'text-generation',
            model=model,
            tokenizer=tokenizer,
            device=-1  # -1 pour CPU
        )
        
        return generator, tokenizer, None
        
    except Exception as e:
        return None, None, f"‚ùå Erreur lors du chargement: {str(e)}"

# Chargement du mod√®le avec barre de progression
with st.spinner('üîÑ Chargement du mod√®le... (premi√®re fois peut prendre 1-2 minutes)'):
    generator, tokenizer, error = load_model_and_tokenizer()

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    if error:
        st.error(error)
    else:
        st.success(f"‚úÖ Mod√®le charg√©: {MODEL_NAME}")
    
    st.divider()
    
    st.subheader("üìä Param√®tres")
    
    max_new_tokens = st.slider(
        'Tokens maximum',
        min_value=10,
        max_value=200,  # Limit√© pour Streamlit Cloud gratuit
        value=50,
        step=10,
        help="Nombre maximum de tokens √† g√©n√©rer"
    )
    
    temperature = st.slider(
        'Temp√©rature',
        min_value=0.1,
        max_value=2.0,
        value=0.7,
        step=0.1,
        help="Cr√©ativit√© (bas = conservateur, haut = cr√©atif)"
    )
    
    top_k = st.slider(
        'Top-K',
        min_value=0,
        max_value=100,
        value=50,
        step=5,
        help="Nombre de tokens candidats"
    )
    
    top_p = st.slider(
        'Top-P',
        min_value=0.1,
        max_value=1.0,
        value=0.9,
        step=0.05,
        help="Probabilit√© cumul√©e"
    )
    
    st.divider()
    
    st.subheader("‚ÑπÔ∏è √Ä propos")
    st.info("""
    **ARSLM** - Mod√®le de langage adaptatif pour:
    - G√©n√©ration de texte
    - Compr√©hension contextuelle
    - Applications conversationnelles
    """)
    
    st.markdown("---")
    st.markdown("**Cr√©√© par:** Benjamin Amaad Kama")
    st.markdown("üìß benjokama@hotmail.fr")
    st.markdown("[GitHub](https://github.com/benjaminpolydeq/ARSLM)")

# Zone principale
if generator and tokenizer:
    
    # Onglets
    tab1, tab2, tab3 = st.tabs(["‚úçÔ∏è G√©n√©ration", "üéØ Exemples", "üìù Historique"])
    
    with tab1:
        st.markdown("üí° **Entrez votre prompt ci-dessous et g√©n√©rez du texte intelligent !**")
        
        # Zone de texte
        prompt = st.text_area(
            'Votre prompt:',
            value="L'intelligence artificielle va transformer",
            height=100,
            help="Entrez le texte de d√©part"
        )
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            generate_btn = st.button('üöÄ G√©n√©rer', type="primary", use_container_width=True)
        
        with col2:
            clear_btn = st.button('üóëÔ∏è Effacer', use_container_width=True)
        
        # G√©n√©ration
        if generate_btn and prompt:
            with st.spinner('‚ú® G√©n√©ration en cours...'):
                try:
                    result = generator(
                        prompt,
                        max_new_tokens=max_new_tokens,
                        num_return_sequences=1,
                        pad_token_id=tokenizer.pad_token_id,
                        temperature=temperature,
                        top_k=top_k,
                        top_p=top_p,
                        do_sample=True,
                        repetition_penalty=1.2
                    )
                    
                    generated_text = result[0]['generated_text']
                    
                    st.success('‚úÖ G√©n√©ration termin√©e !')
                    st.subheader('üìÑ Texte g√©n√©r√©')
                    st.markdown(f'<div class="generated-box">{generated_text}</div>', unsafe_allow_html=True)
                    
                    # Statistiques
                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                    with col_stat1:
                        st.metric("Caract√®res", len(generated_text))
                    with col_stat2:
                        st.metric("Mots", len(generated_text.split()))
                    with col_stat3:
                        st.metric("Tokens", len(tokenizer.encode(generated_text)))
                    
                    # Code copiable
                    st.code(generated_text, language=None)
                    
                    # Sauvegarder dans l'historique
                    if 'history' not in st.session_state:
                        st.session_state.history = []
                    st.session_state.history.append({
                        'prompt': prompt,
                        'result': generated_text,
                        'params': {
                            'max_tokens': max_new_tokens,
                            'temperature': temperature,
                            'top_k': top_k,
                            'top_p': top_p
                        }
                    })
                    
                except Exception as e:
                    st.error(f"‚ùå Erreur: {str(e)}")
                    st.info("üí° Essayez de r√©duire le nombre de tokens ou de relancer")
        
        elif generate_btn:
            st.warning('‚ö†Ô∏è Veuillez entrer un prompt')
        
        if clear_btn:
            st.rerun()
    
    with tab2:
        st.subheader("üéØ Exemples de prompts")
        
        examples = {
            "ü§ñ Technologie": [
                "L'avenir de l'intelligence artificielle est",
                "Les robots du futur pourront",
                "La blockchain va r√©volutionner",
            ],
            "üìö √âducation": [
                "L'√©ducation en ligne permet",
                "Les √©tudiants de demain apprendront",
                "La technologie √©ducative transforme",
            ],
            "üíº Business": [
                "Les startups innovent en",
                "L'entrepreneuriat digital offre",
                "Le commerce √©lectronique √©volue vers",
            ],
            "üåç Soci√©t√©": [
                "Le d√©veloppement durable n√©cessite",
                "Les villes intelligentes vont",
                "La transformation num√©rique change",
            ]
        }
        
        for category, prompts in examples.items():
            with st.expander(category):
                for p in prompts:
                    if st.button(f"üìù {p}", key=p, use_container_width=True):
                        st.info(f"üí° Prompt s√©lectionn√© ! Allez dans l'onglet 'G√©n√©ration' et collez: {p}")
    
    with tab3:
        st.subheader("üìù Historique des g√©n√©rations")
        
        if 'history' in st.session_state and st.session_state.history:
            for idx, entry in enumerate(reversed(st.session_state.history)):
                with st.expander(f"G√©n√©ration #{len(st.session_state.history) - idx}"):
                    st.write("**Prompt:**", entry['prompt'])
                    st.write("**R√©sultat:**")
                    st.write(entry['result'])
                    st.json(entry['params'])
            
            if st.button("üóëÔ∏è Effacer l'historique"):
                st.session_state.history = []
                st.rerun()
        else:
            st.info("Aucune g√©n√©ration pour le moment. Commencez dans l'onglet 'G√©n√©ration' !")

else:
    st.error("‚ùå Impossible de charger le mod√®le")
    
    with st.expander("üìã Informations de d√©pannage"):
        st.markdown("""
        ### Probl√®mes possibles:
        
        1. **Premi√®re utilisation**: Le t√©l√©chargement du mod√®le peut prendre 1-2 minutes
        2. **Connexion lente**: V√©rifiez votre connexion Internet
        3. **Mod√®le indisponible**: V√©rifiez que le mod√®le existe sur Hugging Face
        
        ### Solutions:
        
        - Rafra√Æchissez la page (F5)
        - Attendez quelques minutes
        - Contactez le support si le probl√®me persiste
        """)
    
    st.info(f"Mod√®le utilis√©: **{MODEL_NAME}**")

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        <p>Propuls√© par <strong>ARSLM</strong> | Cr√©√© avec ‚ù§Ô∏è par Benjamin Amaad Kama</p>
        <p style='font-size: 0.9rem;'>
            <a href='https://github.com/benjaminpolydeq/ARSLM' target='_blank'>GitHub</a> | 
            <a href='mailto:benjokama@hotmail.fr'>Contact</a>
        </p>
    </div>
    """,
    unsafe_allow_html=True
)