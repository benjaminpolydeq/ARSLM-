import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# ================================
# üîπ Config
# ================================
MODEL_PATH = "./model_checkpoint"     # chemin vers ton mod√®le fine-tun√© (ou base si LoRA pr√©sent)
TOKENIZER_PATH = "./tokenizer_checkpoint"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================================
# üîπ Charger mod√®le et tokenizer
# ================================
@st.cache_resource(show_spinner=True)
def load_model():
    # Charger tokenizer (tombe sur TOKENIZER_PATH si renseign√©)
    tokenizer_path = TOKENIZER_PATH if TOKENIZER_PATH else MODEL_PATH
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)

    # Essayer de charger un mod√®le PEFT (LoRA) si le checkpoint contient des adaptateurs,
    # sinon charger un mod√®le causal standard.
    try:
        # Charger le mod√®le de base
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            device_map="auto" if torch.cuda.is_available() else None,
        )

        # Essayer d'envelopper avec PeftModel (si MODEL_PATH contient des poids LoRA)
        model = PeftModel.from_pretrained(base_model, MODEL_PATH, device_map="auto" if torch.cuda.is_available() else None)
    except Exception:
        # Fallback : charger directement le mod√®le causal si Peft √©choue
        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto" if torch.cuda.is_available() else None)

    # Si device_map="auto" a √©t√© utilis√©, le mod√®le est probablement d√©j√† sur le bon device.
    # Sinon on force le to(DEVICE).
    try:
        model.to(DEVICE)
    except Exception:
        pass

    model.eval()
    return tokenizer, model


tokenizer, model = load_model()

# ================================
# üîπ G√©n√©ration de r√©ponse
# ================================
def generate_response(user_input, max_length=200, temperature=0.8, top_p=0.9):
    prompt = f"You are ARSLM, an intelligent and friendly assistant that speaks English.\nUser: {user_input}\nARSLM:"
    inputs = tokenizer(prompt, return_tensors="pt")
    # d√©placer les tenseurs vers l'appareil si n√©cessaire
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs.get("attention_mask", None),
        max_new_tokens=max_length,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        pad_token_id=tokenizer.eos_token_id,
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.split("ARSLM:")[-1].strip()
    return response

# ================================
# üîπ Streamlit Interface
# ================================
st.set_page_config(page_title="ARSLM ‚Äì MicroLLM SaaS", page_icon="ü§ñ")
st.title("ü§ñ ARSLM - MicroLLM SaaS")
st.write("ARSLM est pr√™t √† discuter en anglais !")

user_input = st.text_input("üí¨ Pose une question √† ARSLM :")

if st.button("Envoyer") and user_input.strip() != "":
    with st.spinner("ARSLM r√©fl√©chit..."):
        answer = generate_response(user_input)
        st.markdown(f"**ARSLM:** {answer}")
