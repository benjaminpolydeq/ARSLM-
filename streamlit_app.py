import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# =========================
# CONFIG
# =========================
BASE_MODEL = "distilgpt2"          # ou ton mod√®le de base
LORA_PATH = "./arslm_lora"          # dossier LoRA fine-tun√©
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# =========================
# PROMPT SYST√àME (FREEZE)
# =========================
SYSTEM_PROMPT = """Tu es ARSLM, un assistant intelligent, professionnel et fiable.
Tu r√©ponds en fran√ßais, de mani√®re claire, structur√©e et utile.
Tu ne r√©p√®tes jamais inutilement les mots.
Tu expliques les concepts de fa√ßon p√©dagogique.
Si une question est ambigu√´, tu demandes une clarification.
R√©ponds toujours de mani√®re naturelle et coh√©rente.
"""

# =========================
# CHARGEMENT MOD√àLE
# =========================
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    tokenizer.pad_token = tokenizer.eos_token

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32
    )

    model = PeftModel.from_pretrained(base_model, LORA_PATH)
    model.to(DEVICE)
    model.eval()

    return tokenizer, model

tokenizer, model = load_model()

# =========================
# UI STREAMLIT
# =========================
st.title("üß† ARSLM ‚Äì MicroLLM SaaS")
st.write("Le mod√®le ARSLM est pr√™t √† √™tre test√©.")

user_input = st.text_area("Entrez un texte pour ARSLM :", height=120)

if st.button("G√©n√©rer la r√©ponse"):
    if user_input.strip() == "":
        st.warning("Veuillez entrer une question.")
    else:
        # PROMPT FINAL
        prompt = f"""
{SYSTEM_PROMPT}

Question : {user_input}
R√©ponse :
"""

        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("R√©ponse :")[-1].strip()

        st.subheader("R√©ponse ARSLM :")
        st.write(response)